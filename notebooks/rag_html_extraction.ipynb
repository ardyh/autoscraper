{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG-Based HTML Information Extraction\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) system to extract structured information from HTML documents.\n",
        "\n",
        "## System Architecture\n",
        "1. **Chunking**: Split HTML into semantic chunks (tags, sections)\n",
        "2. **Embedding**: Use `nomic-ai/nomic-embed-code-GGUF` for code-aware embeddings\n",
        "3. **Retrieval**: FAISS vector store for efficient similarity search\n",
        "4. **Generation**: LM Studio hosted model to generate structured JSON output\n",
        "\n",
        "## Scenarios\n",
        "- **Scenario 1**: E-commerce (books) - extract name and price\n",
        "- **Scenario 2**: Job listings - extract title, location, salary, company\n",
        "- **Scenario 3**: Club listings - extract names, logo links, websites\n",
        "- **Scenario 4**: Hidden information - extract property details, coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import httpx\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import faiss\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = Path(\"../data/html\").resolve()\n",
        "EMBEDDING_MODEL_PATH = Path.home() / \".cache/nomic-embed-code-v1.5.Q4_K_M.gguf\"  # Adjust path\n",
        "\n",
        "# LM Studio configuration\n",
        "LMSTUDIO_BASE_URL = os.getenv(\"LMSTUDIO_BASE_URL\", \"http://localhost:1234/v1\")\n",
        "LMSTUDIO_MODEL = os.getenv(\"LMSTUDIO_MODEL\", \"qwen2.5-7b-instruct-1m\")\n",
        "LMSTUDIO_API_KEY = os.getenv(\"LMSTUDIO_API_KEY\", \"lm-studio\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. HTML Chunking Strategy\n",
        "\n",
        "We'll use a semantic chunking approach that preserves HTML structure while creating meaningful chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class HTMLChunk:\n",
        "    \"\"\"Represents a chunk of HTML with metadata.\"\"\"\n",
        "    content: str\n",
        "    tag_path: str  # e.g., \"html > body > div.container > article\"\n",
        "    attributes: Dict[str, str]\n",
        "    chunk_id: int\n",
        "    \n",
        "\n",
        "def extract_tag_path(element) -> str:\n",
        "    \"\"\"Build a CSS-like path for an element.\"\"\"\n",
        "    path_parts = []\n",
        "    for parent in element.parents:\n",
        "        if parent.name is None:\n",
        "            continue\n",
        "        name = parent.name\n",
        "        if parent.get('class'):\n",
        "            name += f\".{parent.get('class')[0]}\"\n",
        "        elif parent.get('id'):\n",
        "            name += f\"#{parent.get('id')}\"\n",
        "        path_parts.append(name)\n",
        "    return \" > \".join(reversed(path_parts[-5:]))  # Last 5 levels\n",
        "\n",
        "\n",
        "def chunk_html(html_content: str, max_chunk_size: int = 1000) -> List[HTMLChunk]:\n",
        "    \"\"\"Chunk HTML into semantic units.\n",
        "    \n",
        "    Strategy:\n",
        "    - Extract meaningful containers (divs, articles, sections, li, tr, etc.)\n",
        "    - Include attributes (class, id, data-*) as metadata\n",
        "    - Keep chunks small enough for embedding but large enough for context\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "    chunks = []\n",
        "    chunk_id = 0\n",
        "\n",
        "    # Expanded set of target tags for more thorough HTML extraction\n",
        "    target_tags = [\n",
        "        'article', 'div', 'section', 'li', 'tr', 'dl', 'aside', 'main', 'header',\n",
        "        'footer', 'nav', 'table', 'thead', 'tbody', 'tfoot', 'ul', 'ol', 'dt',\n",
        "        'dd', 'figure', 'figcaption', 'form', 'fieldset', 'legend', 'h1', 'h2',\n",
        "        'h3', 'h4', 'h5', 'h6', 'pre', 'code', 'blockquote', 'address', 'summary',\n",
        "        'details', 'p'\n",
        "    ]\n",
        "\n",
        "    for tag_name in target_tags:\n",
        "        elements = soup.find_all(tag_name)\n",
        "        for elem in elements:\n",
        "            text = elem.get_text(separator=' ', strip=True)\n",
        "\n",
        "            # Skip empty or very short chunks\n",
        "            if len(text) < 20:\n",
        "                continue\n",
        "\n",
        "            # Get inner HTML (preserving structure)\n",
        "            content = str(elem)[:max_chunk_size]\n",
        "\n",
        "            # Extract attributes\n",
        "            attrs = {k: ' '.join(v) if isinstance(v, list) else v\n",
        "                     for k, v in elem.attrs.items()}\n",
        "\n",
        "            # Build tag path\n",
        "            tag_path = extract_tag_path(elem)\n",
        "\n",
        "            chunks.append(HTMLChunk(\n",
        "                content=content,\n",
        "                tag_path=tag_path,\n",
        "                attributes=attrs,\n",
        "                chunk_id=chunk_id\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "    # Also extract script tags with JSON data\n",
        "    for script in soup.find_all('script', type='application/json'):\n",
        "        if script.string and len(script.string) > 50:\n",
        "            chunks.append(HTMLChunk(\n",
        "                content=script.string[:max_chunk_size],\n",
        "                tag_path=\"script[type=application/json]\",\n",
        "                attributes=script.attrs,\n",
        "                chunk_id=chunk_id\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "    # Extract inline script data (like __NEXT_DATA__)\n",
        "    for script in soup.find_all('script', id=True):\n",
        "        if script.string and ('{' in script.string or '[' in script.string):\n",
        "            chunks.append(HTMLChunk(\n",
        "                content=script.string[:max_chunk_size],\n",
        "                tag_path=f\"script#{script.get('id')}\",\n",
        "                attributes=script.attrs,\n",
        "                chunk_id=chunk_id\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Embedding Model Setup\n",
        "\n",
        "We'll use nomic-embed-code for code-aware embeddings that understand HTML structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NomicEmbedder:\n",
        "    \"\"\"Wrapper for nomic-embed-code using llama.cpp.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_path: Path, embedding_dim: int = 768):\n",
        "        \"\"\"Initialize the embedding model.\n",
        "        \n",
        "        Args:\n",
        "            model_path: Path to the GGUF model file\n",
        "            embedding_dim: Dimension of embeddings (768 for nomic-embed-code)\n",
        "        \"\"\"\n",
        "        if not model_path.exists():\n",
        "            raise FileNotFoundError(\n",
        "                f\"Embedding model not found at {model_path}. \"\n",
        "                f\"Download from: https://huggingface.co/nomic-ai/nomic-embed-code-GGUF\"\n",
        "            )\n",
        "        \n",
        "        self.model = Llama(\n",
        "            model_path=str(model_path),\n",
        "            embedding=True,\n",
        "            n_ctx=2048,\n",
        "            n_batch=512,\n",
        "            verbose=False\n",
        "        )\n",
        "        self.embedding_dim = embedding_dim\n",
        "    \n",
        "    def embed_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for a single text.\"\"\"\n",
        "        # Nomic models expect specific prefix for code\n",
        "        prefixed_text = f\"search_document: {text}\"\n",
        "        embedding = self.model.embed(prefixed_text)\n",
        "        return np.array(embedding, dtype=np.float32)\n",
        "    \n",
        "    def embed_batch(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Generate embeddings for multiple texts.\"\"\"\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            embeddings.append(self.embed_text(text))\n",
        "        return np.vstack(embeddings)\n",
        "    \n",
        "    def embed_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for a search query.\"\"\"\n",
        "        prefixed_query = f\"search_query: {query}\"\n",
        "        embedding = self.model.embed(prefixed_query)\n",
        "        return np.array(embedding, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vector Store and Retrieval\n",
        "\n",
        "FAISS for efficient similarity search over embedded chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HTMLVectorStore:\n",
        "    \"\"\"Vector store for HTML chunks using FAISS.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder: NomicEmbedder):\n",
        "        self.embedder = embedder\n",
        "        self.index: Optional[faiss.Index] = None\n",
        "        self.chunks: List[HTMLChunk] = []\n",
        "        self.dimension = embedder.embedding_dim\n",
        "    \n",
        "    def build_index(self, html_content: str, chunk_size: int = 1000):\n",
        "        \"\"\"Build FAISS index from HTML content.\"\"\"\n",
        "        print(f\"Chunking HTML (target size: {chunk_size})...\")\n",
        "        self.chunks = chunk_html(html_content, max_chunk_size=chunk_size)\n",
        "        print(f\"Created {len(self.chunks)} chunks\")\n",
        "        \n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"No chunks created from HTML\")\n",
        "        \n",
        "        # Extract text for embedding\n",
        "        print(\"Generating embeddings...\")\n",
        "        texts_to_embed = []\n",
        "        for chunk in self.chunks:\n",
        "            # Combine content with metadata for richer embeddings\n",
        "            text = f\"{chunk.tag_path}\\n\"\n",
        "            if chunk.attributes:\n",
        "                attrs_str = \" \".join([f\"{k}={v}\" for k, v in chunk.attributes.items()])\n",
        "                text += f\"Attributes: {attrs_str}\\n\"\n",
        "            text += chunk.content\n",
        "            texts_to_embed.append(text[:2000])  # Limit for embedding\n",
        "        \n",
        "        embeddings = self.embedder.embed_batch(texts_to_embed)\n",
        "        \n",
        "        # Build FAISS index\n",
        "        print(\"Building FAISS index...\")\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)  # Inner product (cosine similarity)\n",
        "        \n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "        \n",
        "        print(f\"Index built with {self.index.ntotal} vectors\")\n",
        "    \n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve top-k most relevant chunks for a query.\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not built. Call build_index first.\")\n",
        "        \n",
        "        # Embed query\n",
        "        query_embedding = self.embedder.embed_query(query).reshape(1, -1)\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        \n",
        "        # Search\n",
        "        scores, indices = self.index.search(query_embedding, top_k)\n",
        "        \n",
        "        # Format results\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(self.chunks):\n",
        "                chunk = self.chunks[idx]\n",
        "                results.append({\n",
        "                    \"content\": chunk.content,\n",
        "                    \"tag_path\": chunk.tag_path,\n",
        "                    \"attributes\": chunk.attributes,\n",
        "                    \"score\": float(score),\n",
        "                    \"chunk_id\": chunk.chunk_id\n",
        "                })\n",
        "        \n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. LLM Generation\n",
        "\n",
        "Use LM Studio to generate structured JSON from retrieved chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lmstudio_chat(\n",
        "    messages: List[Dict[str, str]], \n",
        "    model: Optional[str] = None,\n",
        "    temperature: float = 0.1, \n",
        "    max_tokens: int = 2048\n",
        ") -> str:\n",
        "    \"\"\"Call LM Studio's OpenAI-compatible chat endpoint.\"\"\"\n",
        "    base = LMSTUDIO_BASE_URL.rstrip(\"/\")\n",
        "    if not base.endswith(\"/v1\"):\n",
        "        base = base + \"/v1\"\n",
        "    url = f\"{base}/chat/completions\"\n",
        "    \n",
        "    headers = {\"Authorization\": f\"Bearer {LMSTUDIO_API_KEY}\"}\n",
        "    payload = {\n",
        "        \"model\": model or LMSTUDIO_MODEL,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"stream\": False,\n",
        "    }\n",
        "    \n",
        "    with httpx.Client(timeout=180) as client:\n",
        "        resp = client.post(url, headers=headers, json=payload)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    \n",
        "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def extract_json_from_response(response: str) -> Any:\n",
        "    \"\"\"Extract JSON from LLM response, handling markdown code blocks.\"\"\"\n",
        "    # Try to find JSON in code blocks\n",
        "    match = re.search(r\"```(?:json)?\\s*([\\s\\S]+?)```\", response)\n",
        "    if match:\n",
        "        json_str = match.group(1).strip()\n",
        "    else:\n",
        "        json_str = response.strip()\n",
        "    \n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        # Try to find JSON object/array in the text\n",
        "        for pattern in [r'\\{[\\s\\S]+\\}', r'\\[[\\s\\S]+\\]']:\n",
        "            match = re.search(pattern, response)\n",
        "            if match:\n",
        "                try:\n",
        "                    return json.loads(match.group(0))\n",
        "                except:\n",
        "                    continue\n",
        "        raise ValueError(f\"Could not extract JSON from response: {response[:200]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. RAG Pipeline\n",
        "\n",
        "Combine retrieval and generation for end-to-end extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGHTMLExtractor:\n",
        "    \"\"\"RAG-based HTML information extraction system.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder: NomicEmbedder):\n",
        "        self.embedder = embedder\n",
        "        self.vector_store = HTMLVectorStore(embedder)\n",
        "    \n",
        "    def index_html(self, html_content: str, chunk_size: int = 1000):\n",
        "        \"\"\"Index HTML content for retrieval.\"\"\"\n",
        "        self.vector_store.build_index(html_content, chunk_size=chunk_size)\n",
        "    \n",
        "    def extract(\n",
        "        self, \n",
        "        query: str, \n",
        "        top_k: int = 8,\n",
        "        temperature: float = 0.1,\n",
        "        max_retries: int = 2\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Extract structured information using RAG.\n",
        "        \n",
        "        Args:\n",
        "            query: Natural language query describing what to extract\n",
        "            top_k: Number of chunks to retrieve\n",
        "            temperature: LLM temperature\n",
        "            max_retries: Number of retries if JSON parsing fails\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with extracted information\n",
        "        \"\"\"\n",
        "        # Retrieve relevant chunks\n",
        "        print(f\"\\nRetrieving top-{top_k} chunks for query: {query}\")\n",
        "        retrieved_chunks = self.vector_store.retrieve(query, top_k=top_k)\n",
        "        \n",
        "        if not retrieved_chunks:\n",
        "            return {\"error\": \"No relevant chunks found\", \"data\": []}\n",
        "        \n",
        "        # Show retrieval results\n",
        "        print(\"\\nTop retrieved chunks:\")\n",
        "        for i, chunk in enumerate(retrieved_chunks[:3], 1):\n",
        "            print(f\"  {i}. Score: {chunk['score']:.3f} | Path: {chunk['tag_path'][:60]}\")\n",
        "        \n",
        "        # Build context from retrieved chunks\n",
        "        context_parts = []\n",
        "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "            context_parts.append(f\"--- Chunk {i} (score: {chunk['score']:.3f}) ---\")\n",
        "            context_parts.append(f\"Path: {chunk['tag_path']}\")\n",
        "            if chunk['attributes']:\n",
        "                attrs = ', '.join([f\"{k}={v}\" for k, v in list(chunk['attributes'].items())[:3]])\n",
        "                context_parts.append(f\"Attributes: {attrs}\")\n",
        "            context_parts.append(f\"Content:\\n{chunk['content'][:800]}\")\n",
        "            context_parts.append(\"\")\n",
        "        \n",
        "        context = \"\\n\".join(context_parts)\n",
        "        \n",
        "        # Build prompt\n",
        "        system_prompt = \"\"\"You are an expert at extracting structured information from HTML.\n",
        "Given HTML chunks retrieved for a specific query, extract the requested information and return it as valid JSON.\n",
        "\n",
        "Rules:\n",
        "- Return ONLY valid JSON (array or object)\n",
        "- Extract ALL items found in the chunks\n",
        "- Use null for missing fields\n",
        "- Be precise with data types (numbers as numbers, not strings)\n",
        "- For prices/salaries, extract numeric values when possible\n",
        "- Do not truncate or limit results unless explicitly requested\n",
        "\"\"\"\n",
        "        \n",
        "        user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "Retrieved HTML chunks:\n",
        "{context}\n",
        "\n",
        "Extract the requested information and return as JSON. If the query asks for multiple items, return an array. Each object should have clear field names matching the query.\"\"\"\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "        \n",
        "        # Generate with retries\n",
        "        for attempt in range(max_retries + 1):\n",
        "            try:\n",
        "                print(f\"\\nGenerating response (attempt {attempt + 1}/{max_retries + 1})...\")\n",
        "                response = lmstudio_chat(messages, temperature=temperature)\n",
        "                \n",
        "                # Extract and parse JSON\n",
        "                result = extract_json_from_response(response)\n",
        "                \n",
        "                print(f\"‚úì Successfully extracted {len(result) if isinstance(result, list) else 1} item(s)\")\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"data\": result,\n",
        "                    \"query\": query,\n",
        "                    \"chunks_used\": len(retrieved_chunks)\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚úó Attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt < max_retries:\n",
        "                    # Add feedback for retry\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "                    messages.append({\n",
        "                        \"role\": \"user\", \n",
        "                        \"content\": f\"The response was not valid JSON. Error: {e}. Please return ONLY valid JSON.\"\n",
        "                    })\n",
        "                else:\n",
        "                    return {\n",
        "                        \"success\": False,\n",
        "                        \"error\": str(e),\n",
        "                        \"raw_response\": response,\n",
        "                        \"query\": query\n",
        "                    }\n",
        "    \n",
        "    def extract_from_file(\n",
        "        self, \n",
        "        html_path: Path, \n",
        "        query: str,\n",
        "        top_k: int = 8,\n",
        "        chunk_size: int = 1000\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Extract information from an HTML file.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing: {html_path.name}\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Load HTML\n",
        "        html_content = html_path.read_text(encoding='utf-8', errors='ignore')\n",
        "        \n",
        "        # Index\n",
        "        self.index_html(html_content, chunk_size=chunk_size)\n",
        "        \n",
        "        # Extract\n",
        "        result = self.extract(query, top_k=top_k)\n",
        "        \n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize System\n",
        "\n",
        "Load the embedding model and create the RAG extractor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model...\n",
            "‚úó Embedding model not found at /Users/ardyh/.cache/nomic-embed-code-v1.5.Q4_K_M.gguf. Download from: https://huggingface.co/nomic-ai/nomic-embed-code-GGUF\n",
            "\n",
            "To download the model:\n",
            "  huggingface-cli download nomic-ai/nomic-embed-code-GGUF nomic-embed-code-v1.5.Q4_K_M.gguf --local-dir ~/.cache/\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "Embedding model not found at /Users/ardyh/.cache/nomic-embed-code-v1.5.Q4_K_M.gguf. Download from: https://huggingface.co/nomic-ai/nomic-embed-code-GGUF",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading embedding model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     embedder = \u001b[43mNomicEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDING_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Loaded nomic-embed-code from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_MODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mNomicEmbedder.__init__\u001b[39m\u001b[34m(self, model_path, embedding_dim)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize the embedding model.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    model_path: Path to the GGUF model file\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    embedding_dim: Dimension of embeddings (768 for nomic-embed-code)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     13\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding model not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload from: https://huggingface.co/nomic-ai/nomic-embed-code-GGUF\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.model = Llama(\n\u001b[32m     18\u001b[39m     model_path=\u001b[38;5;28mstr\u001b[39m(model_path),\n\u001b[32m     19\u001b[39m     embedding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.embedding_dim = embedding_dim\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Embedding model not found at /Users/ardyh/.cache/nomic-embed-code-v1.5.Q4_K_M.gguf. Download from: https://huggingface.co/nomic-ai/nomic-embed-code-GGUF"
          ]
        }
      ],
      "source": [
        "# Initialize embedder\n",
        "print(\"Loading embedding model...\")\n",
        "try:\n",
        "    embedder = NomicEmbedder(EMBEDDING_MODEL_PATH)\n",
        "    print(f\"‚úì Loaded nomic-embed-code from {EMBEDDING_MODEL_PATH}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚úó {e}\")\n",
        "    print(\"\\nTo download the model:\")\n",
        "    print(\"  huggingface-cli download nomic-ai/nomic-embed-code-GGUF nomic-embed-code-v1.5.Q4_K_M.gguf --local-dir ~/.cache/\")\n",
        "    raise\n",
        "\n",
        "# Create RAG extractor\n",
        "rag_extractor = RAGHTMLExtractor(embedder)\n",
        "print(\"‚úì RAG system initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Scenarios\n",
        "\n",
        "Run the extraction on all test scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 1: E-commerce Book Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario1_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario1_books.html\",\n",
        "    query=\"Extract all books with their name and price\",\n",
        "    top_k=10,\n",
        "    chunk_size=800\n",
        ")\n",
        "\n",
        "if scenario1_result[\"success\"]:\n",
        "    print(\"\\nüìö Sample extracted books:\")\n",
        "    print(json.dumps(scenario1_result[\"data\"][:3], indent=2))\n",
        "    print(f\"\\nTotal books extracted: {len(scenario1_result['data'])}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario1_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 2: Job Listings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario2_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario2_jobs.html\",\n",
        "    query=\"Extract job title, location, salary, and company name from all job listings\",\n",
        "    top_k=12,\n",
        "    chunk_size=1200\n",
        ")\n",
        "\n",
        "if scenario2_result[\"success\"]:\n",
        "    print(\"\\nüíº Sample extracted jobs:\")\n",
        "    print(json.dumps(scenario2_result[\"data\"][:3], indent=2))\n",
        "    print(f\"\\nTotal jobs extracted: {len(scenario2_result['data'])}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario2_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 3: Club Listings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario3_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario3_clubs.html\",\n",
        "    query=\"Get the club names, logo image links and their official websites\",\n",
        "    top_k=10,\n",
        "    chunk_size=1000\n",
        ")\n",
        "\n",
        "if scenario3_result[\"success\"]:\n",
        "    print(\"\\n‚öΩ Sample extracted clubs:\")\n",
        "    print(json.dumps(scenario3_result[\"data\"][:3], indent=2))\n",
        "    print(f\"\\nTotal clubs extracted: {len(scenario3_result['data'])}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario3_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 4: Property Details (Hidden Information)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario4_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario4_property.html\",\n",
        "    query=\"Return the property name, address, latitude and longitude\",\n",
        "    top_k=8,\n",
        "    chunk_size=1500\n",
        ")\n",
        "\n",
        "if scenario4_result[\"success\"]:\n",
        "    print(\"\\nüè† Extracted property details:\")\n",
        "    print(json.dumps(scenario4_result[\"data\"], indent=2))\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario4_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_summary = {\n",
        "    \"Scenario 1 (Books)\": {\n",
        "        \"success\": scenario1_result[\"success\"],\n",
        "        \"items_extracted\": len(scenario1_result.get(\"data\", [])) if isinstance(scenario1_result.get(\"data\"), list) else 1,\n",
        "        \"chunks_used\": scenario1_result.get(\"chunks_used\", 0)\n",
        "    },\n",
        "    \"Scenario 2 (Jobs)\": {\n",
        "        \"success\": scenario2_result[\"success\"],\n",
        "        \"items_extracted\": len(scenario2_result.get(\"data\", [])) if isinstance(scenario2_result.get(\"data\"), list) else 1,\n",
        "        \"chunks_used\": scenario2_result.get(\"chunks_used\", 0)\n",
        "    },\n",
        "    \"Scenario 3 (Clubs)\": {\n",
        "        \"success\": scenario3_result[\"success\"],\n",
        "        \"items_extracted\": len(scenario3_result.get(\"data\", [])) if isinstance(scenario3_result.get(\"data\"), list) else 1,\n",
        "        \"chunks_used\": scenario3_result.get(\"chunks_used\", 0)\n",
        "    },\n",
        "    \"Scenario 4 (Property)\": {\n",
        "        \"success\": scenario4_result[\"success\"],\n",
        "        \"items_extracted\": 1 if scenario4_result[\"success\"] else 0,\n",
        "        \"chunks_used\": scenario4_result.get(\"chunks_used\", 0)\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(json.dumps(results_summary, indent=2))\n",
        "\n",
        "# Save results\n",
        "output_dir = Path(\"../generated\").resolve()\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "output_file = output_dir / f\"rag_extraction_results_{int(time.time())}.json\"\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump({\n",
        "        \"scenario1\": scenario1_result,\n",
        "        \"scenario2\": scenario2_result,\n",
        "        \"scenario3\": scenario3_result,\n",
        "        \"scenario4\": scenario4_result,\n",
        "        \"summary\": results_summary\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Results saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Interactive Extraction\n",
        "\n",
        "Test custom queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_extract(scenario_file: str, custom_query: str):\n",
        "    \"\"\"Run a custom extraction query.\"\"\"\n",
        "    result = rag_extractor.extract_from_file(\n",
        "        html_path=DATA_DIR / scenario_file,\n",
        "        query=custom_query,\n",
        "        top_k=10\n",
        "    )\n",
        "    \n",
        "    if result[\"success\"]:\n",
        "        print(\"\\n‚úì Extraction successful!\")\n",
        "        print(json.dumps(result[\"data\"], indent=2)[:1000])  # First 1000 chars\n",
        "    else:\n",
        "        print(f\"\\n‚úó Failed: {result['error']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Example: Custom query (uncomment to use)\n",
        "# custom_result = interactive_extract(\n",
        "#     scenario_file=\"scenario1_books.html\",\n",
        "#     custom_query=\"Find all books with 5-star ratings and extract their titles and prices\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### Model Setup\n",
        "1. **Embedding Model**: Download nomic-embed-code GGUF from HuggingFace:\n",
        "   ```bash\n",
        "   huggingface-cli download nomic-ai/nomic-embed-code-GGUF nomic-embed-code-v1.5.Q4_K_M.gguf --local-dir ~/.cache/\n",
        "   ```\n",
        "\n",
        "2. **LM Studio**: Ensure LM Studio is running with a model loaded (e.g., Qwen2.5-7B-Instruct)\n",
        "\n",
        "### Performance Tuning\n",
        "- **chunk_size**: Smaller chunks (500-800) for precise extraction, larger (1000-1500) for context\n",
        "- **top_k**: More chunks (10-15) for comprehensive extraction, fewer (5-8) for speed\n",
        "- **temperature**: Lower (0.0-0.2) for consistent structured output\n",
        "\n",
        "### Advantages of RAG Approach\n",
        "- ‚úì Handles large HTML files efficiently\n",
        "- ‚úì Retrieves only relevant sections\n",
        "- ‚úì Better context understanding with semantic search\n",
        "- ‚úì Scalable to multiple documents\n",
        "- ‚úì Works with hidden/embedded JSON data\n",
        "\n",
        "### Dependencies\n",
        "```bash\n",
        "pip install beautifulsoup4 lxml faiss-cpu llama-cpp-python httpx numpy\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mrscraper",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
