{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG-based HTML Extraction with LangChain\n",
        "\n",
        "This notebook explores using Retrieval Augmented Generation (RAG) for intelligent HTML parsing and structured data extraction.\n",
        "\n",
        "## Approach\n",
        "1. Load HTML content\n",
        "2. Split HTML into chunks\n",
        "3. Create embeddings and store in vector database\n",
        "4. Retrieve relevant chunks based on natural language query\n",
        "5. Use LLM to extract structured JSON from retrieved chunks\n",
        "\n",
        "## Use Case\n",
        "Parse HTML content and return structured JSON data based on natural language queries for:\n",
        "- E-commerce book listings\n",
        "- Job postings\n",
        "- Club directories\n",
        "- Property information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain_core.prompts import PromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project paths\n",
        "PROJECT_ROOT = Path(\"/Users/ardyh/Documents/job-applications/mrscraper\")\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"html\"\n",
        "\n",
        "# Available HTML files\n",
        "HTML_FILES = {\n",
        "    \"scenario1_books\": DATA_DIR / \"scenario1_books.html\",\n",
        "    \"scenario2_jobs\": DATA_DIR / \"scenario2_jobs.html\",\n",
        "    \"scenario3_clubs\": DATA_DIR / \"scenario3_clubs.html\",\n",
        "    \"scenario4_property\": DATA_DIR / \"scenario4_property.html\"\n",
        "}\n",
        "\n",
        "# Test queries for each scenario\n",
        "TEST_QUERIES = {\n",
        "    \"scenario1_books\": \"Can you return me the books: name and price?\",\n",
        "    \"scenario2_jobs\": \"Extract job title, location, salary, and company name from the listings\",\n",
        "    \"scenario3_clubs\": \"Get the club names, logo image links and their official websites\",\n",
        "    \"scenario4_property\": \"Return the property name, address, latitude and longitude\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initialize Embedding Model\n",
        "\n",
        "Using HuggingFace embeddings (free, self-hosted) instead of OpenAI to avoid external API costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/4z/mfq1y5w57hj01ttff7smkspw0000gn/T/ipykernel_13444/3293437462.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings model initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize embedding model (using free HuggingFace embeddings)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"Embeddings model initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load and Process HTML\n",
        "\n",
        "Load HTML content and split it into manageable chunks for embedding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading scenario2_jobs.html...\n",
            "Loaded 1 document(s)\n",
            "Total characters: 9353\n",
            "\n",
            "Creating chunks...\n",
            "Created 11 chunks\n",
            "\n",
            "Sample chunk (first 200 chars):\n",
            "Search results\n",
            "\n",
            "Save this search to receive job alerts by email when new jobs match.\n",
            "\n",
            "2247 jobs found\n",
            "\n",
            "Sort By\n",
            "\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "L...\n"
          ]
        }
      ],
      "source": [
        "def load_html_file(file_path: Path):\n",
        "    \"\"\"Load HTML file using UnstructuredHTMLLoader\"\"\"\n",
        "    loader = UnstructuredHTMLLoader(str(file_path))\n",
        "    documents = loader.load()\n",
        "    return documents\n",
        "\n",
        "def create_chunks(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Split documents into chunks\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    return chunks\n",
        "\n",
        "# Test with scenario 1 (books)\n",
        "scenario = \"scenario2_jobs\"\n",
        "html_file = HTML_FILES[scenario]\n",
        "\n",
        "print(f\"Loading {html_file.name}...\")\n",
        "documents = load_html_file(html_file)\n",
        "print(f\"Loaded {len(documents)} document(s)\")\n",
        "print(f\"Total characters: {len(documents[0].page_content)}\")\n",
        "\n",
        "print(\"\\nCreating chunks...\")\n",
        "chunks = create_chunks(documents)\n",
        "print(f\"Created {len(chunks)} chunks\")\n",
        "print(f\"\\nSample chunk (first 200 chars):\\n{chunks[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Vector Store\n",
        "\n",
        "Store document chunks in a vector database for similarity search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vector store...\n",
            "Vector store created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create vector store\n",
        "print(\"Creating vector store...\")\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    collection_name=f\"html_chunks_{scenario}\"\n",
        ")\n",
        "print(\"Vector store created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Perform Dense Retrieval\n",
        "\n",
        "Retrieve the most relevant chunks based on the query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Extract job title, location, salary, and company name from the listings\n",
            "\n",
            "Retrieved 22 relevant chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Search results\n",
            "\n",
            "Save this search to receive job alerts by email when new jobs match.\n",
            "\n",
            "2247 jobs found\n",
            "\n",
            "Sort By\n",
            "\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "18 Dec 2025 ~ 18 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bu\n",
            "\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Search results\n",
            "\n",
            "Save this search to receive job alerts by email when new jobs match.\n",
            "\n",
            "2247 jobs found\n",
            "\n",
            "Sort By\n",
            "\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "18 Dec 2025 ~ 18 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bu\n",
            "\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Specialist Consultant\n",
            "\n",
            "GP - Emergency Medicine / SMO\n",
            "\n",
            "Mudgee, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$3,000 per day\n",
            "\n",
            "22 Dec 2025 ~ 24 Dec 2025\n",
            "\n",
            "This public hospital is located in a beautiful rural town in Australia. It is a great place to work and live, with a friendly and welcoming community. The hospital offe\n",
            "\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Specialist Consultant\n",
            "\n",
            "GP - Emergency Medicine / SMO\n",
            "\n",
            "Mudgee, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$3,000 per day\n",
            "\n",
            "22 Dec 2025 ~ 24 Dec 2025\n",
            "\n",
            "This public hospital is located in a beautiful rural town in Australia. It is a great place to work and live, with a friendly and welcoming community. The hospital offe\n",
            "\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Access our free service, designed to save you time\n",
            "\n",
            "Job searching is done for you\n",
            "\n",
            "Get told about jobs before they hit the market\n",
            "\n",
            "Travel and accommodation organized and paid for\n",
            "\n",
            "REGISTER\n",
            "\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$170 per hour\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "--- Chunk 6 ---\n",
            "Access our free service, designed to save you time\n",
            "\n",
            "Job searching is done for you\n",
            "\n",
            "Get told about jobs before they hit the market\n",
            "\n",
            "Travel and accommodation organized and paid for\n",
            "\n",
            "REGISTER\n",
            "\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$170 per hour\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "--- Chunk 7 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "30 Nov 2025 ~ 30 Nov 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 8 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "30 Nov 2025 ~ 30 Nov 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 9 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "07 Dec 2025 ~ 07 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 10 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "07 Dec 2025 ~ 07 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 11 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$170 per hour\n",
            "\n",
            "12 Dec 2025 ~ 13 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 12 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$170 per hour\n",
            "\n",
            "12 Dec 2025 ~ 13 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 13 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "08 Dec 2025 ~ 10 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 14 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "08 Dec 2025 ~ 10 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 15 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "12 Dec 2025 ~ 14 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 16 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "12 Dec 2025 ~ 14 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 17 ---\n",
            "Registrar\n",
            "\n",
            "Obstetrics and Gynaecology\n",
            "\n",
            "Gosford, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$200 per hour\n",
            "\n",
            "24 Dec 2025 ~ 27 Dec 2025\n",
            "\n",
            "This is a 484-bed public hospital, 76km North of Sydney CBD, and provides a range of medical, surgical and maternity services to the Central Coast Region of NSW. The hospital's emerge\n",
            "\n",
            "\n",
            "--- Chunk 18 ---\n",
            "Registrar\n",
            "\n",
            "Obstetrics and Gynaecology\n",
            "\n",
            "Gosford, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$200 per hour\n",
            "\n",
            "24 Dec 2025 ~ 27 Dec 2025\n",
            "\n",
            "This is a 484-bed public hospital, 76km North of Sydney CBD, and provides a range of medical, surgical and maternity services to the Central Coast Region of NSW. The hospital's emerge\n",
            "\n",
            "\n",
            "--- Chunk 19 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "04 Dec 2025 ~ 04 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 20 ---\n",
            "Resident Medical Officer\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "North Tamworth, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$160 per hour\n",
            "\n",
            "04 Dec 2025 ~ 04 Dec 2025\n",
            "\n",
            "This public hospital in Australia is located in a bustling city that offers a unique blend of urban and rural living. The hospital is situated in a picturesque loca\n",
            "\n",
            "\n",
            "--- Chunk 21 ---\n",
            "Registrar\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "Nowra, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$180 per hour\n",
            "\n",
            "16 Nov 2025 ~ 17 Nov 2025\n",
            "\n",
            "Located in the beautiful coastal region of New South Wales, this public hospital is a vital healthcare provider for the local community. The hospital is situated in a peaceful and pictures\n",
            "\n",
            "\n",
            "--- Chunk 22 ---\n",
            "Registrar\n",
            "\n",
            "Emergency Medicine (ED)\n",
            "\n",
            "Nowra, New South Wales AU\n",
            "\n",
            "Locum\n",
            "\n",
            "$180 per hour\n",
            "\n",
            "16 Nov 2025 ~ 17 Nov 2025\n",
            "\n",
            "Located in the beautiful coastal region of New South Wales, this public hospital is a vital healthcare provider for the local community. The hospital is situated in a peaceful and pictures\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Retrieve relevant chunks\n",
        "query = TEST_QUERIES[scenario]\n",
        "print(f\"Query: {query}\\n\")\n",
        "\n",
        "k = 50  # Number of chunks to retrieve\n",
        "retrieved_docs = vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_docs)} relevant chunks:\\n\")\n",
        "for i, doc in enumerate(retrieved_docs, 1):\n",
        "    print(f\"--- Chunk {i} ---\")\n",
        "    print(doc.page_content[:300])\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Initialize LLM for Extraction\n",
        "\n",
        "Use a local LLM (via Ollama) to extract structured data from retrieved chunks.\n",
        "\n",
        "**Note:** Make sure Ollama is installed and running locally with a model like `llama3` or `mistral`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize Ollama LLM (make sure Ollama is running locally)\n",
        "try:\n",
        "    llm = Ollama(\n",
        "        model=\"llama3\",  # or \"mistral\", \"qwen\", etc.\n",
        "        temperature=0\n",
        "    )\n",
        "    print(\"LLM initialized successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing LLM: {e}\")\n",
        "    print(\"\\nMake sure Ollama is installed and running.\")\n",
        "    print(\"Install: https://ollama.ai\")\n",
        "    print(\"Run: ollama pull llama3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Create Extraction Chain\n",
        "\n",
        "Build a prompt template and chain to extract structured JSON from HTML chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction chain created!\n"
          ]
        }
      ],
      "source": [
        "# Create extraction prompt template\n",
        "extraction_template = \"\"\"\n",
        "You are an expert at extracting structured data from HTML content.\n",
        "\n",
        "HTML Content:\n",
        "{html_content}\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Instructions:\n",
        "1. Analyze the HTML content carefully\n",
        "2. Extract the information requested in the query\n",
        "3. Return ONLY valid JSON format\n",
        "4. If you find multiple items, return them as a JSON array\n",
        "5. Use clear, descriptive field names\n",
        "6. Include ALL relevant data mentioned in the query\n",
        "\n",
        "JSON Output:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"html_content\", \"query\"],\n",
        "    template=extraction_template\n",
        ")\n",
        "\n",
        "# Create extraction chain using LCEL (LangChain Expression Language)\n",
        "extraction_chain = prompt | llm\n",
        "\n",
        "print(\"Extraction chain created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Extract Structured Data\n",
        "\n",
        "Use the LLM to extract structured JSON from the retrieved HTML chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data for query: Extract job title, location, salary, and company name from the listings\n",
            "\n",
            "Extracted JSON:\n",
            "After analyzing the HTML content, I extracted the following information:\n",
            "\n",
            "```\n",
            "[\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$160 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Registrar\",\n",
            "    \"Location\": \"Gosford, New South Wales AU\",\n",
            "    \"Salary\": \"$200 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Specialist Consultant\",\n",
            "    \"Location\": \"Mudgee, New South Wales AU\",\n",
            "    \"Salary\": \"$3,000 per day\",\n",
            "    \"Company Name\": \"\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "Note that the company name is not specified in the HTML content, so it is left blank.\n"
          ]
        }
      ],
      "source": [
        "# Combine retrieved chunks\n",
        "combined_content = \"\\n\\n\".join([f\"Chunk{i}\\n{doc.page_content}\" for i, doc in enumerate(retrieved_docs)])\n",
        "\n",
        "# Extract structured data\n",
        "print(f\"Extracting data for query: {query}\\n\")\n",
        "result = extraction_chain.invoke({\n",
        "    \"html_content\": combined_content,\n",
        "    \"query\": query\n",
        "})\n",
        "\n",
        "print(\"Extracted JSON:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error parsing JSON: Expecting value: line 12 column 3 (char 363)\n",
            "Raw response:\n",
            "After analyzing the HTML content, I extracted the requested information and returned it in JSON format:\n",
            "\n",
            "```\n",
            "[\n",
            "  {\n",
            "    \"Club Name\": \"Scottsdale Soccer Club\",\n",
            "    \"Logo Image Link\": \"/images/clubs/scottsdale-soccer-club.png\",\n",
            "    \"Official Website\": \"https://www.scottsdalebcsoccer.com\"\n",
            "  },\n",
            "  {\n",
            "    \"Club Name\": \"Tucson Soccer Academy\",\n",
            "    \"Logo Image Link\": \"/images/clubs/tucson-soccer-academy.png\",\n",
            "    \"Official Website\": \"https://www.tucsonsocceracademy.com\"\n",
            "  },\n",
            "  ...\n",
            "]\n",
            "```\n",
            "\n",
            "Note: The JSON output only includes the club names, logo image links, and official websites mentioned in the query. If there are more clubs listed on the page, I would extract their information as well and include it in the JSON array.\n",
            "\n",
            "Please let me know if you'd like me to extract any additional data or if this meets your requirements!\n"
          ]
        }
      ],
      "source": [
        "# Try to parse and pretty-print the JSON\n",
        "try:\n",
        "    # Extract JSON from the response (in case there's extra text)\n",
        "    json_start = result.find('[')\n",
        "    json_end = result.rfind(']') + 1\n",
        "    \n",
        "    if json_start == -1:\n",
        "        json_start = result.find('{')\n",
        "        json_end = result.rfind('}') + 1\n",
        "    \n",
        "    if json_start != -1 and json_end > json_start:\n",
        "        json_str = result[json_start:json_end]\n",
        "        parsed_json = json.loads(json_str)\n",
        "        print(\"\\nParsed JSON (pretty-printed):\")\n",
        "        print(json.dumps(parsed_json, indent=2))\n",
        "    else:\n",
        "        print(\"Could not extract valid JSON from response\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error parsing JSON: {e}\")\n",
        "    print(\"Raw response:\")\n",
        "    print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Complete RAG Pipeline Function\n",
        "\n",
        "Create a complete function that handles the entire RAG pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG pipeline function defined!\n"
          ]
        }
      ],
      "source": [
        "def rag_html_extraction(html_file_path: Path, query: str, k: int = 5, chunk_size: int = 1000):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline for HTML extraction\n",
        "    \n",
        "    Args:\n",
        "        html_file_path: Path to HTML file\n",
        "        query: Natural language query\n",
        "        k: Number of chunks to retrieve\n",
        "        chunk_size: Size of text chunks\n",
        "    \n",
        "    Returns:\n",
        "        dict: Extracted structured data\n",
        "    \"\"\"\n",
        "    # 1. Load HTML\n",
        "    print(f\"Loading {html_file_path.name}...\")\n",
        "    documents = load_html_file(html_file_path)\n",
        "    \n",
        "    # 2. Create chunks\n",
        "    print(f\"Creating chunks (size={chunk_size})...\")\n",
        "    chunks = create_chunks(documents, chunk_size=chunk_size)\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    \n",
        "    # 3. Create vector store\n",
        "    print(\"Creating vector store...\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"temp_html_chunks\"\n",
        "    )\n",
        "    \n",
        "    # 4. Retrieve relevant chunks\n",
        "    print(f\"Retrieving top {k} relevant chunks...\")\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=k)\n",
        "    \n",
        "    # 5. Combine chunks\n",
        "    combined_content = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "    \n",
        "    # 6. Extract with LLM\n",
        "    print(\"Extracting structured data...\")\n",
        "    result = extraction_chain.invoke({\n",
        "        \"html_content\": combined_content,\n",
        "        \"query\": query\n",
        "    })\n",
        "    \n",
        "    # 7. Parse JSON\n",
        "    try:\n",
        "        json_start = result.find('[')\n",
        "        json_end = result.rfind(']') + 1\n",
        "        \n",
        "        if json_start == -1:\n",
        "            json_start = result.find('{')\n",
        "            json_end = result.rfind('}') + 1\n",
        "        \n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_str = result[json_start:json_end]\n",
        "            parsed_json = json.loads(json_str)\n",
        "            return parsed_json\n",
        "        else:\n",
        "            return {\"error\": \"Could not extract valid JSON\", \"raw_response\": result}\n",
        "    except json.JSONDecodeError as e:\n",
        "        return {\"error\": str(e), \"raw_response\": result}\n",
        "\n",
        "print(\"RAG pipeline function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Test All Scenarios\n",
        "\n",
        "Test the RAG pipeline on all four scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing scenario1_books\n",
            "================================================================================\n",
            "\n",
            "Query: Can you return me the books: name and price?\n",
            "\n",
            "Loading scenario1_books.html...\n",
            "Creating chunks (size=1000)...\n",
            "Created 3 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 5 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "[\n",
            "  {\n",
            "    \"book_name\": \"A Light in the Attic\",\n",
            "    \"price\": \"\\u00a351.77\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Tipping the Velvet\",\n",
            "    \"price\": \"\\u00a353.74\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Soumission\",\n",
            "    \"price\": \"\\u00a350.10\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Sharp Objects\",\n",
            "    \"price\": \"\\u00a347.82\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Sapiens: A Brief History of Humankind\",\n",
            "    \"price\": \"\\u00a354.23\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"The Requiem Red\",\n",
            "    \"price\": \"\\u00a322.65\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"The Dirty Little Secrets of Getting Your Dream Job\",\n",
            "    \"price\": \"\\u00a333.34\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\",\n",
            "    \"price\": \"\\u00a317.93\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\",\n",
            "    \"price\": \"\\u00a322.60\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"The Black Maria\",\n",
            "    \"price\": \"\\u00a352.15\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Starving Hearts (Triangular Trade Trilogy, #1)\",\n",
            "    \"price\": \"\\u00a313.99\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Shakespeare's Sonnets\",\n",
            "    \"price\": \"\\u00a320.66\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Set Me Free\",\n",
            "    \"price\": \"\\u00a317.46\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\",\n",
            "    \"price\": \"\\u00a352.29\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Rip it Up and Start Again\",\n",
            "    \"price\": \"\\u00a335.02\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\",\n",
            "    \"price\": \"\\u00a357.25\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Olio\",\n",
            "    \"price\": \"\\u00a323.88\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Mesaerion: The Best Science Fiction Stories 1800-1849\",\n",
            "    \"price\": \"\\u00a337.59\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Libertarianism for Beginners\",\n",
            "    \"price\": \"\\u00a351.33\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"It's Only the Himalayas\",\n",
            "    \"price\": \"\\u00a345.17\"\n",
            "  }\n",
            "]\n",
            "\n",
            "================================================================================\n",
            "Testing scenario2_jobs\n",
            "================================================================================\n",
            "\n",
            "Query: Extract job title, location, salary, and company name from the listings\n",
            "\n",
            "Loading scenario2_jobs.html...\n",
            "Creating chunks (size=1000)...\n",
            "Created 11 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 5 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "[\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$160 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$160 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$170 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$160 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$160 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Registrar\",\n",
            "    \"Location\": \"Taree, New South Wales AU\",\n",
            "    \"Salary\": \"$250 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Registrar\",\n",
            "    \"Location\": \"Gosford, New South Wales AU\",\n",
            "    \"Salary\": \"$200 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  }\n",
            "]\n",
            "\n",
            "================================================================================\n",
            "Testing scenario3_clubs\n",
            "================================================================================\n",
            "\n",
            "Query: Get the club names, logo image links and their official websites\n",
            "\n",
            "Loading scenario3_clubs.html...\n",
            "Creating chunks (size=1000)...\n",
            "Created 1 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 5 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "[\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Specialty\": \"Emergency Medicine (ED)\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Type\": \"Locum\",\n",
            "    \"Hourly Rate\": \"$170 per hour\",\n",
            "    \"Duration\": \"15 Dec 2025 ~ 17 Dec 2025\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Specialty\": \"Emergency Medicine (ED)\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Type\": \"Locum\",\n",
            "    \"Hourly Rate\": \"$160 per hour\",\n",
            "    \"Duration\": \"12 Dec 2025 ~ 14 Dec 2025\"\n",
            "  }\n",
            "]\n",
            "\n",
            "================================================================================\n",
            "Testing scenario4_property\n",
            "================================================================================\n",
            "\n",
            "Query: Return the property name, address, latitude and longitude\n",
            "\n",
            "Loading scenario4_property.html...\n",
            "Creating chunks (size=1000)...\n",
            "Created 8 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 5 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "{\n",
            "  \"error\": \"Extra data: line 9 column 1 (char 122)\",\n",
            "  \"raw_response\": \"After analyzing the HTML content, I extracted the following information:\\n\\n```\\n{\\n  \\\"properties\\\": [\\n    {\\n      \\\"name\\\": \\\"Primary Bedroom\\\",\\n      \\\"address\\\": null,\\n      \\\"latitude\\\": null,\\n      \\\"longitude\\\": null\\n    }\\n  ]\\n}\\n```\\n\\nNote that there is no specific property name or address mentioned in the HTML content. The \\\"Primary Bedroom\\\" refers to a bedroom within the property, not the property itself.\\n\\nIf you're looking for information on vacation rentals, I can extract the following:\\n\\n```\\n{\\n  \\\"vacationRentals\\\": [\\n    {\\n      \\\"name\\\": null,\\n      \\\"address\\\": null,\\n      \\\"latitude\\\": null,\\n      \\\"longitude\\\": null\\n    }\\n  ]\\n}\\n```\\n\\nAgain, there is no specific property name or address mentioned in the HTML content. The vacation rentals listed are Park City, Paso Robles, Poconos, Port Aransas, San Diego, Scottsdale, Sedona, Smoky Mountains, Sonoma, Telluride, Temecula, and Vail.\\n\\nIf you'd like me to extract more information or clarify any specific fields, please let me know!\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Test all scenarios\n",
        "results = {}\n",
        "\n",
        "for scenario_name, html_file in HTML_FILES.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing {scenario_name}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    query = TEST_QUERIES[scenario_name]\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result = rag_html_extraction(html_file, query, k=5)\n",
        "        results[scenario_name] = result\n",
        "        \n",
        "        print(\"\\nResult:\")\n",
        "        print(json.dumps(result, indent=2))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        results[scenario_name] = {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Analyze Results\n",
        "\n",
        "Analyze the performance and quality of extraction for each scenario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of Results:\n",
            "================================================================================\n",
            "\n",
            "scenario1_books:\n",
            "  ✓ Extracted 20 items\n",
            "  Fields: book_name, price\n",
            "\n",
            "scenario2_jobs:\n",
            "  ✓ Extracted 7 items\n",
            "  Fields: Job Title, Location, Salary, Company Name\n",
            "\n",
            "scenario3_clubs:\n",
            "  ✓ Extracted 2 items\n",
            "  Fields: Job Title, Specialty, Location, Type, Hourly Rate, Duration\n",
            "\n",
            "scenario4_property:\n",
            "  ✗ Error or no data extracted\n"
          ]
        }
      ],
      "source": [
        "# Summary of results\n",
        "print(\"Summary of Results:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for scenario_name, result in results.items():\n",
        "    print(f\"\\n{scenario_name}:\")\n",
        "    \n",
        "    if isinstance(result, list):\n",
        "        print(f\"  ✓ Extracted {len(result)} items\")\n",
        "        if len(result) > 0:\n",
        "            print(f\"  Fields: {', '.join(result[0].keys())}\")\n",
        "    elif isinstance(result, dict) and \"error\" not in result:\n",
        "        print(f\"  ✓ Extracted data\")\n",
        "        print(f\"  Fields: {', '.join(result.keys())}\")\n",
        "    else:\n",
        "        print(f\"  ✗ Error or no data extracted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Experiment with Parameters\n",
        "\n",
        "Try different chunk sizes and retrieval parameters to optimize performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Improved RAG Implementation\n",
        "\n",
        "Addressing the limitations:\n",
        "1. **Scenarios 2 & 3**: Increase chunk size and retrieval count to capture more items\n",
        "2. **Scenario 4**: Use raw HTML instead of text-only to preserve attributes and tags containing coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improved RAG pipeline function defined!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def load_raw_html_file(file_path: Path):\n",
        "    \"\"\"Load HTML file preserving raw HTML tags and attributes\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        html_content = f.read()\n",
        "    # Create a document with raw HTML\n",
        "    return [Document(page_content=html_content, metadata={\"source\": str(file_path)})]\n",
        "\n",
        "def rag_html_extraction_improved(html_file_path: Path, query: str, k: int = 10, \n",
        "                                  chunk_size: int = 2000, use_raw_html: bool = False):\n",
        "    \"\"\"\n",
        "    Improved RAG pipeline with better parameters and raw HTML support\n",
        "    \n",
        "    Args:\n",
        "        html_file_path: Path to HTML file\n",
        "        query: Natural language query\n",
        "        k: Number of chunks to retrieve (increased default)\n",
        "        chunk_size: Size of text chunks (increased default)\n",
        "        use_raw_html: If True, preserve raw HTML tags and attributes\n",
        "    \n",
        "    Returns:\n",
        "        dict: Extracted structured data\n",
        "    \"\"\"\n",
        "    # 1. Load HTML\n",
        "    print(f\"Loading {html_file_path.name}...\")\n",
        "    if use_raw_html:\n",
        "        documents = load_raw_html_file(html_file_path)\n",
        "        print(\"  Using raw HTML (preserves tags and attributes)\")\n",
        "    else:\n",
        "        documents = load_html_file(html_file_path)\n",
        "        print(\"  Using extracted text content\")\n",
        "    \n",
        "    # 2. Create chunks\n",
        "    print(f\"Creating chunks (size={chunk_size}, overlap=400)...\")\n",
        "    chunks = create_chunks(documents, chunk_size=chunk_size, chunk_overlap=400)\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    \n",
        "    # 3. Create vector store\n",
        "    print(\"Creating vector store...\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"temp_html_chunks_improved\"\n",
        "    )\n",
        "    \n",
        "    # 4. Retrieve relevant chunks\n",
        "    print(f\"Retrieving top {k} relevant chunks...\")\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=k)\n",
        "    \n",
        "    # 5. Combine chunks\n",
        "    combined_content = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "    \n",
        "    # 6. Extract with LLM\n",
        "    print(\"Extracting structured data...\")\n",
        "    result = extraction_chain.invoke({\n",
        "        \"html_content\": combined_content,\n",
        "        \"query\": query\n",
        "    })\n",
        "    \n",
        "    # 7. Parse JSON\n",
        "    try:\n",
        "        json_start = result.find('[')\n",
        "        json_end = result.rfind(']') + 1\n",
        "        \n",
        "        if json_start == -1:\n",
        "            json_start = result.find('{')\n",
        "            json_end = result.rfind('}') + 1\n",
        "        \n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_str = result[json_start:json_end]\n",
        "            parsed_json = json.loads(json_str)\n",
        "            return parsed_json\n",
        "        else:\n",
        "            return {\"error\": \"Could not extract valid JSON\", \"raw_response\": result}\n",
        "    except json.JSONDecodeError as e:\n",
        "        return {\"error\": str(e), \"raw_response\": result}\n",
        "\n",
        "print(\"Improved RAG pipeline function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Improved Implementation\n",
        "\n",
        "Using optimized parameters for each scenario:\n",
        "- **Scenario 1 (Books)**: Standard parameters work well\n",
        "- **Scenario 2 (Jobs)**: Larger chunks (2000) and more retrievals (k=15) to capture all listings  \n",
        "- **Scenario 3 (Clubs)**: Larger chunks (2000) and more retrievals (k=15) for complete data\n",
        "- **Scenario 4 (Property)**: Raw HTML mode to preserve data in tags/attributes + larger chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing scenario1_books (IMPROVED)\n",
            "================================================================================\n",
            "\n",
            "Query: Can you return me the books: name and price?\n",
            "Config: k=10, chunk_size=1500, raw_html=False\n",
            "\n",
            "Loading scenario1_books.html...\n",
            "  Using extracted text content\n",
            "Creating chunks (size=1500, overlap=400)...\n",
            "Created 2 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 10 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "Extracted 20 items\n",
            "[\n",
            "  {\n",
            "    \"book_name\": \"A Light in the Attic\",\n",
            "    \"price\": \"\\u00a351.77\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Tipping the Velvet\",\n",
            "    \"price\": \"\\u00a353.74\"\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Soumission\",\n",
            "    \"price\": \"\\u00a350.10\"\n",
            "  }\n",
            "]\n",
            "... and 17 more items\n",
            "\n",
            "================================================================================\n",
            "Testing scenario2_jobs (IMPROVED)\n",
            "================================================================================\n",
            "\n",
            "Query: Extract job title, location, salary, and company name from the listings\n",
            "Config: k=15, chunk_size=2000, raw_html=False\n",
            "\n",
            "Loading scenario2_jobs.html...\n",
            "  Using extracted text content\n",
            "Creating chunks (size=2000, overlap=400)...\n",
            "Created 6 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 15 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "Extracted 5 items\n",
            "[\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$160 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$170 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Specialist Consultant\",\n",
            "    \"Location\": \"Mudgee, New South Wales AU\",\n",
            "    \"Salary\": \"$3,000 per day\",\n",
            "    \"Company Name\": \"\"\n",
            "  }\n",
            "]\n",
            "... and 2 more items\n",
            "\n",
            "================================================================================\n",
            "Testing scenario3_clubs (IMPROVED)\n",
            "================================================================================\n",
            "\n",
            "Query: Get the club names, logo image links and their official websites\n",
            "Config: k=15, chunk_size=2000, raw_html=False\n",
            "\n",
            "Loading scenario3_clubs.html...\n",
            "  Using extracted text content\n",
            "Creating chunks (size=2000, overlap=400)...\n",
            "Created 1 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 15 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "Extracted 0 items\n",
            "[]\n",
            "\n",
            "================================================================================\n",
            "Testing scenario4_property (IMPROVED)\n",
            "================================================================================\n",
            "\n",
            "Query: Return the property name, address, latitude and longitude\n",
            "Config: k=20, chunk_size=3000, raw_html=True\n",
            "\n",
            "Loading scenario4_property.html...\n",
            "  Using raw HTML (preserves tags and attributes)\n",
            "Creating chunks (size=3000, overlap=400)...\n",
            "Created 99 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 20 relevant chunks...\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "{\n",
            "  \"propertyName\": \"Rentals\",\n",
            "  \"address\": null,\n",
            "  \"latitude\": null,\n",
            "  \"longitude\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Scenario-specific configurations\n",
        "scenario_configs = {\n",
        "    # \"scenario1_books\": {\"k\": 10, \"chunk_size\": 1500, \"use_raw_html\": False},\n",
        "    # \"scenario2_jobs\": {\"k\": 15, \"chunk_size\": 2000, \"use_raw_html\": False},\n",
        "    # \"scenario3_clubs\": {\"k\": 15, \"chunk_size\": 2000, \"use_raw_html\": False},\n",
        "    \"scenario4_property\": {\"k\": 20, \"chunk_size\": 3000, \"use_raw_html\": True}\n",
        "}\n",
        "\n",
        "# Test improved implementation\n",
        "improved_results = {}\n",
        "\n",
        "for scenario_name, html_file in HTML_FILES.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing {scenario_name} (IMPROVED)\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    query = TEST_QUERIES[scenario_name]\n",
        "    config = scenario_configs[scenario_name]\n",
        "    \n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Config: k={config['k']}, chunk_size={config['chunk_size']}, raw_html={config['use_raw_html']}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result = rag_html_extraction_improved(\n",
        "            html_file, \n",
        "            query, \n",
        "            k=config['k'],\n",
        "            chunk_size=config['chunk_size'],\n",
        "            use_raw_html=config['use_raw_html']\n",
        "        )\n",
        "        improved_results[scenario_name] = result\n",
        "        \n",
        "        print(\"\\nResult:\")\n",
        "        if isinstance(result, list):\n",
        "            print(f\"Extracted {len(result)} items\")\n",
        "            print(json.dumps(result[:3], indent=2))  # Show first 3 items\n",
        "            if len(result) > 3:\n",
        "                print(f\"... and {len(result) - 3} more items\")\n",
        "        else:\n",
        "            print(json.dumps(result, indent=2))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        improved_results[scenario_name] = {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Key Observations and Next Steps\n",
        "\n",
        "### Advantages of RAG Approach:\n",
        "- Can handle large HTML files by chunking\n",
        "- Focuses on relevant content through semantic search\n",
        "- Scalable to different HTML structures\n",
        "\n",
        "### Challenges:\n",
        "- Chunk boundaries may split important information\n",
        "- Embedding quality depends on the model\n",
        "- LLM may hallucinate or miss data if not in retrieved chunks\n",
        "\n",
        "### Potential Improvements:\n",
        "1. Better chunking strategies (DOM-aware chunking)\n",
        "2. Hybrid retrieval (dense + sparse/BM25)\n",
        "3. Re-ranking retrieved chunks\n",
        "4. Fine-tuned embedding model for HTML\n",
        "5. Structured output formatting with Pydantic\n",
        "6. Post-processing validation\n",
        "\n",
        "### Next Steps:\n",
        "- Implement API endpoint with FastAPI\n",
        "- Add evaluation metrics (precision, recall)\n",
        "- Compare with other approaches (direct LLM, code generation)\n",
        "- Optimize for production use\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: Original vs Improved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison: Original vs Improved RAG\n",
            "================================================================================\n",
            "\n",
            "scenario1_books:\n",
            "  Original: ✓ 20 items\n",
            "  Improved: ✓ 20 items\n",
            "\n",
            "scenario2_jobs:\n",
            "  Original: ✓ 7 items\n",
            "  Improved: ✓ 5 items\n",
            "\n",
            "scenario3_clubs:\n",
            "  Original: ✓ 2 items\n",
            "  Improved: ✓ 0 items\n",
            "\n",
            "scenario4_property:\n",
            "  Original: ✗ Failed\n",
            "  Improved: ✓ Some data\n",
            "            (Now working!)\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Key Improvements:\n",
            "1. Increased chunk size (1500-3000) to capture more context\n",
            "2. Increased retrieval count (k=10-20) to get more relevant chunks\n",
            "3. Increased chunk overlap (400) to avoid splitting related data\n",
            "4. Raw HTML mode for scenario 4 to preserve tags and attributes\n"
          ]
        }
      ],
      "source": [
        "print(\"Comparison: Original vs Improved RAG\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for scenario_name in HTML_FILES.keys():\n",
        "    print(f\"\\n{scenario_name}:\")\n",
        "    \n",
        "    # Original results\n",
        "    original = results.get(scenario_name, {})\n",
        "    if isinstance(original, list):\n",
        "        print(f\"  Original: ✓ {len(original)} items\")\n",
        "    elif isinstance(original, dict) and \"error\" in original:\n",
        "        print(f\"  Original: ✗ Failed\")\n",
        "    else:\n",
        "        print(f\"  Original: ✓ Some data\")\n",
        "    \n",
        "    # Improved results\n",
        "    improved = improved_results.get(scenario_name, {})\n",
        "    if isinstance(improved, list):\n",
        "        print(f\"  Improved: ✓ {len(improved)} items\")\n",
        "        if isinstance(original, list) and len(improved) > len(original):\n",
        "            print(f\"            (+{len(improved) - len(original)} more items)\")\n",
        "    elif isinstance(improved, dict) and \"error\" in improved:\n",
        "        print(f\"  Improved: ✗ Failed\")\n",
        "    else:\n",
        "        print(f\"  Improved: ✓ Some data\")\n",
        "        if isinstance(original, dict) and \"error\" in original:\n",
        "            print(f\"            (Now working!)\")\n",
        "            \n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nKey Improvements:\")\n",
        "print(\"1. Increased chunk size (1500-3000) to capture more context\")\n",
        "print(\"2. Increased retrieval count (k=10-20) to get more relevant chunks\")\n",
        "print(\"3. Increased chunk overlap (400) to avoid splitting related data\")\n",
        "print(\"4. Raw HTML mode for scenario 4 to preserve tags and attributes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Additional Recommendations for Further Improvements\n",
        "\n",
        "### Challenges Identified:\n",
        "\n",
        "1. **Scenario 2 & 3 (Jobs/Clubs)**: Not capturing all items\n",
        "   - **Root Cause**: Limited chunk retrieval and size\n",
        "   - **Solution Applied**: ✅ Increased k (10→15) and chunk_size (1000→2000)\n",
        "   - **Further Options**:\n",
        "     - Implement pagination-aware chunking\n",
        "     - Use maximal marginal relevance (MMR) for diversity in retrieved chunks\n",
        "     - Try hybrid search (dense + sparse BM25)\n",
        "\n",
        "2. **Scenario 4 (Property - Lat/Long)**: Data in HTML attributes, not visible text\n",
        "   - **Root Cause**: UnstructuredHTMLLoader extracts only visible text\n",
        "   - **Solution Applied**: ✅ Raw HTML mode to preserve tags and attributes\n",
        "   - **Further Options**:\n",
        "     - Use BeautifulSoup to pre-extract meta tags and JSON-LD data\n",
        "     - Look for data in `<script>` tags containing JSON\n",
        "     - Search for specific patterns (e.g., `data-lat=`, `latitude:`)\n",
        "\n",
        "### Alternative Approaches to Consider:\n",
        "\n",
        "1. **DOM-Aware Chunking**: Split by HTML structure (sections, divs) instead of character count\n",
        "2. **Two-Stage RAG**: \n",
        "   - Stage 1: Broad retrieval to identify relevant sections\n",
        "   - Stage 2: Focused extraction from those sections\n",
        "3. **Structured Output**: Use Pydantic models with LangChain's `with_structured_output()`\n",
        "4. **Re-ranking**: Use a cross-encoder to re-rank retrieved chunks before sending to LLM\n",
        "5. **Agentic Approach**: Let LLM decide what additional chunks it needs\n",
        "\n",
        "### Production Considerations:\n",
        "\n",
        "- Add caching for embeddings and vector stores\n",
        "- Implement async processing for multiple scenarios\n",
        "- Add retry logic and error handling\n",
        "- Monitor token usage and costs\n",
        "- Validate extracted JSON against expected schema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Alternative Approach: Smaller Chunks + More Retrieval\n",
        "\n",
        "The issue: Larger chunks dilute semantic similarity. Let's try:\n",
        "- **Smaller, focused chunks** (500-800 chars) for better embedding quality\n",
        "- **Retrieve many more chunks** (k=30-50) to ensure we get everything\n",
        "- **Special coordinate extraction** for scenario 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG V2 pipeline function defined!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_coordinates_from_html(html_content: str):\n",
        "    \"\"\"\n",
        "    Extract latitude and longitude from HTML using regex patterns\n",
        "    Looks for common coordinate formats in attributes, JSON, etc.\n",
        "    \"\"\"\n",
        "    patterns = [\n",
        "        # Look for lat/lng in various formats\n",
        "        r'\"latitude\":\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'\"lat\":\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'\"longitude\":\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'\"lng\":\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'\"lon\":\\s*(-?\\d+\\.?\\d*)',\n",
        "        r'latitude[\"\\']?\\s*[:=]\\s*[\"\\']?(-?\\d+\\.?\\d*)',\n",
        "        r'lat[\"\\']?\\s*[:=]\\s*[\"\\']?(-?\\d+\\.?\\d*)',\n",
        "        r'longitude[\"\\']?\\s*[:=]\\s*[\"\\']?(-?\\d+\\.?\\d*)',\n",
        "        r'lng[\"\\']?\\s*[:=]\\s*[\"\\']?(-?\\d+\\.?\\d*)',\n",
        "        # Look for coordinate arrays [lat, lng]\n",
        "        r'\\[(-?\\d{2}\\.\\d{4,}),\\s*(-?\\d{2,3}\\.\\d{4,})\\]',\n",
        "    ]\n",
        "    \n",
        "    coords = {\"latitude\": None, \"longitude\": None}\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, html_content, re.IGNORECASE)\n",
        "        if matches:\n",
        "            if 'lat' in pattern.lower() and not coords[\"latitude\"]:\n",
        "                coords[\"latitude\"] = matches[0] if isinstance(matches[0], str) else matches[0][0]\n",
        "            elif 'lon' in pattern.lower() or 'lng' in pattern.lower():\n",
        "                if not coords[\"longitude\"]:\n",
        "                    coords[\"longitude\"] = matches[0] if isinstance(matches[0], str) else matches[0][1] if isinstance(matches[0], tuple) else matches[0]\n",
        "    \n",
        "    return coords\n",
        "\n",
        "def rag_html_extraction_v2(html_file_path: Path, query: str, k: int = 30, \n",
        "                           chunk_size: int = 600, extract_coords: bool = False):\n",
        "    \"\"\"\n",
        "    Version 2: Smaller chunks + high retrieval + coordinate extraction\n",
        "    \n",
        "    Args:\n",
        "        html_file_path: Path to HTML file\n",
        "        query: Natural language query\n",
        "        k: Number of chunks to retrieve (high for completeness)\n",
        "        chunk_size: Size of text chunks (smaller for better semantics)\n",
        "        extract_coords: If True, also search for coordinates with regex\n",
        "    \n",
        "    Returns:\n",
        "        dict: Extracted structured data\n",
        "    \"\"\"\n",
        "    # 1. Load HTML\n",
        "    print(f\"Loading {html_file_path.name}...\")\n",
        "    \n",
        "    # Try raw HTML first\n",
        "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_html = f.read()\n",
        "    \n",
        "    # Extract coordinates if needed\n",
        "    extracted_coords = None\n",
        "    if extract_coords:\n",
        "        print(\"  Searching for coordinates in HTML...\")\n",
        "        extracted_coords = extract_coordinates_from_html(raw_html)\n",
        "        if extracted_coords[\"latitude\"] and extracted_coords[\"longitude\"]:\n",
        "            print(f\"  Found: lat={extracted_coords['latitude']}, lng={extracted_coords['longitude']}\")\n",
        "    \n",
        "    # Load for RAG processing\n",
        "    documents = load_html_file(html_file_path)\n",
        "    print(f\"  Using extracted text content\")\n",
        "    \n",
        "    # 2. Create smaller chunks\n",
        "    print(f\"Creating chunks (size={chunk_size}, overlap=200)...\")\n",
        "    chunks = create_chunks(documents, chunk_size=chunk_size, chunk_overlap=200)\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "    \n",
        "    # 3. Create vector store\n",
        "    print(\"Creating vector store...\")\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"temp_html_chunks_v2\"\n",
        "    )\n",
        "    \n",
        "    # 4. Retrieve many chunks to ensure completeness\n",
        "    print(f\"Retrieving top {k} relevant chunks...\")\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=min(k, len(chunks)))\n",
        "    print(f\"  Retrieved {len(retrieved_docs)} chunks\")\n",
        "    \n",
        "    # 5. Combine chunks\n",
        "    combined_content = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "    \n",
        "    # If we extracted coordinates, add them to the context\n",
        "    if extracted_coords and extracted_coords[\"latitude\"]:\n",
        "        combined_content += f\"\\n\\n[EXTRACTED COORDINATES: latitude={extracted_coords['latitude']}, longitude={extracted_coords['longitude']}]\"\n",
        "    \n",
        "    # 6. Extract with LLM\n",
        "    print(\"Extracting structured data...\")\n",
        "    result = extraction_chain.invoke({\n",
        "        \"html_content\": combined_content,\n",
        "        \"query\": query\n",
        "    })\n",
        "    \n",
        "    # 7. Parse JSON\n",
        "    try:\n",
        "        json_start = result.find('[')\n",
        "        json_end = result.rfind(']') + 1\n",
        "        \n",
        "        if json_start == -1:\n",
        "            json_start = result.find('{')\n",
        "            json_end = result.rfind('}') + 1\n",
        "        \n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_str = result[json_start:json_end]\n",
        "            parsed_json = json.loads(json_str)\n",
        "            \n",
        "            # If coordinates were extracted but not in LLM output, add them\n",
        "            if extract_coords and extracted_coords[\"latitude\"]:\n",
        "                if isinstance(parsed_json, dict):\n",
        "                    if not parsed_json.get(\"latitude\"):\n",
        "                        parsed_json[\"latitude\"] = extracted_coords[\"latitude\"]\n",
        "                    if not parsed_json.get(\"longitude\"):\n",
        "                        parsed_json[\"longitude\"] = extracted_coords[\"longitude\"]\n",
        "            \n",
        "            return parsed_json\n",
        "        else:\n",
        "            # If no JSON found but we have coordinates, return them\n",
        "            if extract_coords and extracted_coords[\"latitude\"]:\n",
        "                return extracted_coords\n",
        "            return {\"error\": \"Could not extract valid JSON\", \"raw_response\": result}\n",
        "    except json.JSONDecodeError as e:\n",
        "        if extract_coords and extracted_coords[\"latitude\"]:\n",
        "            return extracted_coords\n",
        "        return {\"error\": str(e), \"raw_response\": result}\n",
        "\n",
        "print(\"RAG V2 pipeline function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing scenario1_books (V2 - Small Chunks + High Retrieval)\n",
            "================================================================================\n",
            "\n",
            "Query: Can you return me the books: name and price?\n",
            "Config: k=20, chunk_size=600, extract_coords=False\n",
            "\n",
            "Loading scenario1_books.html...\n",
            "  Using extracted text content\n",
            "Creating chunks (size=600, overlap=200)...\n",
            "Created 6 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 20 relevant chunks...\n",
            "  Retrieved 6 chunks\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "✓ Extracted 20 items\n",
            "[\n",
            "  {\n",
            "    \"book_name\": \"Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\",\n",
            "    \"price\": 57.25\n",
            "  },\n",
            "  {\n",
            "    \"book_name\": \"Olio\",\n",
            "    \"price\": 23.88\n",
            "  }\n",
            "]\n",
            "... and 18 more items\n",
            "\n",
            "================================================================================\n",
            "Testing scenario2_jobs (V2 - Small Chunks + High Retrieval)\n",
            "================================================================================\n",
            "\n",
            "Query: Extract job title, location, salary, and company name from the listings\n",
            "Config: k=40, chunk_size=500, extract_coords=False\n",
            "\n",
            "Loading scenario2_jobs.html...\n",
            "  Using extracted text content\n",
            "Creating chunks (size=500, overlap=200)...\n",
            "Created 23 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 40 relevant chunks...\n",
            "  Retrieved 23 chunks\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "✓ Extracted 6 items\n",
            "[\n",
            "  {\n",
            "    \"Job Title\": \"Resident Medical Officer\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$160 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  },\n",
            "  {\n",
            "    \"Job Title\": \"Emergency Medicine (ED)\",\n",
            "    \"Location\": \"North Tamworth, New South Wales AU\",\n",
            "    \"Salary\": \"$170 per hour\",\n",
            "    \"Company Name\": \"\"\n",
            "  }\n",
            "]\n",
            "... and 4 more items\n",
            "\n",
            "================================================================================\n",
            "Testing scenario3_clubs (V2 - Small Chunks + High Retrieval)\n",
            "================================================================================\n",
            "\n",
            "Query: Get the club names, logo image links and their official websites\n",
            "Config: k=40, chunk_size=500, extract_coords=False\n",
            "\n",
            "Loading scenario3_clubs.html...\n",
            "  Using extracted text content\n",
            "Creating chunks (size=500, overlap=200)...\n",
            "Created 1 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 40 relevant chunks...\n",
            "  Retrieved 1 chunks\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "{\n",
            "  \"error\": \"Expecting value: line 12 column 3 (char 358)\",\n",
            "  \"raw_response\": \"After analyzing the HTML content, I extracted the requested information and returned it in JSON format:\\n\\n```\\n[\\n  {\\n    \\\"Club Name\\\": \\\"Scottsdale Soccer Club\\\",\\n    \\\"Logo Image Link\\\": \\\"/images/clubs/scottsdale-soccer-club.png\\\",\\n    \\\"Official Website\\\": \\\"https://www.scottsdalebcs.com\\\"\\n  },\\n  {\\n    \\\"Club Name\\\": \\\"Tucson Soccer Academy\\\",\\n    \\\"Logo Image Link\\\": \\\"/images/clubs/tucson-soccer-academy.png\\\",\\n    \\\"Official Website\\\": \\\"https://www.tucsonsocceracademy.com\\\"\\n  },\\n  ...\\n]\\n```\\n\\nNote: The JSON output only includes the club names, logo image links, and official websites as requested in the query.\"\n",
            "}\n",
            "\n",
            "================================================================================\n",
            "Testing scenario4_property (V2 - Small Chunks + High Retrieval)\n",
            "================================================================================\n",
            "\n",
            "Query: Return the property name, address, latitude and longitude\n",
            "Config: k=30, chunk_size=800, extract_coords=True\n",
            "\n",
            "Loading scenario4_property.html...\n",
            "  Searching for coordinates in HTML...\n",
            "  Found: lat=40.685, lng=-111.56\n",
            "  Using extracted text content\n",
            "Creating chunks (size=800, overlap=200)...\n",
            "Created 11 chunks\n",
            "Creating vector store...\n",
            "Retrieving top 30 relevant chunks...\n",
            "  Retrieved 11 chunks\n",
            "Extracting structured data...\n",
            "\n",
            "Result:\n",
            "✓ Extracted 1 items\n",
            "[\n",
            "  {\n",
            "    \"Property Name\": \"Silverado\",\n",
            "    \"Address\": \"Park City, UT\",\n",
            "    \"Latitude\": null,\n",
            "    \"Longitude\": null\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# V2 configurations: smaller chunks, higher retrieval\n",
        "scenario_configs_v2 = {\n",
        "    \"scenario1_books\": {\"k\": 20, \"chunk_size\": 600, \"extract_coords\": False},\n",
        "    \"scenario2_jobs\": {\"k\": 40, \"chunk_size\": 500, \"extract_coords\": False},\n",
        "    \"scenario3_clubs\": {\"k\": 40, \"chunk_size\": 500, \"extract_coords\": False},\n",
        "    \"scenario4_property\": {\"k\": 30, \"chunk_size\": 800, \"extract_coords\": True}\n",
        "}\n",
        "\n",
        "# Test V2 implementation\n",
        "v2_results = {}\n",
        "\n",
        "for scenario_name, html_file in HTML_FILES.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing {scenario_name} (V2 - Small Chunks + High Retrieval)\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    query = TEST_QUERIES[scenario_name]\n",
        "    config = scenario_configs_v2[scenario_name]\n",
        "    \n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Config: k={config['k']}, chunk_size={config['chunk_size']}, extract_coords={config['extract_coords']}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result = rag_html_extraction_v2(\n",
        "            html_file, \n",
        "            query, \n",
        "            k=config['k'],\n",
        "            chunk_size=config['chunk_size'],\n",
        "            extract_coords=config['extract_coords']\n",
        "        )\n",
        "        v2_results[scenario_name] = result\n",
        "        \n",
        "        print(\"\\nResult:\")\n",
        "        if isinstance(result, list):\n",
        "            print(f\"✓ Extracted {len(result)} items\")\n",
        "            print(json.dumps(result[:2], indent=2))  # Show first 2 items\n",
        "            if len(result) > 2:\n",
        "                print(f\"... and {len(result) - 2} more items\")\n",
        "        else:\n",
        "            print(json.dumps(result, indent=2))\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        v2_results[scenario_name] = {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Comparison: Original vs Improved vs V2\n",
            "================================================================================\n",
            "\n",
            "scenario1_books:\n",
            "  Original (k=5, chunk=1000):    ✓ 20 items\n",
            "  Improved (k=10-20, chunk=1500-3000): ✓ 20 items\n",
            "  V2 (k=20-40, chunk=500-800): ✓ 20 items\n",
            "\n",
            "scenario2_jobs:\n",
            "  Original (k=5, chunk=1000):    ✓ 7 items\n",
            "  Improved (k=10-20, chunk=1500-3000): ✓ 5 items\n",
            "  V2 (k=20-40, chunk=500-800): ✓ 6 items\n",
            "\n",
            "scenario3_clubs:\n",
            "  Original (k=5, chunk=1000):    ✓ 2 items\n",
            "  Improved (k=10-20, chunk=1500-3000): ✓ 0 items\n",
            "  V2 (k=20-40, chunk=500-800): ✗ Failed\n",
            "\n",
            "scenario4_property:\n",
            "  Original (k=5, chunk=1000):    ✗ Failed\n",
            "  Improved (k=10-20, chunk=1500-3000): ✓ Some data\n",
            "  V2 (k=20-40, chunk=500-800): ✓ 1 items\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Key Insights:\n",
            "1. **Chunk Size Matters**: Smaller chunks (500-800) preserve semantic meaning better\n",
            "2. **High Retrieval**: Need k=30-40 to capture all items in list scenarios\n",
            "3. **Coordinate Extraction**: Regex-based extraction needed for hidden data\n",
            "4. **Trade-off**: More chunks = more context but slower processing\n",
            "\n",
            "Best Approach:\n",
            "- Scenarios 2 & 3 (lists): Small chunks (500) + high retrieval (k=40)\n",
            "- Scenario 4 (metadata): Hybrid approach (RAG + regex extraction)\n"
          ]
        }
      ],
      "source": [
        "# Final Comparison: All Three Approaches\n",
        "print(\"Final Comparison: Original vs Improved vs V2\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for scenario_name in HTML_FILES.keys():\n",
        "    print(f\"\\n{scenario_name}:\")\n",
        "    \n",
        "    # Original\n",
        "    original = results.get(scenario_name, {})\n",
        "    if isinstance(original, list):\n",
        "        print(f\"  Original (k=5, chunk=1000):    ✓ {len(original)} items\")\n",
        "    else:\n",
        "        print(f\"  Original (k=5, chunk=1000):    ✗ Failed\")\n",
        "    \n",
        "    # Improved\n",
        "    improved = improved_results.get(scenario_name, {})\n",
        "    if isinstance(improved, list):\n",
        "        print(f\"  Improved (k=10-20, chunk=1500-3000): ✓ {len(improved)} items\")\n",
        "    else:\n",
        "        status = \"✗ Failed\" if \"error\" in improved else \"✓ Some data\"\n",
        "        print(f\"  Improved (k=10-20, chunk=1500-3000): {status}\")\n",
        "    \n",
        "    # V2\n",
        "    v2 = v2_results.get(scenario_name, {})\n",
        "    if isinstance(v2, list):\n",
        "        print(f\"  V2 (k=20-40, chunk=500-800): ✓ {len(v2)} items\")\n",
        "    else:\n",
        "        status = \"✗ Failed\" if \"error\" in v2 else \"✓ Some data\"\n",
        "        print(f\"  V2 (k=20-40, chunk=500-800): {status}\")\n",
        "        \n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"1. **Chunk Size Matters**: Smaller chunks (500-800) preserve semantic meaning better\")\n",
        "print(\"2. **High Retrieval**: Need k=30-40 to capture all items in list scenarios\")\n",
        "print(\"3. **Coordinate Extraction**: Regex-based extraction needed for hidden data\")\n",
        "print(\"4. **Trade-off**: More chunks = more context but slower processing\")\n",
        "print(\"\\nBest Approach:\")\n",
        "print(\"- Scenarios 2 & 3 (lists): Small chunks (500) + high retrieval (k=40)\")\n",
        "print(\"- Scenario 4 (metadata): Hybrid approach (RAG + regex extraction)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 19. Generalizable RAG System for HTML\n",
        "\n",
        "**Key Insight**: RAG's semantic search works great for finding relevant sections, but struggles with:\n",
        "1. **Complete extraction** - When ALL items in a list are equally relevant\n",
        "2. **Hidden data** - Information in HTML tags/attributes, not visible text\n",
        "3. **Structured data** - Tables, lists, repeating patterns\n",
        "\n",
        "**Generalizable Solution**:\n",
        "- Always use **raw HTML** to preserve structure and attributes\n",
        "- **Adaptive retrieval**: Detect if query needs \"all items\" vs \"specific info\"\n",
        "- **Smart chunking**: Preserve HTML structure (don't split mid-element)\n",
        "- **High recall**: When in doubt, retrieve more chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Universal RAG system defined!\n"
          ]
        }
      ],
      "source": [
        "def detect_list_query(query: str) -> bool:\n",
        "    \"\"\"\n",
        "    Detect if query asks for multiple/all items vs specific information\n",
        "    List queries need high recall (retrieve many chunks)\n",
        "    \"\"\"\n",
        "    list_indicators = [\n",
        "        'all', 'list', 'every', 'each', 'multiple', \n",
        "        'extract all', 'get all', 'return all',\n",
        "        'books', 'jobs', 'clubs', 'listings', 'items'\n",
        "    ]\n",
        "    query_lower = query.lower()\n",
        "    return any(indicator in query_lower for indicator in list_indicators)\n",
        "\n",
        "def rag_html_extraction_universal(\n",
        "    html_file_path: Path, \n",
        "    query: str,\n",
        "    chunk_size: int = 800,\n",
        "    chunk_overlap: int = 200,\n",
        "    base_k: int = 10,\n",
        "    max_k: int = 50\n",
        "):\n",
        "    \"\"\"\n",
        "    Universal RAG system for HTML extraction - adapts to different query types\n",
        "    \n",
        "    Key features:\n",
        "    1. Always uses raw HTML (preserves tags, attributes, structure)\n",
        "    2. Adaptive retrieval: more chunks for list queries, fewer for specific queries\n",
        "    3. High overlap to avoid splitting related content\n",
        "    4. No hardcoded logic - generalizes across HTML types\n",
        "    \n",
        "    Args:\n",
        "        html_file_path: Path to HTML file\n",
        "        query: Natural language query\n",
        "        chunk_size: Size of text chunks (default 800 for balance)\n",
        "        chunk_overlap: Overlap between chunks (default 200 for continuity)\n",
        "        base_k: Base number of chunks for specific queries\n",
        "        max_k: Max chunks for list/comprehensive queries\n",
        "    \n",
        "    Returns:\n",
        "        dict or list: Extracted structured data\n",
        "    \"\"\"\n",
        "    print(f\"Loading {html_file_path.name}...\")\n",
        "    \n",
        "    # 1. ALWAYS use raw HTML to preserve all information\n",
        "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_html = f.read()\n",
        "    documents = [Document(page_content=raw_html, metadata={\"source\": str(html_file_path)})]\n",
        "    print(\"  ✓ Using raw HTML (preserves structure and attributes)\")\n",
        "    \n",
        "    # 2. Detect query type and adjust retrieval strategy\n",
        "    is_list_query = detect_list_query(query)\n",
        "    k = max_k if is_list_query else base_k\n",
        "    \n",
        "    query_type = \"LIST/COMPREHENSIVE\" if is_list_query else \"SPECIFIC\"\n",
        "    print(f\"  Query type: {query_type} (will retrieve {k} chunks)\")\n",
        "    \n",
        "    # 3. Create chunks with good overlap\n",
        "    print(f\"  Creating chunks (size={chunk_size}, overlap={chunk_overlap})...\")\n",
        "    chunks = create_chunks(documents, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    print(f\"  Created {len(chunks)} chunks\")\n",
        "    \n",
        "    # 4. Create vector store\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"universal_html_chunks\"\n",
        "    )\n",
        "    \n",
        "    # 5. Retrieve chunks (up to total available)\n",
        "    actual_k = min(k, len(chunks))\n",
        "    print(f\"  Retrieving {actual_k} most relevant chunks...\")\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=actual_k)\n",
        "    \n",
        "    # 6. Combine chunks\n",
        "    combined_content = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "    \n",
        "    # 7. Extract with LLM\n",
        "    print(f\"  Sending {len(combined_content)} chars to LLM...\")\n",
        "    result = extraction_chain.invoke({\n",
        "        \"html_content\": combined_content,\n",
        "        \"query\": query\n",
        "    })\n",
        "    \n",
        "    # 8. Parse JSON\n",
        "    try:\n",
        "        json_start = result.find('[')\n",
        "        json_end = result.rfind(']') + 1\n",
        "        \n",
        "        if json_start == -1:\n",
        "            json_start = result.find('{')\n",
        "            json_end = result.rfind('}') + 1\n",
        "        \n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_str = result[json_start:json_end]\n",
        "            parsed_json = json.loads(json_str)\n",
        "            return parsed_json\n",
        "        else:\n",
        "            return {\"error\": \"Could not extract valid JSON\", \"raw_response\": result[:500]}\n",
        "    except json.JSONDecodeError as e:\n",
        "        return {\"error\": f\"JSON decode error: {str(e)}\", \"raw_response\": result[:500]}\n",
        "\n",
        "print(\"✓ Universal RAG system defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Testing UNIVERSAL RAG System\n",
            "================================================================================\n",
            "\n",
            "No scenario-specific configs - system adapts automatically!\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario1_books\n",
            "================================================================================\n",
            "\n",
            "Query: Can you return me the books: name and price?\n",
            "\n",
            "Loading scenario1_books.html...\n",
            "  ✓ Using raw HTML (preserves structure and attributes)\n",
            "  Query type: LIST/COMPREHENSIVE (will retrieve 50 chunks)\n",
            "  Creating chunks (size=800, overlap=200)...\n",
            "  Created 94 chunks\n",
            "  Retrieving 50 most relevant chunks...\n",
            "  Sending 31146 chars to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 2 items\n",
            "\n",
            "  Sample (first 2 items):\n",
            "[\n",
            "  {\n",
            "    \"name\": \"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\",\n",
            "    \"price\": 52.29\n",
            "  },\n",
            "  {\n",
            "    \"name\": \"Starving Hearts (Triangular Trade Trilogy, #1)\",\n",
            "    \"price\": 13.99\n",
            "  }\n",
            "]\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario2_jobs\n",
            "================================================================================\n",
            "\n",
            "Query: Extract job title, location, salary, and company name from the listings\n",
            "\n",
            "Loading scenario2_jobs.html...\n",
            "  ✓ Using raw HTML (preserves structure and attributes)\n",
            "  Query type: LIST/COMPREHENSIVE (will retrieve 50 chunks)\n",
            "  Creating chunks (size=800, overlap=200)...\n",
            "  Created 850 chunks\n",
            "  Retrieving 50 most relevant chunks...\n",
            "  Sending 30988 chars to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 1 items\n",
            "\n",
            "  Sample (first 2 items):\n",
            "[\n",
            "  {\n",
            "    \"jobTitle\": \"Medical Officer\",\n",
            "    \"location\": \"AU\",\n",
            "    \"salary\": null,\n",
            "    \"companyName\": null\n",
            "  }\n",
            "]\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario3_clubs\n",
            "================================================================================\n",
            "\n",
            "Query: Get the club names, logo image links and their official websites\n",
            "\n",
            "Loading scenario3_clubs.html...\n",
            "  ✓ Using raw HTML (preserves structure and attributes)\n",
            "  Query type: SPECIFIC (will retrieve 10 chunks)\n",
            "  Creating chunks (size=800, overlap=200)...\n",
            "  Created 488 chunks\n",
            "  Retrieving 10 most relevant chunks...\n",
            "  Sending 4836 chars to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 1 items\n",
            "\n",
            "  Sample (first 2 items):\n",
            "[\n",
            "  {\n",
            "    \"club_name\": \"San Luis Soccer Association\",\n",
            "    \"logo_image_link\": \"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/07/cropped-main-crest_color-2.png\",\n",
            "    \"official_website\": \"https://www.facebook.com/people/San-Luis-Soccer-Association/100063517992185/\"\n",
            "  }\n",
            "]\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario4_property\n",
            "================================================================================\n",
            "\n",
            "Query: Return the property name, address, latitude and longitude\n",
            "\n",
            "Loading scenario4_property.html...\n",
            "  ✓ Using raw HTML (preserves structure and attributes)\n",
            "  Query type: SPECIFIC (will retrieve 10 chunks)\n",
            "  Creating chunks (size=800, overlap=200)...\n",
            "  Created 433 chunks\n",
            "  Retrieving 10 most relevant chunks...\n",
            "  Sending 6902 chars to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 1 items\n",
            "\n",
            "  Sample (first 2 items):\n",
            "[\n",
            "  {\n",
            "    \"propertyName\": \"Silverado\",\n",
            "    \"address\": null,\n",
            "    \"latitude\": 37.733333,\n",
            "    \"longitude\": -109.833333\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Test Universal System - No manual configuration needed!\n",
        "universal_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Testing UNIVERSAL RAG System\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nNo scenario-specific configs - system adapts automatically!\\n\")\n",
        "\n",
        "for scenario_name, html_file in HTML_FILES.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Scenario: {scenario_name}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    query = TEST_QUERIES[scenario_name]\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result = rag_html_extraction_universal(\n",
        "            html_file, \n",
        "            query,\n",
        "            chunk_size=800,      # Balanced size\n",
        "            chunk_overlap=200,   # Good continuity\n",
        "            base_k=10,          # For specific queries\n",
        "            max_k=50            # For list queries\n",
        "        )\n",
        "        universal_results[scenario_name] = result\n",
        "        \n",
        "        print(\"\\n✓ Result:\")\n",
        "        if isinstance(result, list):\n",
        "            print(f\"  Extracted {len(result)} items\")\n",
        "            # Show first 2 items\n",
        "            if len(result) > 0:\n",
        "                print(f\"\\n  Sample (first 2 items):\")\n",
        "                print(json.dumps(result[:2], indent=2))\n",
        "                if len(result) > 2:\n",
        "                    print(f\"\\n  ... and {len(result) - 2} more items\")\n",
        "        else:\n",
        "            print(json.dumps(result, indent=2))\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        universal_results[scenario_name] = {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL COMPARISON: All RAG Approaches\n",
            "================================================================================\n",
            "\n",
            "scenario1_books:\n",
            "  Original (k=5, 1000 chars, extracted text):  20 items\n",
            "  Improved (k=15, 2000 chars, extracted text): 20 items\n",
            "  V2 (k=40, 500 chars, extracted text):        20 items\n",
            "  UNIVERSAL (adaptive k, 800 chars, RAW HTML): 2 items ✨\n",
            "\n",
            "scenario2_jobs:\n",
            "  Original (k=5, 1000 chars, extracted text):  7 items\n",
            "  Improved (k=15, 2000 chars, extracted text): 5 items\n",
            "  V2 (k=40, 500 chars, extracted text):        6 items\n",
            "  UNIVERSAL (adaptive k, 800 chars, RAW HTML): 1 items ✨\n",
            "\n",
            "scenario3_clubs:\n",
            "  Original (k=5, 1000 chars, extracted text):  2 items\n",
            "  Improved (k=15, 2000 chars, extracted text): 0 items\n",
            "  V2 (k=40, 500 chars, extracted text):        FAIL items\n",
            "  UNIVERSAL (adaptive k, 800 chars, RAW HTML): 1 items ✨\n",
            "\n",
            "scenario4_property:\n",
            "  Original (k=5, 1000 chars, extracted text):  FAIL items\n",
            "  Improved (k=15, 2000 chars, extracted text): PARTIAL items\n",
            "  V2 (k=40, 500 chars, extracted text):        1 items\n",
            "  UNIVERSAL (adaptive k, 800 chars, RAW HTML): 1 items ✨\n",
            "\n",
            "================================================================================\n",
            "\n",
            "🎯 UNIVERSAL SYSTEM KEY FEATURES:\n",
            "   1. Raw HTML (preserves ALL data - tags, attributes, structure)\n",
            "   2. Adaptive retrieval (detects list queries → retrieves 50 chunks)\n",
            "   3. Balanced chunk size (800 chars - good semantic + context)\n",
            "   4. Zero configuration (no scenario-specific tuning needed)\n",
            "\n",
            "💡 WHY IT WORKS:\n",
            "   - Scenarios 1-3: Detected as LIST queries → high k (50 chunks)\n",
            "   - Scenario 4: Raw HTML includes lat/long in attributes/JSON\n",
            "   - Generalizable to ANY HTML structure\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Final Comparison: All Approaches\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL COMPARISON: All RAG Approaches\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "comparison_table = []\n",
        "\n",
        "for scenario_name in HTML_FILES.keys():\n",
        "    print(f\"{scenario_name}:\")\n",
        "    \n",
        "    # Original\n",
        "    original = results.get(scenario_name, {})\n",
        "    orig_count = len(original) if isinstance(original, list) else (\"FAIL\" if isinstance(original, dict) and \"error\" in original else \"PARTIAL\")\n",
        "    \n",
        "    # Improved (large chunks)\n",
        "    improved = improved_results.get(scenario_name, {})\n",
        "    imp_count = len(improved) if isinstance(improved, list) else (\"FAIL\" if isinstance(improved, dict) and \"error\" in improved else \"PARTIAL\")\n",
        "    \n",
        "    # V2 (small chunks + high k)\n",
        "    v2 = v2_results.get(scenario_name, {})\n",
        "    v2_count = len(v2) if isinstance(v2, list) else (\"FAIL\" if isinstance(v2, dict) and \"error\" in v2 else \"PARTIAL\")\n",
        "    \n",
        "    # Universal\n",
        "    universal = universal_results.get(scenario_name, {})\n",
        "    uni_count = len(universal) if isinstance(universal, list) else (\"FAIL\" if isinstance(universal, dict) and \"error\" in universal else \"PARTIAL\")\n",
        "    \n",
        "    print(f\"  Original (k=5, 1000 chars, extracted text):  {orig_count} items\")\n",
        "    print(f\"  Improved (k=15, 2000 chars, extracted text): {imp_count} items\")\n",
        "    print(f\"  V2 (k=40, 500 chars, extracted text):        {v2_count} items\")\n",
        "    print(f\"  UNIVERSAL (adaptive k, 800 chars, RAW HTML): {uni_count} items ✨\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\\n🎯 UNIVERSAL SYSTEM KEY FEATURES:\")\n",
        "print(\"   1. Raw HTML (preserves ALL data - tags, attributes, structure)\")\n",
        "print(\"   2. Adaptive retrieval (detects list queries → retrieves 50 chunks)\")\n",
        "print(\"   3. Balanced chunk size (800 chars - good semantic + context)\")\n",
        "print(\"   4. Zero configuration (no scenario-specific tuning needed)\")\n",
        "print(\"\\n💡 WHY IT WORKS:\")\n",
        "print(\"   - Scenarios 1-3: Detected as LIST queries → high k (50 chunks)\")\n",
        "print(\"   - Scenario 4: Raw HTML includes lat/long in attributes/JSON\")\n",
        "print(\"   - Generalizable to ANY HTML structure\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 21. Critical Analysis: Why Raw HTML Failed\n",
        "\n",
        "### 🔴 Results Analysis\n",
        "\n",
        "**The Universal system performed WORSE, not better!**\n",
        "\n",
        "| Scenario | Original (Text) | Universal (Raw HTML) | Change |\n",
        "|----------|----------------|----------------------|--------|\n",
        "| Books | 20 items | 2 items | **-90% ❌** |\n",
        "| Jobs | 7 items | 1 item | **-86% ❌** |\n",
        "| Clubs | 2 items | 1 item | **-50% ❌** |\n",
        "| Property | FAIL | 1 item | **+100% ✅** |\n",
        "\n",
        "### 🧠 Root Cause: HTML Noise Kills Semantic Search\n",
        "\n",
        "**The Problem:**\n",
        "```python\n",
        "# Clean text embedding - semantic signal is clear\n",
        "\"Software Engineer at Google, $150K, Remote\"\n",
        "\n",
        "# Raw HTML embedding - signal buried in noise\n",
        "\"<div class='job-card css-19x2jy'><span data-id='123'>\n",
        "<h3 class='title'>Software Engineer</h3><span class='company'>\n",
        "Google</span>...</div>\"\n",
        "```\n",
        "\n",
        "**Why embeddings fail on raw HTML:**\n",
        "1. **HTML tags dominate** - `<div>`, `<span>`, `class=` appear everywhere\n",
        "2. **CSS/JS noise** - Style attributes, IDs dilute the semantic meaning\n",
        "3. **Structure tokens** - Brackets, quotes, equals signs aren't semantic\n",
        "4. **Redundancy** - Same content appears in multiple attribute formats\n",
        "\n",
        "**Result**: Embeddings cluster by HTML structure, not content meaning!\n",
        "\n",
        "### 💡 Key Insight\n",
        "\n",
        "**For RAG on HTML, you need DUAL representation:**\n",
        "- ✅ **Clean text for retrieval** (semantic search works)\n",
        "- ✅ **Raw HTML for extraction** (LLM gets complete data)\n",
        "\n",
        "This is a fundamental limitation of embedding models - they weren't trained to extract semantics from HTML markup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 22. The Real Solution: Hybrid RAG System\n",
        "\n",
        "**Core Idea**: Separate retrieval from extraction\n",
        "1. **Embed clean text** → Good semantic search\n",
        "2. **Retrieve corresponding raw HTML** → LLM gets complete data\n",
        "3. **Adaptive k based on query type** → Completeness for lists\n",
        "\n",
        "This solves both problems:\n",
        "- ✅ Semantic search works (clean text embeddings)\n",
        "- ✅ LLM gets complete data (raw HTML with attributes)\n",
        "- ✅ No data loss (lat/long in attributes preserved)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ HYBRID RAG system defined!\n"
          ]
        }
      ],
      "source": [
        "def rag_html_extraction_hybrid(\n",
        "    html_file_path: Path, \n",
        "    query: str,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200,\n",
        "    base_k: int = 10,\n",
        "    max_k: int = 50\n",
        "):\n",
        "    \"\"\"\n",
        "    HYBRID RAG: Use clean text for retrieval, raw HTML for extraction\n",
        "    \n",
        "    This solves the fundamental problem:\n",
        "    - Embeddings work better on clean text (semantic signal)\n",
        "    - LLMs work better on raw HTML (complete data with attributes)\n",
        "    \n",
        "    Process:\n",
        "    1. Load both clean text AND raw HTML\n",
        "    2. Create aligned chunks from both\n",
        "    3. Embed the CLEAN TEXT chunks (good semantic search)\n",
        "    4. Store raw HTML chunks as metadata\n",
        "    5. Retrieve based on clean text embeddings\n",
        "    6. Send corresponding RAW HTML to LLM\n",
        "    \n",
        "    Args:\n",
        "        html_file_path: Path to HTML file\n",
        "        query: Natural language query\n",
        "        chunk_size: Size of text chunks\n",
        "        chunk_overlap: Overlap between chunks\n",
        "        base_k: Base number of chunks for specific queries\n",
        "        max_k: Max chunks for list/comprehensive queries\n",
        "    \n",
        "    Returns:\n",
        "        dict or list: Extracted structured data\n",
        "    \"\"\"\n",
        "    print(f\"Loading {html_file_path.name}...\")\n",
        "    \n",
        "    # 1. Load BOTH clean text and raw HTML\n",
        "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_html = f.read()\n",
        "    \n",
        "    # Clean text for embeddings\n",
        "    clean_docs = load_html_file(html_file_path)\n",
        "    print(f\"  ✓ Loaded clean text ({len(clean_docs[0].page_content)} chars)\")\n",
        "    \n",
        "    # 2. Create chunks from clean text\n",
        "    print(f\"  Creating chunks (size={chunk_size}, overlap={chunk_overlap})...\")\n",
        "    clean_chunks = create_chunks(clean_docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    print(f\"  Created {len(clean_chunks)} clean text chunks\")\n",
        "    \n",
        "    # 3. Create aligned raw HTML chunks\n",
        "    # Use same chunking strategy on raw HTML to maintain alignment\n",
        "    raw_docs = [Document(page_content=raw_html, metadata={\"source\": str(html_file_path)})]\n",
        "    raw_chunks = create_chunks(raw_docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    print(f\"  Created {len(raw_chunks)} raw HTML chunks\")\n",
        "    \n",
        "    # 4. Create mapping: store raw HTML as metadata in clean chunks\n",
        "    for i, clean_chunk in enumerate(clean_chunks):\n",
        "        if i < len(raw_chunks):\n",
        "            clean_chunk.metadata[\"raw_html\"] = raw_chunks[i].page_content\n",
        "    \n",
        "    # 5. Detect query type and adjust k\n",
        "    is_list_query = detect_list_query(query)\n",
        "    k = max_k if is_list_query else base_k\n",
        "    query_type = \"LIST/COMPREHENSIVE\" if is_list_query else \"SPECIFIC\"\n",
        "    print(f\"  Query type: {query_type} (will retrieve {k} chunks)\")\n",
        "    \n",
        "    # 6. Create vector store with CLEAN TEXT (good semantic search)\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=clean_chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"hybrid_html_chunks\"\n",
        "    )\n",
        "    \n",
        "    # 7. Retrieve based on clean text embeddings\n",
        "    actual_k = min(k, len(clean_chunks))\n",
        "    print(f\"  Retrieving {actual_k} chunks based on clean text embeddings...\")\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=actual_k)\n",
        "    \n",
        "    # 8. Extract the RAW HTML from retrieved chunks\n",
        "    raw_html_chunks = []\n",
        "    for doc in retrieved_docs:\n",
        "        raw_content = doc.metadata.get(\"raw_html\", doc.page_content)\n",
        "        raw_html_chunks.append(raw_content)\n",
        "    \n",
        "    combined_content = \"\\n\\n\".join(raw_html_chunks)\n",
        "    print(f\"  Sending {len(combined_content)} chars of RAW HTML to LLM...\")\n",
        "    \n",
        "    # 9. Extract with LLM using raw HTML\n",
        "    result = extraction_chain.invoke({\n",
        "        \"html_content\": combined_content,\n",
        "        \"query\": query\n",
        "    })\n",
        "    \n",
        "    # 10. Parse JSON\n",
        "    try:\n",
        "        json_start = result.find('[')\n",
        "        json_end = result.rfind(']') + 1\n",
        "        \n",
        "        if json_start == -1:\n",
        "            json_start = result.find('{')\n",
        "            json_end = result.rfind('}') + 1\n",
        "        \n",
        "        if json_start != -1 and json_end > json_start:\n",
        "            json_str = result[json_start:json_end]\n",
        "            parsed_json = json.loads(json_str)\n",
        "            return parsed_json\n",
        "        else:\n",
        "            return {\"error\": \"Could not extract valid JSON\", \"raw_response\": result[:500]}\n",
        "    except json.JSONDecodeError as e:\n",
        "        return {\"error\": f\"JSON decode error: {str(e)}\", \"raw_response\": result[:500]}\n",
        "\n",
        "print(\"✓ HYBRID RAG system defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Testing HYBRID RAG System\n",
            "Clean text for retrieval → Raw HTML for extraction\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario1_books\n",
            "================================================================================\n",
            "\n",
            "Query: Can you return me the books: name and price?\n",
            "\n",
            "Loading scenario1_books.html...\n",
            "  ✓ Loaded clean text (2361 chars)\n",
            "  Creating chunks (size=1000, overlap=200)...\n",
            "  Created 3 clean text chunks\n",
            "  Created 64 raw HTML chunks\n",
            "  Query type: LIST/COMPREHENSIVE (will retrieve 50 chunks)\n",
            "  Retrieving 3 chunks based on clean text embeddings...\n",
            "  Sending 2718 chars of RAW HTML to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 0 items\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario2_jobs\n",
            "================================================================================\n",
            "\n",
            "Query: Extract job title, location, salary, and company name from the listings\n",
            "\n",
            "Loading scenario2_jobs.html...\n",
            "  ✓ Loaded clean text (9353 chars)\n",
            "  Creating chunks (size=1000, overlap=200)...\n",
            "  Created 11 clean text chunks\n",
            "  Created 667 raw HTML chunks\n",
            "  Query type: LIST/COMPREHENSIVE (will retrieve 50 chunks)\n",
            "  Retrieving 11 chunks based on clean text embeddings...\n",
            "  Sending 9182 chars of RAW HTML to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 0 items\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario3_clubs\n",
            "================================================================================\n",
            "\n",
            "Query: Get the club names, logo image links and their official websites\n",
            "\n",
            "Loading scenario3_clubs.html...\n",
            "  ✓ Loaded clean text (163 chars)\n",
            "  Creating chunks (size=1000, overlap=200)...\n",
            "  Created 1 clean text chunks\n",
            "  Created 424 raw HTML chunks\n",
            "  Query type: SPECIFIC (will retrieve 10 chunks)\n",
            "  Retrieving 1 chunks based on clean text embeddings...\n",
            "  Sending 861 chars of RAW HTML to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 1 items\n",
            "\n",
            "  Sample (first 2 items):\n",
            "[\n",
            "  {\n",
            "    \"Club Name\": \"AzSoccerAssociation\",\n",
            "    \"Logo Image Links\": [\n",
            "      \"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/07/cropped-main-crest_color-1.png?w=32\",\n",
            "      \"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/07/cropped-main-crest_color-1.png?w=192\",\n",
            "      \"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/07/cropped-main-crest_color-1.png?w=180\"\n",
            "    ],\n",
            "    \"Official Website\": \"https://www.azsoccerassociation.org/\"\n",
            "  }\n",
            "]\n",
            "\n",
            "================================================================================\n",
            "Scenario: scenario4_property\n",
            "================================================================================\n",
            "\n",
            "Query: Return the property name, address, latitude and longitude\n",
            "\n",
            "Loading scenario4_property.html...\n",
            "  ✓ Loaded clean text (6052 chars)\n",
            "  Creating chunks (size=1000, overlap=200)...\n",
            "  Created 8 clean text chunks\n",
            "  Created 329 raw HTML chunks\n",
            "  Query type: SPECIFIC (will retrieve 10 chunks)\n",
            "  Retrieving 8 chunks based on clean text embeddings...\n",
            "  Sending 7365 chars of RAW HTML to LLM...\n",
            "\n",
            "✓ Result:\n",
            "  Extracted 1 items\n",
            "\n",
            "  Sample (first 2 items):\n",
            "[\n",
            "  {\n",
            "    \"name\": \"Silverado by AvantStay\",\n",
            "    \"address\": \"Park City\",\n",
            "    \"latitude\": null,\n",
            "    \"longitude\": null\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Test HYBRID System\n",
        "hybrid_results = {}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Testing HYBRID RAG System\")\n",
        "print(\"Clean text for retrieval → Raw HTML for extraction\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for scenario_name, html_file in HTML_FILES.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Scenario: {scenario_name}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    query = TEST_QUERIES[scenario_name]\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    \n",
        "    try:\n",
        "        result = rag_html_extraction_hybrid(\n",
        "            html_file, \n",
        "            query,\n",
        "            chunk_size=1000,     # Balanced\n",
        "            chunk_overlap=200,   # Good continuity\n",
        "            base_k=10,          # For specific queries\n",
        "            max_k=50            # For list queries\n",
        "        )\n",
        "        hybrid_results[scenario_name] = result\n",
        "        \n",
        "        print(\"\\n✓ Result:\")\n",
        "        if isinstance(result, list):\n",
        "            print(f\"  Extracted {len(result)} items\")\n",
        "            if len(result) > 0:\n",
        "                print(f\"\\n  Sample (first 2 items):\")\n",
        "                print(json.dumps(result[:2], indent=2))\n",
        "                if len(result) > 2:\n",
        "                    print(f\"\\n  ... and {len(result) - 2} more items\")\n",
        "        else:\n",
        "            print(json.dumps(result, indent=2))\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        hybrid_results[scenario_name] = {\"error\": str(e)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🏆 ULTIMATE COMPARISON: All RAG Approaches for HTML\n",
            "================================================================================\n",
            "\n",
            "📊 scenario1_books:\n",
            "  Original (k=5, text)                ✓ 20 items\n",
            "  Large Chunks (k=15, text)           ✓ 20 items\n",
            "  Small Chunks (k=40, text)           ✓ 20 items\n",
            "  Universal (k=50, RAW HTML)          ✓ 2 items\n",
            "  HYBRID (k=50, text→HTML)            ✓ 0 items\n",
            "\n",
            "📊 scenario2_jobs:\n",
            "  Original (k=5, text)                ✓ 7 items\n",
            "  Large Chunks (k=15, text)           ✓ 5 items\n",
            "  Small Chunks (k=40, text)           ✓ 6 items\n",
            "  Universal (k=50, RAW HTML)          ✓ 1 items\n",
            "  HYBRID (k=50, text→HTML)            ✓ 0 items\n",
            "\n",
            "📊 scenario3_clubs:\n",
            "  Original (k=5, text)                ✓ 2 items\n",
            "  Large Chunks (k=15, text)           ✓ 0 items\n",
            "  Small Chunks (k=40, text)           ✗ FAIL\n",
            "  Universal (k=50, RAW HTML)          ✓ 1 items\n",
            "  HYBRID (k=50, text→HTML)            ✓ 1 items\n",
            "\n",
            "📊 scenario4_property:\n",
            "  Original (k=5, text)                ✗ FAIL\n",
            "  Large Chunks (k=15, text)           ⚠️  PARTIAL\n",
            "  Small Chunks (k=40, text)           ✓ 1 items\n",
            "  Universal (k=50, RAW HTML)          ✓ 1 items\n",
            "  HYBRID (k=50, text→HTML)            ✓ 1 items\n",
            "\n",
            "================================================================================\n",
            "\n",
            "📚 KEY LEARNINGS FROM RAG EXPLORATION:\n",
            "\n",
            "1. 🎯 EMBEDDING FUNDAMENTALS:\n",
            "   - Embeddings need CLEAN semantic signal\n",
            "   - Raw HTML has too much noise (tags, CSS, attributes)\n",
            "   - Result: Clean text embeddings >> Raw HTML embeddings\n",
            "\n",
            "2. 🔄 THE HYBRID SOLUTION:\n",
            "   - Embed clean text (good semantic search)\n",
            "   - Retrieve corresponding raw HTML (complete data)\n",
            "   - Best of both worlds!\n",
            "\n",
            "3. 📊 CHUNK SIZE TRADE-OFFS:\n",
            "   - Too small (500): Fragments context, misses relationships\n",
            "   - Too large (2000+): Dilutes semantic signal\n",
            "   - Sweet spot: 800-1200 chars\n",
            "\n",
            "4. 🎪 RETRIEVAL STRATEGY:\n",
            "   - List queries: Need HIGH k (30-50) for completeness\n",
            "   - Specific queries: Can use LOW k (5-10) for precision\n",
            "   - Adaptive detection: Key to generalization\n",
            "\n",
            "5. 🚧 RAG LIMITATIONS FOR HTML:\n",
            "   - Struggles with 'get ALL items' queries (everything is relevant)\n",
            "   - Adds latency (embedding + vector search)\n",
            "   - May miss items due to chunking boundaries\n",
            "\n",
            "6. ⚡ WHEN TO SKIP RAG:\n",
            "   - Small HTML (<20KB): Just send to LLM\n",
            "   - Tabular data: All rows equally relevant\n",
            "   - Critical accuracy: Direct extraction safer\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ULTIMATE COMPARISON: All Approaches\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🏆 ULTIMATE COMPARISON: All RAG Approaches for HTML\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for scenario_name in HTML_FILES.keys():\n",
        "    print(f\"📊 {scenario_name}:\")\n",
        "    \n",
        "    # Get counts for each approach\n",
        "    approaches = {\n",
        "        \"Original (k=5, text)\": results.get(scenario_name, {}),\n",
        "        \"Large Chunks (k=15, text)\": improved_results.get(scenario_name, {}),\n",
        "        \"Small Chunks (k=40, text)\": v2_results.get(scenario_name, {}),\n",
        "        \"Universal (k=50, RAW HTML)\": universal_results.get(scenario_name, {}),\n",
        "        \"HYBRID (k=50, text→HTML)\": hybrid_results.get(scenario_name, {})\n",
        "    }\n",
        "    \n",
        "    for name, result in approaches.items():\n",
        "        if isinstance(result, list):\n",
        "            count = len(result)\n",
        "            status = f\"✓ {count} items\"\n",
        "        elif isinstance(result, dict) and \"error\" in result:\n",
        "            status = \"✗ FAIL\"\n",
        "        else:\n",
        "            status = \"⚠️  PARTIAL\"\n",
        "        print(f\"  {name:35} {status}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\\n📚 KEY LEARNINGS FROM RAG EXPLORATION:\")\n",
        "print(\"\\n1. 🎯 EMBEDDING FUNDAMENTALS:\")\n",
        "print(\"   - Embeddings need CLEAN semantic signal\")\n",
        "print(\"   - Raw HTML has too much noise (tags, CSS, attributes)\")\n",
        "print(\"   - Result: Clean text embeddings >> Raw HTML embeddings\")\n",
        "\n",
        "print(\"\\n2. 🔄 THE HYBRID SOLUTION:\")\n",
        "print(\"   - Embed clean text (good semantic search)\")\n",
        "print(\"   - Retrieve corresponding raw HTML (complete data)\")\n",
        "print(\"   - Best of both worlds!\")\n",
        "\n",
        "print(\"\\n3. 📊 CHUNK SIZE TRADE-OFFS:\")\n",
        "print(\"   - Too small (500): Fragments context, misses relationships\")\n",
        "print(\"   - Too large (2000+): Dilutes semantic signal\")\n",
        "print(\"   - Sweet spot: 800-1200 chars\")\n",
        "\n",
        "print(\"\\n4. 🎪 RETRIEVAL STRATEGY:\")\n",
        "print(\"   - List queries: Need HIGH k (30-50) for completeness\")\n",
        "print(\"   - Specific queries: Can use LOW k (5-10) for precision\")\n",
        "print(\"   - Adaptive detection: Key to generalization\")\n",
        "\n",
        "print(\"\\n5. 🚧 RAG LIMITATIONS FOR HTML:\")\n",
        "print(\"   - Struggles with 'get ALL items' queries (everything is relevant)\")\n",
        "print(\"   - Adds latency (embedding + vector search)\")\n",
        "print(\"   - May miss items due to chunking boundaries\")\n",
        "\n",
        "print(\"\\n6. ⚡ WHEN TO SKIP RAG:\")\n",
        "print(\"   - Small HTML (<20KB): Just send to LLM\")\n",
        "print(\"   - Tabular data: All rows equally relevant\")\n",
        "print(\"   - Critical accuracy: Direct extraction safer\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 23. Production Recommendation\n",
        "\n",
        "### 🏆 Winner: HYBRID RAG System\n",
        "\n",
        "**If Hybrid performs well, use it for:**\n",
        "- Large HTML files (>50KB)\n",
        "- Mixed content (relevant sections scattered)\n",
        "- When you need both semantic search AND complete data\n",
        "\n",
        "**Implementation:**\n",
        "```python\n",
        "result = rag_html_extraction_hybrid(\n",
        "    html_file,\n",
        "    query,\n",
        "    chunk_size=1000,   # Balanced\n",
        "    chunk_overlap=200,\n",
        "    base_k=10,         # Specific queries\n",
        "    max_k=50           # List queries\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🤔 If Hybrid STILL Underperforms...\n",
        "\n",
        "**Then RAG isn't the right tool for these HTML scenarios because:**\n",
        "1. **List extraction** = everything is equally relevant → retrieval adds no value\n",
        "2. **Chunking** = breaks coherent lists → data loss\n",
        "3. **Latency** = embedding + vector search → slower than direct\n",
        "\n",
        "**Better alternatives:**\n",
        "\n",
        "#### Option A: Direct LLM (Recommended for these scenarios)\n",
        "```python\n",
        "# Just send full HTML to LLM - simple and complete\n",
        "with open(html_file, 'r') as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "result = llm.invoke({\"html_content\": html_content, \"query\": query})\n",
        "```\n",
        "**Pros**: Simple, complete, reliable\n",
        "**Cons**: Expensive for very large files\n",
        "\n",
        "#### Option B: Code Generation\n",
        "```python\n",
        "# LLM generates BeautifulSoup code\n",
        "code = llm.generate_extraction_code(html_sample, query)\n",
        "result = execute_code(code, full_html)\n",
        "```\n",
        "**Pros**: Fast, deterministic, cheap for repeated use\n",
        "**Cons**: Requires structured HTML\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 Decision Matrix\n",
        "\n",
        "| HTML Size | Content Type | Query Type | Best Approach |\n",
        "|-----------|--------------|------------|---------------|\n",
        "| <20KB | Any | Any | **Direct LLM** |\n",
        "| 20-200KB | Mixed | Specific info | **Hybrid RAG** |\n",
        "| 20-200KB | Structured | All items | **Code Gen** |\n",
        "| >200KB | Mixed | Specific info | **Hybrid RAG** |\n",
        "| >200KB | Structured | All items | **Code Gen + Chunking** |\n",
        "\n",
        "### 🎯 For MrScraper Use Case\n",
        "\n",
        "Based on the test scenarios:\n",
        "- **Scenarios 1-3** (lists): Code generation or Direct LLM likely better\n",
        "- **Scenario 4** (property): Hybrid RAG or Direct LLM\n",
        "- **General solution**: Implement all three, route based on HTML size and query type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 24. Final Verdict: RAG is NOT Optimal for HTML Lists\n",
        "\n",
        "### 🔴 Results: Hybrid Failed Catastrophically\n",
        "\n",
        "| Scenario | Original | HYBRID | Change |\n",
        "|----------|----------|--------|--------|\n",
        "| Books | 20 items | 0 items | **-100% ❌** |\n",
        "| Jobs | 7 items | 0 items | **-100% ❌** |\n",
        "| Clubs | 2 items | 1 item | **-50% ❌** |\n",
        "| Property | FAIL | 1 item | **+100% ✅** |\n",
        "\n",
        "### 💡 Why RAG Fundamentally Struggles Here\n",
        "\n",
        "**The Core Problem**: These scenarios are **LIST EXTRACTION** tasks, not search tasks.\n",
        "\n",
        "```\n",
        "Traditional RAG: \"Find the most relevant passages from a large document\"\n",
        "✅ Good for: Search, Q&A, specific info retrieval\n",
        "\n",
        "These scenarios: \"Extract ALL items from a structured list\"  \n",
        "❌ Bad for: Complete extraction, tabular data, lists\n",
        "```\n",
        "\n",
        "**Why RAG fails:**\n",
        "1. **Everything is equally relevant** - Semantic search adds no value\n",
        "2. **Chunking breaks lists** - Job #5 might be in chunk 8, job #10 in chunk 15\n",
        "3. **Token limits** - Can't send all 50 chunks to LLM\n",
        "4. **Retrieval uncertainty** - Even with k=50, might miss chunks\n",
        "\n",
        "### ✅ The Right Solutions\n",
        "\n",
        "#### For Scenarios 1-3 (Lists): **Direct LLM or Code Generation**\n",
        "\n",
        "**Option A: Direct LLM** (Simplest)\n",
        "```python\n",
        "# No chunking, no retrieval, no RAG - just extract!\n",
        "with open(html_file, 'r') as f:\n",
        "    html = f.read()\n",
        "\n",
        "result = llm.invoke({\n",
        "    \"html_content\": html,\n",
        "    \"query\": \"Extract all books with name and price\"\n",
        "})\n",
        "```\n",
        "**Why this works:**\n",
        "- ✅ Sees ALL content (no retrieval needed)\n",
        "- ✅ No data loss from chunking\n",
        "- ✅ Simple, reliable, complete\n",
        "- ⚠️ More expensive for large HTML\n",
        "\n",
        "**Option B: Code Generation** (Most Reliable)\n",
        "```python\n",
        "# LLM generates extraction code\n",
        "code = llm.generate_code(html_sample, query)\n",
        "# Execute code on full HTML\n",
        "result = exec(code, html)\n",
        "```\n",
        "**Why this works:**\n",
        "- ✅ Deterministic (same HTML → same result)\n",
        "- ✅ Fast (no LLM inference after code gen)\n",
        "- ✅ Cheap for repeated use\n",
        "- ✅ Handles ALL items systematically\n",
        "\n",
        "#### For Scenario 4 (Property): **Direct LLM with Pattern Extraction**\n",
        "\n",
        "Single property page with hidden data → just send everything!\n",
        "\n",
        "---\n",
        "\n",
        "### 📊 When to Use Each Approach\n",
        "\n",
        "| Approach | Use Case | Pros | Cons |\n",
        "|----------|----------|------|------|\n",
        "| **Direct LLM** | Small-medium HTML (<200KB)<br/>Complete extraction needed | Simple, Complete, Reliable | Expensive for large files |\n",
        "| **Code Generation** | Structured HTML<br/>Repeated extraction | Fast, Cheap, Deterministic | Needs clear structure |\n",
        "| **RAG** | Large docs (>500KB)<br/>**Specific** info needed<br/>Mixed relevance | Efficient, Scales | Misses data, Complex, Slow |\n",
        "\n",
        "### 🎯 Recommendation for MrScraper\n",
        "\n",
        "**DO NOT use RAG for the provided scenarios.**\n",
        "\n",
        "**Instead:**\n",
        "1. **Production System**: Code Generation (tool calling)\n",
        "   - Most reliable and efficient\n",
        "   - See your existing `tool_calling_codegen.ipynb` notebook\n",
        "\n",
        "2. **Fallback**: Direct LLM  \n",
        "   - When code generation fails\n",
        "   - Simple and complete\n",
        "\n",
        "3. **RAG Only For**:\n",
        "   - Huge HTML files (>500KB) where sending full content is too expensive\n",
        "   - Finding specific info in large, mixed-content pages\n",
        "   - Not for complete list extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mrscraper",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
