{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG-Based HTML Information Extraction\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) system to extract structured information from HTML documents using LM Studio.\n",
        "\n",
        "## System Architecture\n",
        "1. **Chunking**: Split HTML into semantic chunks (tags, sections, scripts)\n",
        "2. **Embedding**: Use `nomic-embed-code` via LM Studio's OpenAI-compatible API\n",
        "3. **Retrieval**: FAISS vector store for efficient similarity search\n",
        "4. **Generation**: LM Studio chat model to generate structured JSON output\n",
        "\n",
        "## LM Studio Configuration\n",
        "- **Server**: http://localhost:1234/v1 (default)\n",
        "- **Embeddings**: nomic-embed-code\n",
        "- **Generation**: Any chat model (e.g., Qwen2.5-7B-Instruct)\n",
        "- Both models can run on the same LM Studio instance\n",
        "\n",
        "## Test Scenarios\n",
        "- **Scenario 1**: E-commerce (books) - extract name and price\n",
        "- **Scenario 2**: Job listings - extract title, location, salary, company\n",
        "- **Scenario 3**: Club listings - extract names, logo links, websites\n",
        "- **Scenario 4**: Hidden information - extract property details, coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from openai import OpenAI\n",
        "import httpx\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import faiss\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = Path(\"../data/html\").resolve()\n",
        "\n",
        "# LM Studio configuration\n",
        "LMSTUDIO_BASE_URL = os.getenv(\"LMSTUDIO_BASE_URL\", \"http://localhost:1234/v1\")\n",
        "LMSTUDIO_MODEL = os.getenv(\"LMSTUDIO_MODEL\", \"qwen2.5-7b-instruct-1m\")\n",
        "LMSTUDIO_API_KEY = os.getenv(\"LMSTUDIO_API_KEY\", \"lm-studio\")\n",
        "\n",
        "# Embedding configuration\n",
        "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"nomic-embed-code\")\n",
        "\n",
        "# Initialize OpenAI client for embeddings\n",
        "embedding_client = OpenAI(base_url=LMSTUDIO_BASE_URL, api_key=LMSTUDIO_API_KEY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Utility Functions\n",
        "\n",
        "Helper functions for embedding and similarity calculations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dot(va, vb):\n",
        "    \"\"\"Calculate dot product for cosine similarity.\"\"\"\n",
        "    return sum(a * b for a, b in zip(va, vb))\n",
        "\n",
        "\n",
        "def get_embedding(text: str, model: str = EMBEDDING_MODEL) -> List[float]:\n",
        "    \"\"\"Get embedding for a single text from LM Studio.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to embed\n",
        "        model: Model identifier (default: nomic-embed-code)\n",
        "        \n",
        "    Returns:\n",
        "        Embedding vector\n",
        "    \"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    return embedding_client.embeddings.create(input=[text], model=model).data[0].embedding\n",
        "\n",
        "\n",
        "def embed(texts: List[str]) -> List[List[float]]:\n",
        "    \"\"\"Get embeddings for multiple texts from LM Studio.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of texts to embed\n",
        "        \n",
        "    Returns:\n",
        "        List of embedding vectors\n",
        "    \"\"\"\n",
        "    # Clean texts\n",
        "    cleaned_texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
        "    response = embedding_client.embeddings.create(input=cleaned_texts, model=EMBEDDING_MODEL)\n",
        "    return [d.embedding for d in response.data]\n",
        "\n",
        "\n",
        "def embed_query(query: str) -> List[float]:\n",
        "    \"\"\"Embed a search query with the proper prefix.\n",
        "    \n",
        "    Args:\n",
        "        query: Search query text\n",
        "        \n",
        "    Returns:\n",
        "        Embedding vector\n",
        "    \"\"\"\n",
        "    prefixed_query = f'Represent this query for searching relevant code: {query}'\n",
        "    return get_embedding(prefixed_query)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HTMLChunk:\n",
        "    \"\"\"Represents a chunk of HTML with metadata.\"\"\"\n",
        "    content: str\n",
        "    tag_path: str  # e.g., \"html > body > div.container > article\"\n",
        "    attributes: Dict[str, str]\n",
        "    chunk_id: int\n",
        "    \n",
        "\n",
        "def extract_tag_path(element) -> str:\n",
        "    \"\"\"Build a CSS-like path for an element.\"\"\"\n",
        "    path_parts = []\n",
        "    for parent in element.parents:\n",
        "        if parent.name is None:\n",
        "            continue\n",
        "        name = parent.name\n",
        "        if parent.get('class'):\n",
        "            name += f\".{parent.get('class')[0]}\"\n",
        "        elif parent.get('id'):\n",
        "            name += f\"#{parent.get('id')}\"\n",
        "        path_parts.append(name)\n",
        "    return \" > \".join(reversed(path_parts[-5:]))  # Last 5 levels\n",
        "\n",
        "\n",
        "def chunk_html(html_content: str, max_chunk_size: int = 1000) -> List[HTMLChunk]:\n",
        "    \"\"\"Chunk HTML into semantic units.\n",
        "    \n",
        "    Strategy:\n",
        "    - Extract meaningful containers (divs, articles, sections, li, tr, etc.)\n",
        "    - Include attributes (class, id, data-*) as metadata\n",
        "    - Keep chunks small enough for embedding but large enough for context\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "    chunks = []\n",
        "    chunk_id = 0\n",
        "\n",
        "    # Expanded set of target tags for more thorough HTML extraction\n",
        "    target_tags = [\n",
        "        'article', 'div', 'section', 'li', 'tr', 'dl', 'aside', 'main', 'header',\n",
        "        'footer', 'nav', 'table', 'thead', 'tbody', 'tfoot', 'ul', 'ol', 'dt',\n",
        "        'dd', 'figure', 'figcaption', 'form', 'fieldset', 'legend', 'h1', 'h2',\n",
        "        'h3', 'h4', 'h5', 'h6', 'pre', 'code', 'blockquote', 'address', 'summary',\n",
        "        'details', 'p'\n",
        "    ]\n",
        "\n",
        "    for tag_name in target_tags:\n",
        "        elements = soup.find_all(tag_name)\n",
        "        for elem in elements:\n",
        "            text = elem.get_text(separator=' ', strip=True)\n",
        "\n",
        "            # Skip empty or very short chunks\n",
        "            if len(text) < 20:\n",
        "                continue\n",
        "\n",
        "            # Get inner HTML (preserving structure)\n",
        "            content = str(elem)[:max_chunk_size]\n",
        "\n",
        "            # Extract attributes\n",
        "            attrs = {k: ' '.join(v) if isinstance(v, list) else v\n",
        "                     for k, v in elem.attrs.items()}\n",
        "\n",
        "            # Build tag path\n",
        "            tag_path = extract_tag_path(elem)\n",
        "\n",
        "            chunks.append(HTMLChunk(\n",
        "                content=content,\n",
        "                tag_path=tag_path,\n",
        "                attributes=attrs,\n",
        "                chunk_id=chunk_id\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "    # Also extract script tags with JSON data\n",
        "    for script in soup.find_all('script', type='application/json'):\n",
        "        if script.string and len(script.string) > 50:\n",
        "            chunks.append(HTMLChunk(\n",
        "                content=script.string[:max_chunk_size],\n",
        "                tag_path=\"script[type=application/json]\",\n",
        "                attributes=script.attrs,\n",
        "                chunk_id=chunk_id\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "    # Extract inline script data (like __NEXT_DATA__)\n",
        "    for script in soup.find_all('script', id=True):\n",
        "        if script.string and ('{' in script.string or '[' in script.string):\n",
        "            chunks.append(HTMLChunk(\n",
        "                content=script.string[:max_chunk_size],\n",
        "                tag_path=f\"script#{script.get('id')}\",\n",
        "                attributes=script.attrs,\n",
        "                chunk_id=chunk_id\n",
        "            ))\n",
        "            chunk_id += 1\n",
        "\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. HTML Chunking Strategy\n",
        "\n",
        "We'll use a semantic chunking approach that preserves HTML structure while creating meaningful chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embedding Model Wrapper\n",
        "\n",
        "Wrapper class for the nomic-embed-code model hosted in LM Studio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NomicEmbedder:\n",
        "    \"\"\"Wrapper for nomic-embed-code hosted in LM Studio.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_dim: int = 768):\n",
        "        \"\"\"Initialize the embedding model wrapper.\n",
        "        \n",
        "        Args:\n",
        "            embedding_dim: Dimension of embeddings (768 for nomic-embed-code)\n",
        "        \"\"\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "        \n",
        "        # Test the connection\n",
        "        try:\n",
        "            test_embedding = get_embedding(\"test\")\n",
        "            self.embedding_dim = len(test_embedding)\n",
        "            print(f\"‚úì Connected to LM Studio embeddings. Dimension: {self.embedding_dim}\")\n",
        "            print(f\"‚úì Using model: {EMBEDDING_MODEL}\")\n",
        "        except Exception as e:\n",
        "            raise ConnectionError(\n",
        "                f\"Could not connect to LM Studio at {LMSTUDIO_BASE_URL}. \"\n",
        "                f\"Make sure LM Studio is running with {EMBEDDING_MODEL} loaded. Error: {e}\"\n",
        "            )\n",
        "    \n",
        "    def embed_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for a single text.\"\"\"\n",
        "        embedding = get_embedding(text)\n",
        "        return np.array(embedding, dtype=np.float32)\n",
        "    \n",
        "    def embed_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
        "        \"\"\"Generate embeddings for multiple texts.\n",
        "        \n",
        "        Args:\n",
        "            texts: List of texts to embed\n",
        "            batch_size: Number of texts to embed in each API call\n",
        "        \"\"\"\n",
        "        all_embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i + batch_size]\n",
        "            batch_embeddings = embed(batch)\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "        return np.array(all_embeddings, dtype=np.float32)\n",
        "    \n",
        "    def embed_query(self, query: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for a search query with proper prefix.\"\"\"\n",
        "        embedding = embed_query(query)\n",
        "        return np.array(embedding, dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Vector Store and Retrieval\n",
        "\n",
        "FAISS for efficient similarity search over embedded chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HTMLVectorStore:\n",
        "    \"\"\"Vector store for HTML chunks using FAISS.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder: NomicEmbedder):\n",
        "        self.embedder = embedder\n",
        "        self.index: Optional[faiss.Index] = None\n",
        "        self.chunks: List[HTMLChunk] = []\n",
        "        self.dimension = embedder.embedding_dim\n",
        "    \n",
        "    def build_index(self, html_content: str, chunk_size: int = 1000):\n",
        "        \"\"\"Build FAISS index from HTML content.\"\"\"\n",
        "        print(f\"Chunking HTML (target size: {chunk_size})...\")\n",
        "        self.chunks = chunk_html(html_content, max_chunk_size=chunk_size)\n",
        "        print(f\"Created {len(self.chunks)} chunks\")\n",
        "        \n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"No chunks created from HTML\")\n",
        "        \n",
        "        # Extract text for embedding\n",
        "        print(\"Generating embeddings...\")\n",
        "        texts_to_embed = []\n",
        "        for chunk in self.chunks:\n",
        "            # Combine content with metadata for richer embeddings\n",
        "            text = f\"{chunk.tag_path}\\n\"\n",
        "            if chunk.attributes:\n",
        "                attrs_str = \" \".join([f\"{k}={v}\" for k, v in chunk.attributes.items()])\n",
        "                text += f\"Attributes: {attrs_str}\\n\"\n",
        "            text += chunk.content\n",
        "            texts_to_embed.append(text[:2000])  # Limit for embedding\n",
        "        \n",
        "        embeddings = self.embedder.embed_batch(texts_to_embed)\n",
        "        \n",
        "        # Build FAISS index\n",
        "        print(\"Building FAISS index...\")\n",
        "        self.index = faiss.IndexFlatIP(self.dimension)  # Inner product (cosine similarity)\n",
        "        \n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "        \n",
        "        print(f\"Index built with {self.index.ntotal} vectors\")\n",
        "    \n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve top-k most relevant chunks for a query.\"\"\"\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"Index not built. Call build_index first.\")\n",
        "        \n",
        "        # Embed query\n",
        "        query_embedding = self.embedder.embed_query(query).reshape(1, -1)\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "        \n",
        "        # Search\n",
        "        scores, indices = self.index.search(query_embedding, top_k)\n",
        "        \n",
        "        # Format results\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(self.chunks):\n",
        "                chunk = self.chunks[idx]\n",
        "                results.append({\n",
        "                    \"content\": chunk.content,\n",
        "                    \"tag_path\": chunk.tag_path,\n",
        "                    \"attributes\": chunk.attributes,\n",
        "                    \"score\": float(score),\n",
        "                    \"chunk_id\": chunk.chunk_id\n",
        "                })\n",
        "        \n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. LLM Generation\n",
        "\n",
        "Use LM Studio to generate structured JSON from retrieved chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lmstudio_chat(\n",
        "    messages: List[Dict[str, str]], \n",
        "    model: Optional[str] = None,\n",
        "    temperature: float = 0.1, \n",
        "    max_tokens: int = 2048\n",
        ") -> str:\n",
        "    \"\"\"Call LM Studio's OpenAI-compatible chat endpoint.\"\"\"\n",
        "    base = LMSTUDIO_BASE_URL.rstrip(\"/\")\n",
        "    if not base.endswith(\"/v1\"):\n",
        "        base = base + \"/v1\"\n",
        "    url = f\"{base}/chat/completions\"\n",
        "    \n",
        "    headers = {\"Authorization\": f\"Bearer {LMSTUDIO_API_KEY}\"}\n",
        "    payload = {\n",
        "        \"model\": model or LMSTUDIO_MODEL,\n",
        "        \"messages\": messages,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"stream\": False,\n",
        "    }\n",
        "    \n",
        "    with httpx.Client(timeout=180) as client:\n",
        "        resp = client.post(url, headers=headers, json=payload)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "    \n",
        "    return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def extract_json_from_response(response: str) -> Any:\n",
        "    \"\"\"Extract JSON from LLM response, handling markdown code blocks.\"\"\"\n",
        "    # Try to find JSON in code blocks\n",
        "    match = re.search(r\"```(?:json)?\\s*([\\s\\S]+?)```\", response)\n",
        "    if match:\n",
        "        json_str = match.group(1).strip()\n",
        "    else:\n",
        "        json_str = response.strip()\n",
        "    \n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        # Try to find JSON object/array in the text\n",
        "        for pattern in [r'\\{[\\s\\S]+\\}', r'\\[[\\s\\S]+\\]']:\n",
        "            match = re.search(pattern, response)\n",
        "            if match:\n",
        "                try:\n",
        "                    return json.loads(match.group(0))\n",
        "                except:\n",
        "                    continue\n",
        "        raise ValueError(f\"Could not extract JSON from response: {response[:200]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. RAG Pipeline\n",
        "\n",
        "Combine retrieval and generation for end-to-end extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGHTMLExtractor:\n",
        "    \"\"\"RAG-based HTML information extraction system.\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder: NomicEmbedder):\n",
        "        self.embedder = embedder\n",
        "        self.vector_store = HTMLVectorStore(embedder)\n",
        "    \n",
        "    def index_html(self, html_content: str, chunk_size: int = 1000):\n",
        "        \"\"\"Index HTML content for retrieval.\"\"\"\n",
        "        self.vector_store.build_index(html_content, chunk_size=chunk_size)\n",
        "    \n",
        "    def extract(\n",
        "        self, \n",
        "        query: str, \n",
        "        top_k: int = 8,\n",
        "        temperature: float = 0.1,\n",
        "        max_retries: int = 2\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Extract structured information using RAG.\n",
        "        \n",
        "        Args:\n",
        "            query: Natural language query describing what to extract\n",
        "            top_k: Number of chunks to retrieve\n",
        "            temperature: LLM temperature\n",
        "            max_retries: Number of retries if JSON parsing fails\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with extracted information\n",
        "        \"\"\"\n",
        "        # Retrieve relevant chunks\n",
        "        print(f\"\\nRetrieving top-{top_k} chunks for query: {query}\")\n",
        "        retrieved_chunks = self.vector_store.retrieve(query, top_k=top_k)\n",
        "        \n",
        "        if not retrieved_chunks:\n",
        "            return {\"error\": \"No relevant chunks found\", \"data\": []}\n",
        "        \n",
        "        # Show retrieval results\n",
        "        print(\"\\nTop retrieved chunks:\")\n",
        "        for i, chunk in enumerate(retrieved_chunks[:3], 1):\n",
        "            print(f\"  {i}. Score: {chunk['score']:.3f} | Path: {chunk['tag_path'][:60]}\")\n",
        "        \n",
        "        # Build context from retrieved chunks\n",
        "        context_parts = []\n",
        "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "            context_parts.append(f\"--- Chunk {i} (score: {chunk['score']:.3f}) ---\")\n",
        "            context_parts.append(f\"Path: {chunk['tag_path']}\")\n",
        "            if chunk['attributes']:\n",
        "                attrs = ', '.join([f\"{k}={v}\" for k, v in list(chunk['attributes'].items())[:3]])\n",
        "                context_parts.append(f\"Attributes: {attrs}\")\n",
        "            context_parts.append(f\"Content:\\n{chunk['content'][:800]}\")\n",
        "            context_parts.append(\"\")\n",
        "        \n",
        "        context = \"\\n\".join(context_parts)\n",
        "        \n",
        "        # Build prompt\n",
        "        system_prompt = \"\"\"You are an expert at extracting structured information from HTML.\n",
        "Given HTML chunks retrieved for a specific query, extract the requested information and return it as valid JSON.\n",
        "\n",
        "Rules:\n",
        "- Return ONLY valid JSON (array or object)\n",
        "- Extract ALL items found in the chunks\n",
        "- Use null for missing fields\n",
        "- Be precise with data types (numbers as numbers, not strings)\n",
        "- For prices/salaries, extract numeric values when possible\n",
        "- Do not truncate or limit results unless explicitly requested\n",
        "\"\"\"\n",
        "        \n",
        "        user_prompt = f\"\"\"Query: {query}\n",
        "\n",
        "Retrieved HTML chunks:\n",
        "{context}\n",
        "\n",
        "Extract the requested information and return as JSON. If the query asks for multiple items, return an array. Each object should have clear field names matching the query.\"\"\"\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "        \n",
        "        # Generate with retries\n",
        "        for attempt in range(max_retries + 1):\n",
        "            try:\n",
        "                print(f\"\\nGenerating response (attempt {attempt + 1}/{max_retries + 1})...\")\n",
        "                response = lmstudio_chat(messages, temperature=temperature)\n",
        "                \n",
        "                # Extract and parse JSON\n",
        "                result = extract_json_from_response(response)\n",
        "                \n",
        "                print(f\"‚úì Successfully extracted {len(result) if isinstance(result, list) else 1} item(s)\")\n",
        "                return {\n",
        "                    \"success\": True,\n",
        "                    \"data\": result,\n",
        "                    \"query\": query,\n",
        "                    \"chunks_used\": len(retrieved_chunks)\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚úó Attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt < max_retries:\n",
        "                    # Add feedback for retry\n",
        "                    messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "                    messages.append({\n",
        "                        \"role\": \"user\", \n",
        "                        \"content\": f\"The response was not valid JSON. Error: {e}. Please return ONLY valid JSON.\"\n",
        "                    })\n",
        "                else:\n",
        "                    return {\n",
        "                        \"success\": False,\n",
        "                        \"error\": str(e),\n",
        "                        \"raw_response\": response,\n",
        "                        \"query\": query\n",
        "                    }\n",
        "    \n",
        "    def extract_from_file(\n",
        "        self, \n",
        "        html_path: Path, \n",
        "        query: str,\n",
        "        top_k: int = 8,\n",
        "        chunk_size: int = 1000\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Extract information from an HTML file.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing: {html_path.name}\")\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Load HTML\n",
        "        html_content = html_path.read_text(encoding='utf-8', errors='ignore')\n",
        "        \n",
        "        # Index\n",
        "        self.index_html(html_content, chunk_size=chunk_size)\n",
        "        \n",
        "        # Extract\n",
        "        result = self.extract(query, top_k=top_k)\n",
        "        \n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize System\n",
        "\n",
        "Connect to the LM Studio embeddings endpoint and create the RAG extractor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to LM Studio...\n",
            "LM Studio URL: http://localhost:1234/v1\n",
            "Embedding Model: nomic-embed-code\n",
            "Chat Model: qwen2.5-7b-instruct-1m\n",
            "\n",
            "‚úó Could not connect to LM Studio at http://localhost:1234/v1. Make sure LM Studio is running with nomic-embed-code loaded. Error: Error code: 404 - {'error': {'message': 'Failed to load model \"nomic-embed-code\". Error: Model is not embedding.', 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}\n",
            "\n",
            "Make sure LM Studio is running with:\n",
            "  - nomic-embed-code model loaded for embeddings\n",
            "  - A chat model (e.g., qwen2.5-7b-instruct-1m) loaded for generation\n",
            "  - Server running on http://localhost:1234/v1\n"
          ]
        },
        {
          "ename": "ConnectionError",
          "evalue": "Could not connect to LM Studio at http://localhost:1234/v1. Make sure LM Studio is running with nomic-embed-code loaded. Error: Error code: 404 - {'error': {'message': 'Failed to load model \"nomic-embed-code\". Error: Model is not embedding.', 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mNomicEmbedder.__init__\u001b[39m\u001b[34m(self, embedding_dim)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     test_embedding = \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_dim = \u001b[38;5;28mlen\u001b[39m(test_embedding)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mget_embedding\u001b[39m\u001b[34m(text, model)\u001b[39m\n\u001b[32m     16\u001b[39m text = text.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membedding_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m.data[\u001b[32m0\u001b[39m].embedding\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrscraper/lib/python3.11/site-packages/openai/resources/embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrscraper/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrscraper/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1046\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'Failed to load model \"nomic-embed-code\". Error: Model is not embedding.', 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     embedder = \u001b[43mNomicEmbedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úó \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mNomicEmbedder.__init__\u001b[39m\u001b[34m(self, embedding_dim)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Using model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\n\u001b[32m     20\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not connect to LM Studio at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLMSTUDIO_BASE_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMake sure LM Studio is running with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_MODEL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m loaded. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m     )\n",
            "\u001b[31mConnectionError\u001b[39m: Could not connect to LM Studio at http://localhost:1234/v1. Make sure LM Studio is running with nomic-embed-code loaded. Error: Error code: 404 - {'error': {'message': 'Failed to load model \"nomic-embed-code\". Error: Model is not embedding.', 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}"
          ]
        }
      ],
      "source": [
        "# Initialize embedder\n",
        "print(\"Connecting to LM Studio...\")\n",
        "print(f\"LM Studio URL: {LMSTUDIO_BASE_URL}\")\n",
        "print(f\"Embedding Model: {EMBEDDING_MODEL}\")\n",
        "print(f\"Chat Model: {LMSTUDIO_MODEL}\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    embedder = NomicEmbedder()\n",
        "except ConnectionError as e:\n",
        "    print(f\"‚úó {e}\")\n",
        "    print(\"\\nMake sure LM Studio is running with:\")\n",
        "    print(f\"  - {EMBEDDING_MODEL} model loaded for embeddings\")\n",
        "    print(f\"  - A chat model (e.g., {LMSTUDIO_MODEL}) loaded for generation\")\n",
        "    print(f\"  - Server running on {LMSTUDIO_BASE_URL}\")\n",
        "    raise\n",
        "\n",
        "# Create RAG extractor\n",
        "rag_extractor = RAGHTMLExtractor(embedder)\n",
        "print(\"\\n‚úì RAG system initialized and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Scenarios\n",
        "\n",
        "Run the extraction on all test scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 1: E-commerce Book Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario1_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario1_books.html\",\n",
        "    query=\"Extract all books with their name and price\",\n",
        "    top_k=10,\n",
        "    chunk_size=800\n",
        ")\n",
        "\n",
        "if scenario1_result[\"success\"]:\n",
        "    print(\"\\nüìö Sample extracted books:\")\n",
        "    print(json.dumps(scenario1_result[\"data\"][:3], indent=2))\n",
        "    print(f\"\\nTotal books extracted: {len(scenario1_result['data'])}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario1_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 2: Job Listings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario2_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario2_jobs.html\",\n",
        "    query=\"Extract job title, location, salary, and company name from all job listings\",\n",
        "    top_k=12,\n",
        "    chunk_size=1200\n",
        ")\n",
        "\n",
        "if scenario2_result[\"success\"]:\n",
        "    print(\"\\nüíº Sample extracted jobs:\")\n",
        "    print(json.dumps(scenario2_result[\"data\"][:3], indent=2))\n",
        "    print(f\"\\nTotal jobs extracted: {len(scenario2_result['data'])}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario2_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 3: Club Listings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario3_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario3_clubs.html\",\n",
        "    query=\"Get the club names, logo image links and their official websites\",\n",
        "    top_k=10,\n",
        "    chunk_size=1000\n",
        ")\n",
        "\n",
        "if scenario3_result[\"success\"]:\n",
        "    print(\"\\n‚öΩ Sample extracted clubs:\")\n",
        "    print(json.dumps(scenario3_result[\"data\"][:3], indent=2))\n",
        "    print(f\"\\nTotal clubs extracted: {len(scenario3_result['data'])}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario3_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 4: Property Details (Hidden Information)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scenario4_result = rag_extractor.extract_from_file(\n",
        "    html_path=DATA_DIR / \"scenario4_property.html\",\n",
        "    query=\"Return the property name, address, latitude and longitude\",\n",
        "    top_k=8,\n",
        "    chunk_size=1500\n",
        ")\n",
        "\n",
        "if scenario4_result[\"success\"]:\n",
        "    print(\"\\nüè† Extracted property details:\")\n",
        "    print(json.dumps(scenario4_result[\"data\"], indent=2))\n",
        "else:\n",
        "    print(f\"\\n‚úó Extraction failed: {scenario4_result['error']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_summary = {\n",
        "    \"Scenario 1 (Books)\": {\n",
        "        \"success\": scenario1_result[\"success\"],\n",
        "        \"items_extracted\": len(scenario1_result.get(\"data\", [])) if isinstance(scenario1_result.get(\"data\"), list) else 1,\n",
        "        \"chunks_used\": scenario1_result.get(\"chunks_used\", 0)\n",
        "    },\n",
        "    \"Scenario 2 (Jobs)\": {\n",
        "        \"success\": scenario2_result[\"success\"],\n",
        "        \"items_extracted\": len(scenario2_result.get(\"data\", [])) if isinstance(scenario2_result.get(\"data\"), list) else 1,\n",
        "        \"chunks_used\": scenario2_result.get(\"chunks_used\", 0)\n",
        "    },\n",
        "    \"Scenario 3 (Clubs)\": {\n",
        "        \"success\": scenario3_result[\"success\"],\n",
        "        \"items_extracted\": len(scenario3_result.get(\"data\", [])) if isinstance(scenario3_result.get(\"data\"), list) else 1,\n",
        "        \"chunks_used\": scenario3_result.get(\"chunks_used\", 0)\n",
        "    },\n",
        "    \"Scenario 4 (Property)\": {\n",
        "        \"success\": scenario4_result[\"success\"],\n",
        "        \"items_extracted\": 1 if scenario4_result[\"success\"] else 0,\n",
        "        \"chunks_used\": scenario4_result.get(\"chunks_used\", 0)\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(json.dumps(results_summary, indent=2))\n",
        "\n",
        "# Save results\n",
        "output_dir = Path(\"../generated\").resolve()\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "output_file = output_dir / f\"rag_extraction_results_{int(time.time())}.json\"\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump({\n",
        "        \"scenario1\": scenario1_result,\n",
        "        \"scenario2\": scenario2_result,\n",
        "        \"scenario3\": scenario3_result,\n",
        "        \"scenario4\": scenario4_result,\n",
        "        \"summary\": results_summary\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Results saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Interactive Extraction\n",
        "\n",
        "Test custom queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_extract(scenario_file: str, custom_query: str):\n",
        "    \"\"\"Run a custom extraction query.\"\"\"\n",
        "    result = rag_extractor.extract_from_file(\n",
        "        html_path=DATA_DIR / scenario_file,\n",
        "        query=custom_query,\n",
        "        top_k=10\n",
        "    )\n",
        "    \n",
        "    if result[\"success\"]:\n",
        "        print(\"\\n‚úì Extraction successful!\")\n",
        "        print(json.dumps(result[\"data\"], indent=2)[:1000])  # First 1000 chars\n",
        "    else:\n",
        "        print(f\"\\n‚úó Failed: {result['error']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Example: Custom query (uncomment to use)\n",
        "# custom_result = interactive_extract(\n",
        "#     scenario_file=\"scenario1_books.html\",\n",
        "#     custom_query=\"Find all books with 5-star ratings and extract their titles and prices\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "### LM Studio Setup\n",
        "\n",
        "**Simple Setup (Single Port):**\n",
        "1. Load `nomic-embed-code` in LM Studio\n",
        "2. Start the Local Server (default port 1234)\n",
        "3. Both embeddings and chat will use the same endpoint\n",
        "4. The system will automatically use:\n",
        "   - `nomic-embed-code` for embeddings\n",
        "   - Your loaded chat model for generation\n",
        "\n",
        "**Steps:**\n",
        "1. Open LM Studio\n",
        "2. Go to **Local Server** tab\n",
        "3. Load the `nomic-embed-code` model (or load it along with your chat model)\n",
        "4. Start the server (default: `http://localhost:1234`)\n",
        "5. Make sure both embedding and chat models are loaded\n",
        "\n",
        "**Environment Variables (optional):**\n",
        "```bash\n",
        "export LMSTUDIO_BASE_URL=\"http://localhost:1234/v1\"\n",
        "export EMBEDDING_MODEL=\"nomic-embed-code\"\n",
        "export LMSTUDIO_MODEL=\"your-chat-model-name\"\n",
        "```\n",
        "\n",
        "### Performance Tuning\n",
        "- **chunk_size**: Smaller chunks (500-800) for precise extraction, larger (1000-1500) for context\n",
        "- **top_k**: More chunks (10-15) for comprehensive extraction, fewer (5-8) for speed\n",
        "- **temperature**: Lower (0.0-0.2) for consistent structured output\n",
        "- **batch_size**: Adjust embedding batch size based on your system (default: 32)\n",
        "\n",
        "### Advantages of RAG Approach\n",
        "- ‚úì Handles large HTML files efficiently\n",
        "- ‚úì Retrieves only relevant sections\n",
        "- ‚úì Better context understanding with semantic search\n",
        "- ‚úì Scalable to multiple documents\n",
        "- ‚úì Works with hidden/embedded JSON data\n",
        "- ‚úì Uses LM Studio's OpenAI-compatible API\n",
        "- ‚úì Easy to swap models via LM Studio\n",
        "\n",
        "### Dependencies\n",
        "```bash\n",
        "pip install beautifulsoup4 lxml faiss-cpu httpx numpy openai\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mrscraper",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
