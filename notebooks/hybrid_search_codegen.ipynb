{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Approach: Semantic Search + Code Generation\n",
    "\n",
    "## Core Idea\n",
    "\n",
    "Combine the strengths of both approaches:\n",
    "1. **Semantic Search (RAG)**: Find WHERE the data is located\n",
    "2. **Code Generation (Agent)**: Extract WHAT the data is accurately\n",
    "\n",
    "## Why This Works\n",
    "\n",
    "- ‚úÖ **Search**: Embeddings excel at finding relevant HTML sections\n",
    "- ‚úÖ **Extraction**: Code is deterministic and gets ALL items\n",
    "- ‚úÖ **Iterative**: Agent can refine code based on results\n",
    "- ‚úÖ **Complete**: No data loss from chunking or retrieval limits\n",
    "\n",
    "## Process\n",
    "\n",
    "```\n",
    "1. User Query ‚Üí Semantic Search ‚Üí Find relevant HTML sections\n",
    "2. Agent analyzes sections ‚Üí Generates BeautifulSoup code\n",
    "3. Execute code on FULL HTML ‚Üí Get results\n",
    "4. If incomplete ‚Üí Agent iterates (refine code, search more)\n",
    "5. Return complete structured data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain langchain-community langchain-core sentence-transformers chromadb beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path(\"/Users/ardyh/Documents/job-applications/mrscraper\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"html\"\n",
    "\n",
    "# Test scenarios\n",
    "HTML_FILES = {\n",
    "    \"scenario1_books\": DATA_DIR / \"scenario1_books.html\",\n",
    "    \"scenario2_jobs\": DATA_DIR / \"scenario2_jobs.html\",\n",
    "    \"scenario3_clubs\": DATA_DIR / \"scenario3_clubs.html\",\n",
    "    \"scenario4_property\": DATA_DIR / \"scenario4_property.html\"\n",
    "}\n",
    "\n",
    "TEST_QUERIES = {\n",
    "    \"scenario1_books\": \"Can you return me the books: name and price?\",\n",
    "    \"scenario2_jobs\": \"Extract job title, location, salary, and company name from the listings\",\n",
    "    \"scenario3_clubs\": \"Get the club names, logo image links and their official websites\",\n",
    "    \"scenario4_property\": \"Return the property name, address, latitude and longitude\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/mfq1y5w57hj01ttff7smkspw0000gn/T/ipykernel_5057/3986879727.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Models initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/mfq1y5w57hj01ttff7smkspw0000gn/T/ipykernel_5057/3986879727.py:9: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "# Embedding model for semantic search\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# LLM for code generation (using Ollama)\n",
    "llm = Ollama(\n",
    "    model=\"llama3\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"‚úì Models initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hybrid Semantic Search Engine\n",
    "\n",
    "**Key Innovation**: Embed clean text (good semantic signal), but return raw HTML (complete context)\n",
    "\n",
    "This combines the best of both:\n",
    "- Clean text embeddings = better semantic search\n",
    "- Raw HTML context = accurate code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Hybrid HTMLSearchEngine defined!\n"
     ]
    }
   ],
   "source": [
    "class HTMLSearchEngine:\n",
    "    \"\"\"\n",
    "    Hybrid semantic search engine for HTML content.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Embed CLEAN TEXT (good semantic signal for search)\n",
    "    2. Store RAW HTML in metadata (complete context for LLM)\n",
    "    3. Search on clean text, return raw HTML\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, html_content: str, embeddings):\n",
    "        self.html_content = html_content\n",
    "        self.embeddings = embeddings\n",
    "        self.vectorstore = None\n",
    "        self._build_index()\n",
    "    \n",
    "    def _chunk_html_aligned(self, chunk_size: int = 1000, overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Create aligned chunks of clean text and raw HTML.\n",
    "        Returns: List of (clean_text, raw_html) tuples\n",
    "        \"\"\"\n",
    "        from bs4 import BeautifulSoup\n",
    "        import re\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(self.html_content, 'lxml')\n",
    "        \n",
    "        # Get all text nodes with their positions\n",
    "        clean_text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Simple chunking approach: split both clean and raw HTML\n",
    "        # For production, you'd want more sophisticated alignment\n",
    "        clean_chunks = []\n",
    "        start = 0\n",
    "        while start < len(clean_text):\n",
    "            end = min(start + chunk_size, len(clean_text))\n",
    "            clean_chunks.append(clean_text[start:end])\n",
    "            start = end - overlap if end < len(clean_text) else end\n",
    "        \n",
    "        # For raw HTML, use a simple heuristic: divide proportionally\n",
    "        html_per_char = len(self.html_content) / max(len(clean_text), 1)\n",
    "        raw_chunks = []\n",
    "        \n",
    "        for i, clean_chunk in enumerate(clean_chunks):\n",
    "            # Estimate HTML position based on clean text position\n",
    "            clean_start = i * (chunk_size - overlap)\n",
    "            html_start = int(clean_start * html_per_char)\n",
    "            html_end = int((clean_start + len(clean_chunk)) * html_per_char)\n",
    "            \n",
    "            # Extract raw HTML chunk (with some padding)\n",
    "            padding = 500\n",
    "            html_start = max(0, html_start - padding)\n",
    "            html_end = min(len(self.html_content), html_end + padding)\n",
    "            \n",
    "            raw_chunks.append(self.html_content[html_start:html_end])\n",
    "        \n",
    "        return list(zip(clean_chunks, raw_chunks))\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"\n",
    "        Build vector index: Embed clean text, store raw HTML in metadata\n",
    "        \"\"\"\n",
    "        print(\"  Building hybrid index (clean text embeddings + raw HTML storage)...\")\n",
    "        \n",
    "        # Get aligned chunks\n",
    "        aligned_chunks = self._chunk_html_aligned(chunk_size=1000, overlap=200)\n",
    "        \n",
    "        # Create documents: embed clean text, store raw HTML\n",
    "        documents = []\n",
    "        for i, (clean_text, raw_html) in enumerate(aligned_chunks):\n",
    "            doc = Document(\n",
    "                page_content=clean_text,  # ‚Üê This gets embedded\n",
    "                metadata={\n",
    "                    \"chunk_id\": i,\n",
    "                    \"raw_html\": raw_html,  # ‚Üê This is stored for retrieval\n",
    "                    \"source\": \"html\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Build vector store\n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents,\n",
    "            self.embeddings,\n",
    "            collection_name=\"html_search\"\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úì Indexed {len(documents)} chunks (clean text ‚Üí raw HTML mapping)\")\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        Search for relevant sections.\n",
    "        \n",
    "        Process:\n",
    "        1. Search using clean text embeddings (good semantic match)\n",
    "        2. Return raw HTML chunks (complete context for code generation)\n",
    "        \"\"\"\n",
    "        # Search on clean text embeddings\n",
    "        docs = self.vectorstore.similarity_search(query, k=k)\n",
    "        \n",
    "        # Return the RAW HTML from metadata\n",
    "        raw_html_sections = [doc.metadata[\"raw_html\"] for doc in docs]\n",
    "        \n",
    "        return raw_html_sections\n",
    "\n",
    "# Global search engine (will be initialized per HTML file)\n",
    "search_engine = None\n",
    "\n",
    "print(\"‚úì Hybrid HTMLSearchEngine defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Agent Tools\n",
    "\n",
    "Tools the agent can use to search HTML and execute extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Defined 3 tools for the agent\n",
      "  - search_html_sections: Semantic search ‚Üí returns RAW HTML\n",
      "  - execute_extraction_code: Runs BeautifulSoup on full HTML\n",
      "  - get_html_sample: Returns raw HTML sample\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_html_sections(query: str, num_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search HTML content semantically to find relevant sections.\n",
    "    \n",
    "    **How it works:**\n",
    "    1. Searches using CLEAN TEXT embeddings (good semantic matching)\n",
    "    2. Returns RAW HTML sections (complete context with tags, classes, attributes)\n",
    "    \n",
    "    This gives the LLM the full HTML structure it needs to write accurate BeautifulSoup code.\n",
    "    \n",
    "    Args:\n",
    "        query: What to search for (e.g., \"job listings\", \"book prices\")\n",
    "        num_results: Number of relevant sections to return\n",
    "    \n",
    "    Returns:\n",
    "        String with relevant RAW HTML sections (with full tags and attributes)\n",
    "    \"\"\"\n",
    "    if search_engine is None:\n",
    "        return \"Error: Search engine not initialized\"\n",
    "    \n",
    "    # Returns raw HTML sections (from metadata)\n",
    "    results = search_engine.search(query, k=num_results)\n",
    "    return \"\\n\\n---SECTION---\\n\\n\".join(results[:3])  # Return top 3 to avoid overwhelming\n",
    "\n",
    "@tool\n",
    "def execute_extraction_code(python_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute BeautifulSoup extraction code on the full HTML.\n",
    "    \n",
    "    The code should:\n",
    "    1. Parse HTML with: soup = BeautifulSoup(html_content, 'lxml')\n",
    "    2. Extract data using BeautifulSoup methods\n",
    "    3. Store results in: result = [list of dicts]\n",
    "    \n",
    "    Args:\n",
    "        python_code: BeautifulSoup extraction code\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with extracted data or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create safe execution environment\n",
    "        local_vars = {\n",
    "            'html_content': search_engine.html_content if search_engine else \"\",\n",
    "            'BeautifulSoup': BeautifulSoup,\n",
    "            'json': json\n",
    "        }\n",
    "        \n",
    "        # Execute code\n",
    "        exec(python_code, {}, local_vars)\n",
    "        \n",
    "        # Get result (code should set 'result' variable)\n",
    "        result = local_vars.get('result', [])\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error executing code: {str(e)}\\n\\nCode was:\\n{python_code}\"\n",
    "\n",
    "@tool\n",
    "def get_html_sample(num_chars: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Get a sample of the raw HTML to understand its structure.\n",
    "    \n",
    "    Args:\n",
    "        num_chars: Number of characters to return\n",
    "    \n",
    "    Returns:\n",
    "        Raw HTML sample\n",
    "    \"\"\"\n",
    "    if search_engine is None:\n",
    "        return \"Error: Search engine not initialized\"\n",
    "    \n",
    "    return search_engine.html_content[:num_chars]\n",
    "\n",
    "# Collect all tools\n",
    "tools = [search_html_sections, execute_extraction_code, get_html_sample]\n",
    "\n",
    "print(f\"‚úì Defined {len(tools)} tools for the agent\")\n",
    "print(\"  - search_html_sections: Semantic search ‚Üí returns RAW HTML\")\n",
    "print(\"  - execute_extraction_code: Runs BeautifulSoup on full HTML\")\n",
    "print(\"  - get_html_sample: Returns raw HTML sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Workflow Visualization\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   User Query     ‚îÇ \"Extract job titles, locations, salaries\"\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  search_html_sections()            ‚îÇ\n",
    "‚îÇ  - Embeds query                    ‚îÇ ‚Üê CLEAN TEXT EMBEDDINGS\n",
    "‚îÇ  - Searches vector DB              ‚îÇ   (Good semantic matching)\n",
    "‚îÇ  - Retrieves top K chunks          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Returns RAW HTML sections         ‚îÇ\n",
    "‚îÇ  <div class=\"job-card\">            ‚îÇ ‚Üê RAW HTML WITH TAGS\n",
    "‚îÇ    <h2 class=\"title\">Engineer</h2> ‚îÇ   (Complete context)\n",
    "‚îÇ    <span class=\"loc\">NYC</span>    ‚îÇ\n",
    "‚îÇ  </div>                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  LLM analyzes RAW HTML             ‚îÇ\n",
    "‚îÇ  - Sees class names                ‚îÇ ‚Üê ACCURATE SELECTORS\n",
    "‚îÇ  - Sees tag structure              ‚îÇ   (div.job-card > h2.title)\n",
    "‚îÇ  - Generates BeautifulSoup code    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  execute_extraction_code()         ‚îÇ\n",
    "‚îÇ  soup = BeautifulSoup(html, 'lxml')‚îÇ ‚Üê FULL HTML\n",
    "‚îÇ  jobs = soup.find_all('div',       ‚îÇ   (ALL items extracted)\n",
    "‚îÇ                class_='job-card')   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Complete Structured JSON Data     ‚îÇ ‚úÖ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Key Insight**: Best of both worlds!\n",
    "- Embed clean text (better semantic search)\n",
    "- Return raw HTML (complete context for code generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simple Agent Implementation\n",
    "\n",
    "Since Ollama doesn't natively support tool calling, we'll create a straightforward agent loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Hybrid agent function defined!\n"
     ]
    }
   ],
   "source": [
    "def hybrid_extraction_agent(\n",
    "    html_file_path: Path,\n",
    "    query: str,\n",
    "    max_iterations: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Hybrid extraction: Search semantically, then generate code\n",
    "    \n",
    "    Process:\n",
    "    1. Semantic search on clean text (find relevant sections)\n",
    "    2. Retrieve raw HTML sections (complete context)\n",
    "    3. LLM generates BeautifulSoup code (based on raw HTML)\n",
    "    4. Execute code on full HTML (get all items)\n",
    "    \n",
    "    Args:\n",
    "        html_file_path: Path to HTML file\n",
    "        query: Extraction query\n",
    "        max_iterations: Max attempts to refine\n",
    "    \n",
    "    Returns:\n",
    "        Extracted data\n",
    "    \"\"\"\n",
    "    global search_engine\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {html_file_path.name}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load HTML\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Initialize search engine (builds hybrid index)\n",
    "    print(\"Step 1: Building hybrid search index...\")\n",
    "    search_engine = HTMLSearchEngine(html_content, embeddings)\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Search for relevant sections\n",
    "    print(\"Step 2: Searching for relevant HTML sections...\")\n",
    "    relevant_sections = search_html_sections.invoke({\"query\": query, \"num_results\": 5})\n",
    "    print(f\"  ‚úì Found relevant sections ({len(relevant_sections)} chars of RAW HTML)\\n\")\n",
    "    print(f\"Sample of RAW HTML returned:\\n{relevant_sections[:500]}...\\n\")\n",
    "    \n",
    "    # Step 3: Generate extraction code\n",
    "    print(\"Step 3: Generating BeautifulSoup extraction code...\")\n",
    "    \n",
    "    code_gen_prompt = f\"\"\"\n",
    "Based on this RAW HTML and query, generate BeautifulSoup extraction code.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "RAW HTML Sample (with tags, classes, and attributes):\n",
    "{relevant_sections[:2000]}\n",
    "\n",
    "Generate Python code that:\n",
    "1. Parses HTML: soup = BeautifulSoup(html_content, 'lxml')\n",
    "2. Uses the class names and tag structure you see above\n",
    "3. Finds ALL matching elements (not just one)\n",
    "4. Extracts the requested fields\n",
    "5. Stores results in: result = [list of dicts]\n",
    "\n",
    "Return ONLY the Python code, no explanations.\n",
    "Code:\n",
    "\"\"\"\n",
    "    \n",
    "    code_response = llm.invoke(code_gen_prompt)\n",
    "    \n",
    "    # Extract code from response\n",
    "    code = code_response\n",
    "    if \"```python\" in code:\n",
    "        code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code:\n",
    "        code = code.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    print(f\"  ‚úì Generated code ({len(code)} chars)\\n\")\n",
    "    print(f\"Code:\\n{code}\\n\")\n",
    "    \n",
    "    # Step 4: Execute code\n",
    "    print(\"Step 4: Executing extraction code on FULL HTML...\")\n",
    "    result_json = execute_extraction_code.invoke({\"python_code\": code})\n",
    "    \n",
    "    try:\n",
    "        result = json.loads(result_json)\n",
    "        print(f\"  ‚úì Extracted {len(result) if isinstance(result, list) else 'some'} items\\n\")\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  ‚úó Error: {result_json}\\n\")\n",
    "        return {\"error\": result_json}\n",
    "\n",
    "print(\"‚úì Hybrid agent function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on All Scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing: scenario1_books.html\n",
      "Query: Can you return me the books: name and price?\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building hybrid search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 3 chunks (clean text ‚Üí raw HTML mapping)\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (64388 chars of RAW HTML)\n",
      "\n",
      "Sample of RAW HTML returned:\n",
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\n",
      "    <head>\n",
      "        <title>\n",
      "    All products | Books to Scrape - Sandbox\n",
      "</title>\n",
      "\n",
      "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" /...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (369 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"your html content here\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "books = soup.find_all('div', class_='col-md-4 product-grid-item')\n",
      "\n",
      "for book in books:\n",
      "    name = book.find('h3').text\n",
      "    price = book.find('span', class_='price').text\n",
      "\n",
      "    result.append({'name': name, 'price': price})\n",
      "\n",
      "print(result)\n",
      "\n",
      "\n",
      "Step 4: Executing extraction code on FULL HTML...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "‚úì Extracted 0 items\n",
      "\n",
      "First 2 items:\n",
      "[]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: scenario2_jobs.html\n",
      "Query: Extract job title, location, salary, and company name from the listings\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building hybrid search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 19 chunks (clean text ‚Üí raw HTML mapping)\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (91338 chars of RAW HTML)\n",
      "\n",
      "Sample of RAW HTML returned:\n",
      "e Central Coast Region of NSW. The hospital\\u0026#39;s emergency department is the third busiest in the state.\\u003cbr\\u003e\\u003cbr\\u003eThere are multiple services including:\\u003cbr\\u003e\\u003cbr\\u003e‚Ä¢ Aboriginal Health¬†\\u003cbr\\u003e‚Ä¢ Adult Community Services¬†\\u003cbr\\u003e‚Ä¢ Audiology¬†\\u003cbr\\u003e‚Ä¢ Cancer Services¬†\\u003cbr\\u003e‚Ä¢ Cardiology¬†\\u003cbr\\u003e‚Ä¢ Chest Clinic¬†\\u003cbr\\u003e‚Ä¢ Children‚Äôs Services¬†\\u003cbr\\u003e‚Ä¢ Dental Services¬†\\u003cbr\\u003e‚Ä¢ Women‚Äôs Health and Maternity¬†\\u003cbr...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (724 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"... your RAW HTML content here ...\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "for client_description in soup.find_all('p', class_='clientDescriptionPlainText'):\n",
      "    job_title = None\n",
      "    location = None\n",
      "    salary = None\n",
      "    company_name = None\n",
      "    \n",
      "    for line in client_description.text.split('\\n'):\n",
      "        if 'This is a' in line:\n",
      "            company_name = line.strip()\n",
      "        elif '484-bed public hospital' in line:\n",
      "            job_title = 'Hospital Job'\n",
      "        elif 'Central Coast Region of NSW' in line:\n",
      "            location = line.strip()\n",
      "\n",
      "result = [{'job_title': job_title, 'location': location, 'salary': salary, 'company_name': company_name}]\n",
      "\n",
      "\n",
      "Step 4: Executing extraction code on FULL HTML...\n",
      "  ‚úó Error: Error executing code: name 'job_title' is not defined\n",
      "\n",
      "Code was:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"... your RAW HTML content here ...\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "for client_description in soup.find_all('p', class_='clientDescriptionPlainText'):\n",
      "    job_title = None\n",
      "    location = None\n",
      "    salary = None\n",
      "    company_name = None\n",
      "    \n",
      "    for line in client_description.text.split('\\n'):\n",
      "        if 'This is a' in line:\n",
      "            company_name = line.strip()\n",
      "        elif '484-bed public hospital' in line:\n",
      "            job_title = 'Hospital Job'\n",
      "        elif 'Central Coast Region of NSW' in line:\n",
      "            location = line.strip()\n",
      "\n",
      "result = [{'job_title': job_title, 'location': location, 'salary': salary, 'company_name': company_name}]\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "{\n",
      "  \"error\": \"Error executing code: name 'job_title' is not defined\\n\\nCode was:\\n\\nfrom bs4 import BeautifulSoup\\n\\nhtml_content = \\\"\\\"\\\"... your RAW HTML content here ...\\\"\\\"\\\"\\n\\nsoup = BeautifulSoup(html_content, 'lxml')\\n\\nresult = []\\n\\nfor client_description in soup.find_all('p', class_='clientDescriptionPlainText'):\\n    job_title = None\\n    location = None\\n    salary = None\\n    company_name = None\\n    \\n    for line in client_description.text.split('\\\\n'):\\n        if 'This is a' in line:\\n            company_name = line.strip()\\n        elif '484-bed public hospital' in line:\\n            job_title = 'Hospital Job'\\n        elif 'Central Coast Region of NSW' in line:\\n            location = line.strip()\\n\\nresult = [{'job_title': job_title, 'location': location, 'salary': salary, 'company_name': company_name}]\\n\"\n",
      "}\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: scenario3_clubs.html\n",
      "Query: Get the club names, logo image links and their official websites\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building hybrid search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 5 chunks (clean text ‚Üí raw HTML mapping)\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (145329 chars of RAW HTML)\n",
      "\n",
      "Sample of RAW HTML returned:\n",
      "home.php\"><img loading=\"lazy\" decoding=\"async\" width=\"500\" height=\"500\" src=\"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png\" alt=\"\" class=\"wp-image-752\" srcset=\"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png 500w, https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png?resize=150,150 150w, https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png?resize=300,300 300w, https://www.azsocc...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (1178 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"your html content here\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "for club in soup.find_all('div', class_='wp-block-genesis-blocks-gb-accordion gb-block-accordion'):\n",
      "    for summary in club.find_all('details', summary=True):\n",
      "        for title in summary.find_all('summary', class_='gb-accordion-title'):\n",
      "            for content in title.find_next_siblings('div', class_='gb-accordion-text'):\n",
      "                for grid in content.find_all('ghostkit-grid'):\n",
      "                    for inner in grid.find_all('ghostkit-grid-inner'):\n",
      "                        for column in inner.find_all('ghostkit-col'):\n",
      "                            for image in column.find_all('figure'):\n",
      "                                for link in image.find_all('a'):\n",
      "                                    club_info = {\n",
      "                                        'club_name': title.get_text(),\n",
      "                                        'logo_image_link': link['href'],\n",
      "                                        'official_website': link['href']\n",
      "                                    }\n",
      "                                    result.append(club_info)\n",
      "\n",
      "print(result)\n",
      "\n",
      "\n",
      "Step 4: Executing extraction code on FULL HTML...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "‚úì Extracted 0 items\n",
      "\n",
      "First 2 items:\n",
      "[]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: scenario4_property.html\n",
      "Query: Return the property name, address, latitude and longitude\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building hybrid search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 9 chunks (clean text ‚Üí raw HTML mapping)\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (116881 chars of RAW HTML)\n",
      "\n",
      "Sample of RAW HTML returned:\n",
      "ic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;color:var(--variant-containedColor);background-color:var(--variant-containedBg);box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);--variant-textColor:#022B54;--variant-outlinedColor:#022B54;--variant-outlinedBorder:rgba(2, 43, 84, 0.5);--variant-containedColor:#F...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (536 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"... your raw HTML content here ...\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "for item in soup.find_all(class_='css-1s1p3nq'):\n",
      "    property_name = item.get('aria-label')\n",
      "    address = item.get('data-address')\n",
      "    latitude = item.get('data-latitude')\n",
      "    longitude = item.get('data-longitude')\n",
      "    \n",
      "    result.append({\n",
      "        'property_name': property_name,\n",
      "        'address': address,\n",
      "        'latitude': latitude,\n",
      "        'longitude': longitude\n",
      "    })\n",
      "\n",
      "print(result)\n",
      "\n",
      "\n",
      "Step 4: Executing extraction code on FULL HTML...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "‚úì Extracted 0 items\n",
      "\n",
      "First 2 items:\n",
      "[]\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test hybrid approach on all scenarios\n",
    "hybrid_results = {}\n",
    "\n",
    "for scenario_name, html_file in HTML_FILES.items():\n",
    "    query = TEST_QUERIES[scenario_name]\n",
    "    \n",
    "    try:\n",
    "        result = hybrid_extraction_agent(html_file, query)\n",
    "        hybrid_results[scenario_name] = result\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RESULT:\")\n",
    "        if isinstance(result, list):\n",
    "            print(f\"‚úì Extracted {len(result)} items\")\n",
    "            print(\"\\nFirst 2 items:\")\n",
    "            print(json.dumps(result[:2], indent=2))\n",
    "        else:\n",
    "            print(json.dumps(result, indent=2))\n",
    "        print(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error: {e}\\n\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        hybrid_results[scenario_name] = {\"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYBRID APPROACH RESULTS: Semantic Search + Code Generation\n",
      "================================================================================\n",
      "\n",
      "scenario1_books:\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "scenario2_jobs:\n",
      "  ‚úó Error: Error executing code: name 'job_title' is not defined\n",
      "\n",
      "Code was:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "htm\n",
      "\n",
      "scenario3_clubs:\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "scenario4_property:\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üéØ KEY ADVANTAGES OF THIS APPROACH:\n",
      "\n",
      "1. ‚úÖ Semantic search finds WHERE data is (clean text embeddings)\n",
      "2. ‚úÖ LLM sees RAW HTML with tags/classes (accurate selectors)\n",
      "3. ‚úÖ Code runs on FULL HTML (no chunking limitations)\n",
      "4. ‚úÖ Deterministic extraction (same code ‚Üí same results)\n",
      "5. ‚úÖ Complete data (ALL items extracted)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID APPROACH RESULTS: Semantic Search + Code Generation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for scenario_name, result in hybrid_results.items():\n",
    "    print(f\"{scenario_name}:\")\n",
    "    if isinstance(result, list):\n",
    "        print(f\"  ‚úì Extracted {len(result)} items\")\n",
    "        if len(result) > 0:\n",
    "            print(f\"  Fields: {', '.join(result[0].keys())}\")\n",
    "    elif isinstance(result, dict) and \"error\" in result:\n",
    "        print(f\"  ‚úó Error: {result['error'][:100]}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Partial result\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéØ KEY ADVANTAGES OF THIS APPROACH:\\n\")\n",
    "print(\"1. ‚úÖ Semantic search finds WHERE data is (clean text embeddings)\")\n",
    "print(\"2. ‚úÖ LLM sees RAW HTML with tags/classes (accurate selectors)\")\n",
    "print(\"3. ‚úÖ Code runs on FULL HTML (no chunking limitations)\")\n",
    "print(\"4. ‚úÖ Deterministic extraction (same code ‚Üí same results)\")\n",
    "print(\"5. ‚úÖ Complete data (ALL items extracted)\")\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. The Hybrid Approach Explained\n",
    "\n",
    "### What Makes This \"Hybrid\"?\n",
    "\n",
    "We combine two different representations of the same HTML:\n",
    "\n",
    "| Step | Uses | Why |\n",
    "|------|------|-----|\n",
    "| **Indexing** | Clean Text | Better semantic signal for embeddings |\n",
    "| **Searching** | Clean Text Embeddings | Accurate semantic matching |\n",
    "| **Retrieval** | Raw HTML | Complete context with tags/attributes |\n",
    "| **Code Generation** | Raw HTML | LLM sees full structure for accurate selectors |\n",
    "| **Execution** | Full HTML | Extract ALL items, no chunking limits |\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**Problem with pure approaches:**\n",
    "- Embed clean text ‚Üí return clean text ‚ùå (LLM misses HTML structure)\n",
    "- Embed raw HTML ‚Üí return raw HTML ‚ùå (Poor semantic search due to noise)\n",
    "\n",
    "**Hybrid solution:**\n",
    "- Embed clean text ‚Üí return raw HTML ‚úÖ (Best of both worlds!)\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "# User searches for: \"job listings\"\n",
    "\n",
    "# 1. Semantic search on CLEAN TEXT\n",
    "clean_chunks = [\n",
    "    \"Software Engineer\\nNew York\\n$120k-150k\\nGoogle\",\n",
    "    \"Data Scientist\\nSan Francisco\\n$140k-180k\\nFacebook\"\n",
    "]\n",
    "# ‚Üì Vector similarity finds relevant chunks\n",
    "\n",
    "# 2. Return corresponding RAW HTML\n",
    "raw_html = '''\n",
    "<div class=\"job-card\">\n",
    "    <h2 class=\"job-title\">Software Engineer</h2>\n",
    "    <span class=\"location\">New York</span>\n",
    "    <span class=\"salary\">$120k-150k</span>\n",
    "    <span class=\"company\">Google</span>\n",
    "</div>\n",
    "'''\n",
    "\n",
    "# 3. LLM sees the classes and generates:\n",
    "code = '''\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "jobs = soup.find_all('div', class_='job-card')\n",
    "result = [{\n",
    "    'title': job.find('h2', class_='job-title').text,\n",
    "    'location': job.find('span', class_='location').text,\n",
    "    # ... accurate selectors based on real HTML structure\n",
    "}]\n",
    "'''\n",
    "```\n",
    "\n",
    "This is why the hybrid approach beats both pure RAG and pure code generation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced: LangGraph-Based Agent (With Iteration)\n",
    "\n",
    "Let's implement a more sophisticated agent that can truly iterate and refine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úì LangGraph installed!\n"
     ]
    }
   ],
   "source": [
    "# Install LangGraph for advanced agent workflows\n",
    "%pip install -q langgraph\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "print(\"‚úì LangGraph installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LangGraph workflow builder defined!\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for our iterative agent\"\"\"\n",
    "    query: str\n",
    "    html_path: str\n",
    "    relevant_sections: List[str]\n",
    "    generated_code: str\n",
    "    execution_result: str\n",
    "    extracted_data: List[Dict]\n",
    "    iterations: int\n",
    "    max_iterations: int\n",
    "    errors: List[str]\n",
    "    finished: bool\n",
    "\n",
    "def build_search_extraction_graph():\n",
    "    \"\"\"\n",
    "    Build a LangGraph agent that:\n",
    "    1. Searches HTML semantically\n",
    "    2. Generates extraction code\n",
    "    3. Executes and validates\n",
    "    4. Iterates if needed\n",
    "    \"\"\"\n",
    "    \n",
    "    def search_step(state: AgentState) -> AgentState:\n",
    "        \"\"\"Step 1: Search for relevant HTML sections\"\"\"\n",
    "        print(f\"\\n[Iteration {state['iterations']}] Searching HTML...\")\n",
    "        \n",
    "        # Use different search queries each iteration\n",
    "        queries = [\n",
    "            state['query'],\n",
    "            f\"all {state['query']}\",\n",
    "            f\"list of {state['query']}\",\n",
    "            f\"table data {state['query']}\"\n",
    "        ]\n",
    "        \n",
    "        search_query = queries[min(state['iterations'], len(queries)-1)]\n",
    "        sections = search_engine.search(search_query, k=10)\n",
    "        \n",
    "        state['relevant_sections'] = sections\n",
    "        print(f\"  ‚úì Found {len(sections)} sections\")\n",
    "        return state\n",
    "    \n",
    "    def code_generation_step(state: AgentState) -> AgentState:\n",
    "        \"\"\"Step 2: Generate extraction code\"\"\"\n",
    "        print(f\"[Iteration {state['iterations']}] Generating code...\")\n",
    "        \n",
    "        # Build prompt with context from previous attempts\n",
    "        error_context = \"\"\n",
    "        if state['errors']:\n",
    "            error_context = \"\\nPrevious errors:\\n\" + \"\\n\".join(state['errors'][-2:])\n",
    "        \n",
    "        sections_sample = \"\\n\".join(state['relevant_sections'][:2])\n",
    "        prompt = f\"\"\"\n",
    "Generate BeautifulSoup code to extract data from HTML.\n",
    "\n",
    "Query: {state['query']}\n",
    "\n",
    "HTML Sample (showing {len(state['relevant_sections'])} relevant sections):\n",
    "{sections_sample}\n",
    "\n",
    "{error_context}\n",
    "\n",
    "Requirements:\n",
    "1. Use: soup = BeautifulSoup(html_content, 'lxml')\n",
    "2. Store in: result = [list of dicts]\n",
    "3. Extract ALL items, not just samples\n",
    "4. Handle missing data with try/except or .get()\n",
    "5. Be robust - check if elements exist\n",
    "\n",
    "Return ONLY Python code:\n",
    "\"\"\"\n",
    "        \n",
    "        code_response = llm.invoke(prompt)\n",
    "        \n",
    "        # Clean code\n",
    "        code = code_response\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in code:\n",
    "            code = code.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        state['generated_code'] = code.strip()\n",
    "        print(f\"  ‚úì Generated {len(code)} chars of code\")\n",
    "        return state\n",
    "    \n",
    "    def execution_step(state: AgentState) -> AgentState:\n",
    "        \"\"\"Step 3: Execute code and validate\"\"\"\n",
    "        print(f\"[Iteration {state['iterations']}] Executing code...\")\n",
    "        \n",
    "        try:\n",
    "            # Execute\n",
    "            local_vars = {\n",
    "                'html_content': search_engine.html_content,\n",
    "                'BeautifulSoup': BeautifulSoup,\n",
    "                'json': json\n",
    "            }\n",
    "            \n",
    "            exec(state['generated_code'], {}, local_vars)\n",
    "            result = local_vars.get('result', [])\n",
    "            \n",
    "            state['extracted_data'] = result\n",
    "            state['execution_result'] = f\"Success: {len(result)} items\"\n",
    "            print(f\"  ‚úì Extracted {len(result)} items\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error: {str(e)}\"\n",
    "            state['errors'].append(error_msg)\n",
    "            state['execution_result'] = error_msg\n",
    "            print(f\"  ‚úó {error_msg}\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def should_continue(state: AgentState) -> str:\n",
    "        \"\"\"Decide if we should iterate or finish\"\"\"\n",
    "        \n",
    "        # Check if we have good results\n",
    "        if state['extracted_data'] and len(state['extracted_data']) > 0:\n",
    "            state['finished'] = True\n",
    "            return \"end\"\n",
    "        \n",
    "        # Check iteration limit\n",
    "        if state['iterations'] >= state['max_iterations']:\n",
    "            state['finished'] = True\n",
    "            return \"end\"\n",
    "        \n",
    "        # Continue iterating\n",
    "        state['iterations'] += 1\n",
    "        return \"continue\"\n",
    "    \n",
    "    def increment_iteration(state: AgentState) -> AgentState:\n",
    "        \"\"\"Prepare for next iteration\"\"\"\n",
    "        return state\n",
    "    \n",
    "    # Build graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"search\", search_step)\n",
    "    workflow.add_node(\"generate\", code_generation_step)\n",
    "    workflow.add_node(\"execute\", execution_step)\n",
    "    workflow.add_node(\"iterate\", increment_iteration)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.set_entry_point(\"search\")\n",
    "    workflow.add_edge(\"search\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", \"execute\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"execute\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"iterate\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"iterate\", \"search\")\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "print(\"‚úì LangGraph workflow builder defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LangGraph extraction function defined!\n"
     ]
    }
   ],
   "source": [
    "def langgraph_extraction(html_file_path: Path, query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run extraction using LangGraph agent with true iteration\n",
    "    \"\"\"\n",
    "    global search_engine\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"LangGraph Agent: {html_file_path.name}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load HTML and build search index\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    search_engine = HTMLSearchEngine(html_content, embeddings)\n",
    "    \n",
    "    # Build agent\n",
    "    agent = build_search_extraction_graph()\n",
    "    \n",
    "    # Initial state\n",
    "    initial_state = AgentState(\n",
    "        query=query,\n",
    "        html_path=str(html_file_path),\n",
    "        relevant_sections=[],\n",
    "        generated_code=\"\",\n",
    "        execution_result=\"\",\n",
    "        extracted_data=[],\n",
    "        iterations=0,\n",
    "        max_iterations=3,\n",
    "        errors=[],\n",
    "        finished=False\n",
    "    )\n",
    "    \n",
    "    # Run agent\n",
    "    final_state = agent.invoke(initial_state)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL RESULTS:\")\n",
    "    print(f\"  Iterations: {final_state['iterations']}\")\n",
    "    print(f\"  Extracted: {len(final_state['extracted_data'])} items\")\n",
    "    if final_state['errors']:\n",
    "        print(f\"  Errors encountered: {len(final_state['errors'])}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return final_state['extracted_data']\n",
    "\n",
    "print(\"‚úì LangGraph extraction function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test LangGraph Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LangGraph Agent: scenario2_jobs.html\n",
      "Query: Extract job title, location, salary, and company name from the listings\n",
      "================================================================================\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 19 chunks (clean text ‚Üí raw HTML mapping)\n",
      "\n",
      "[Iteration 0] Searching HTML...\n",
      "  ‚úì Found 10 sections\n",
      "[Iteration 0] Generating code...\n",
      "  ‚úì Generated 945 chars of code\n",
      "[Iteration 0] Executing code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "[Iteration 0] Searching HTML...\n",
      "  ‚úì Found 10 sections\n",
      "[Iteration 0] Generating code...\n",
      "  ‚úì Generated 977 chars of code\n",
      "[Iteration 0] Executing code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "[Iteration 0] Searching HTML...\n",
      "  ‚úì Found 10 sections\n",
      "[Iteration 0] Generating code...\n",
      "  ‚úì Generated 945 chars of code\n",
      "[Iteration 0] Executing code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "[Iteration 0] Searching HTML...\n",
      "  ‚úì Found 10 sections\n",
      "[Iteration 0] Generating code...\n",
      "  ‚úì Generated 977 chars of code\n",
      "[Iteration 0] Executing code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "[Iteration 0] Searching HTML...\n",
      "  ‚úì Found 10 sections\n",
      "[Iteration 0] Generating code...\n",
      "  ‚úì Generated 977 chars of code\n",
      "[Iteration 0] Executing code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "[Iteration 0] Searching HTML...\n",
      "  ‚úì Found 10 sections\n",
      "[Iteration 0] Generating code...\n",
      "  ‚úì Generated 977 chars of code\n",
      "[Iteration 0] Executing code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "[Iteration 0] Searching HTML...\n",
      "  ‚úì Found 10 sections\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test LangGraph agent on one scenario\u001b[39;00m\n\u001b[32m      2\u001b[39m test_scenario = \u001b[33m\"\u001b[39m\u001b[33mscenario2_jobs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mlanggraph_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mHTML_FILES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_scenario\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTEST_QUERIES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_scenario\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä EXTRACTED DATA:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(result[:\u001b[32m3\u001b[39m], indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mlanggraph_extraction\u001b[39m\u001b[34m(html_file_path, query)\u001b[39m\n\u001b[32m     22\u001b[39m initial_state = AgentState(\n\u001b[32m     23\u001b[39m     query=query,\n\u001b[32m     24\u001b[39m     html_path=\u001b[38;5;28mstr\u001b[39m(html_file_path),\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     finished=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Run agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m final_state = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFINAL RESULTS:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrscraper/lib/python3.11/site-packages/langgraph/pregel/main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/mrscraper/lib/python3.11/site-packages/langgraph/pregel/main.py:2707\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2699\u001b[39m     msg = create_error_message(\n\u001b[32m   2700\u001b[39m         message=(\n\u001b[32m   2701\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2705\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   2706\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2707\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   2708\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   2709\u001b[39m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "# Test LangGraph agent on one scenario\n",
    "test_scenario = \"scenario2_jobs\"\n",
    "result = langgraph_extraction(\n",
    "    HTML_FILES[test_scenario], \n",
    "    TEST_QUERIES[test_scenario]\n",
    ")\n",
    "\n",
    "print(\"\\nüìä EXTRACTED DATA:\")\n",
    "print(json.dumps(result[:3], indent=2))\n",
    "print(f\"\\nTotal items: {len(result)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Analysis: Why This Hybrid Approach Works\n",
    "\n",
    "### The Problem with Pure Approaches\n",
    "\n",
    "**Pure RAG:**\n",
    "- ‚ùå Retrieves only K chunks (misses data)\n",
    "- ‚ùå LLM context limits number of items\n",
    "- ‚ùå No guarantee of completeness\n",
    "- ‚úÖ Good at finding WHERE data is\n",
    "\n",
    "**Pure Code Generation:**\n",
    "- ‚ùå Needs to know HTML structure upfront\n",
    "- ‚ùå Blind to content location\n",
    "- ‚úÖ Extracts ALL matching items\n",
    "- ‚úÖ Deterministic and repeatable\n",
    "\n",
    "### The Hybrid Solution\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Query     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Semantic   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Relevant  ‚îÇ\n",
    "‚îÇ             ‚îÇ         ‚îÇ    Search    ‚îÇ         ‚îÇ   Sections  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                         ‚îÇ\n",
    "                                                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Complete   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   Execute    ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Generate   ‚îÇ\n",
    "‚îÇ    Data     ‚îÇ         ‚îÇ     Code     ‚îÇ         ‚îÇ    Code     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ                                                 ‚îÇ\n",
    "       ‚îÇ                                                 ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Iterate if needed ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "1. **Semantic Understanding**: Search finds the right sections using embeddings\n",
    "2. **Complete Extraction**: Code runs on FULL HTML, gets ALL items\n",
    "3. **Iterative Refinement**: Agent can improve code based on results\n",
    "4. **Deterministic**: Same structure ‚Üí same code ‚Üí same results\n",
    "5. **Robust**: Handles edge cases with error handling\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "| Scenario | Best Approach | Why |\n",
    "|----------|--------------|-----|\n",
    "| Small HTML (<10KB) | Direct LLM | No need for search, fits in context |\n",
    "| Large HTML with lists | **Hybrid** | Search finds structure, code extracts all |\n",
    "| Known HTML structure | Pure Code Gen | Skip search, write code directly |\n",
    "| Question answering | Pure RAG | No need for completeness, just answer |\n",
    "| Dynamic/varying sites | **Hybrid** | Search adapts, code generation is flexible |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Defined 3 tools for the agent\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def search_html_sections(query: str, num_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Search HTML content semantically to find relevant sections.\n",
    "    Use this to understand the HTML structure before generating extraction code.\n",
    "    \n",
    "    Args:\n",
    "        query: What to search for (e.g., \"job listings\", \"book prices\")\n",
    "        num_results: Number of relevant sections to return\n",
    "    \n",
    "    Returns:\n",
    "        String with relevant HTML sections\n",
    "    \"\"\"\n",
    "    if search_engine is None:\n",
    "        return \"Error: Search engine not initialized\"\n",
    "    \n",
    "    results = search_engine.search(query, k=num_results)\n",
    "    return \"\\n\\n---\\n\\n\".join(results[:3])  # Return top 3 to avoid overwhelming\n",
    "\n",
    "@tool\n",
    "def execute_extraction_code(python_code: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute BeautifulSoup extraction code on the full HTML.\n",
    "    \n",
    "    The code should:\n",
    "    1. Parse HTML with: soup = BeautifulSoup(html_content, 'lxml')\n",
    "    2. Extract data using BeautifulSoup methods\n",
    "    3. Return a list of dictionaries with extracted data\n",
    "    \n",
    "    Args:\n",
    "        python_code: BeautifulSoup extraction code\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with extracted data or error message\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create safe execution environment\n",
    "        local_vars = {\n",
    "            'html_content': search_engine.html_content if search_engine else \"\",\n",
    "            'BeautifulSoup': BeautifulSoup,\n",
    "            'json': json\n",
    "        }\n",
    "        \n",
    "        # Execute code\n",
    "        exec(python_code, {}, local_vars)\n",
    "        \n",
    "        # Get result (code should set 'result' variable)\n",
    "        result = local_vars.get('result', [])\n",
    "        \n",
    "        return json.dumps(result, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error executing code: {str(e)}\\n\\nCode was:\\n{python_code}\"\n",
    "\n",
    "@tool\n",
    "def get_html_sample(num_chars: int = 8000) -> str:\n",
    "    \"\"\"\n",
    "    Get a sample of the HTML to understand its structure.\n",
    "    \n",
    "    Args:\n",
    "        num_chars: Number of characters to return\n",
    "    \n",
    "    Returns:\n",
    "        HTML sample\n",
    "    \"\"\"\n",
    "    if search_engine is None:\n",
    "        return \"Error: Search engine not initialized\"\n",
    "    \n",
    "    return search_engine.html_content[:num_chars]\n",
    "\n",
    "# Collect all tools\n",
    "tools = [search_html_sections, execute_extraction_code, get_html_sample]\n",
    "\n",
    "print(f\"‚úì Defined {len(tools)} tools for the agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Agent\n",
    "\n",
    "ReAct-style agent that can iteratively search and generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent prompt created\n"
     ]
    }
   ],
   "source": [
    "# Create agent prompt\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are an expert at extracting structured data from HTML.\n",
    "\n",
    "Your approach:\n",
    "1. Use search_html_sections to find where the relevant data is located\n",
    "2. Analyze the HTML structure from search results\n",
    "3. Generate BeautifulSoup extraction code using execute_extraction_code\n",
    "4. If results are incomplete, iterate:\n",
    "   - Search for different patterns\n",
    "   - Refine your extraction code\n",
    "   - Try different selectors\n",
    "5. Return complete, structured JSON data\n",
    "\n",
    "Code generation rules:\n",
    "- Always use: soup = BeautifulSoup(html_content, 'lxml')\n",
    "- Store results in a variable called 'result' (list of dicts)\n",
    "- Extract ALL matching items, not just a sample\n",
    "- Use robust selectors (class names, tag structure)\n",
    "- Handle missing data gracefully\n",
    "\n",
    "Example code structure:\n",
    "```python\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "result = []\n",
    "items = soup.find_all('div', class_='item')\n",
    "for item in items:\n",
    "    result.append({\n",
    "        'name': item.find('h2').text.strip(),\n",
    "        'price': item.find('span', class_='price').text.strip()\n",
    "    })\n",
    "```\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Note: create_tool_calling_agent requires a model that supports tool calling\n",
    "# For Ollama, we'll use a simpler approach with structured prompting\n",
    "print(\"‚úì Agent prompt created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simple Agent Implementation\n",
    "\n",
    "Since Ollama doesn't natively support tool calling, let's create a simpler agent loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Hybrid agent function defined\n"
     ]
    }
   ],
   "source": [
    "def hybrid_extraction_agent(\n",
    "    html_file_path: Path,\n",
    "    query: str,\n",
    "    max_iterations: int = 3\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Hybrid extraction: Search semantically, then generate code\n",
    "    \n",
    "    Args:\n",
    "        html_file_path: Path to HTML file\n",
    "        query: Extraction query\n",
    "        max_iterations: Max attempts to refine\n",
    "    \n",
    "    Returns:\n",
    "        Extracted data\n",
    "    \"\"\"\n",
    "    global search_engine\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {html_file_path.name}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Load HTML\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Initialize search engine\n",
    "    print(\"Step 1: Building semantic search index...\")\n",
    "    search_engine = HTMLSearchEngine(html_content, embeddings)\n",
    "    print(\"  ‚úì Index built\\n\")\n",
    "    \n",
    "    # Step 2: Search for relevant sections\n",
    "    print(\"Step 2: Searching for relevant HTML sections...\")\n",
    "    relevant_sections = search_html_sections.invoke({\"query\": query, \"num_results\": 20})\n",
    "    print(f\"  ‚úì Found relevant sections ({len(relevant_sections)} chars)\\n\")\n",
    "    print(f\"Sample:\\n{relevant_sections[:500]}...\\n\")\n",
    "    \n",
    "    # Step 3: Generate extraction code\n",
    "    print(\"Step 3: Generating BeautifulSoup extraction code...\")\n",
    "    \n",
    "    code_gen_prompt = f\"\"\"\n",
    "Based on this HTML sample and query, generate BeautifulSoup extraction code.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "HTML Sample:\n",
    "{relevant_sections[:8000]}\n",
    "\n",
    "Generate Python code that:\n",
    "1. Parses HTML: soup = BeautifulSoup(html_content, 'lxml')\n",
    "2. Finds ALL matching elements (not just one)\n",
    "3. Extracts the requested fields\n",
    "4. Stores results in: result = [list of dicts]\n",
    "\n",
    "Return ONLY the Python code, no explanations.\n",
    "Code:\n",
    "\"\"\"\n",
    "    \n",
    "    code_response = llm.invoke(code_gen_prompt)\n",
    "    \n",
    "    # Extract code from response\n",
    "    code = code_response\n",
    "    if \"```python\" in code:\n",
    "        code = code.split(\"```python\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in code:\n",
    "        code = code.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    print(f\"  ‚úì Generated code ({len(code)} chars)\\n\")\n",
    "    print(f\"Code:\\n{code}\\n\")\n",
    "    \n",
    "    # Step 4: Execute code\n",
    "    print(\"Step 4: Executing extraction code...\")\n",
    "    result_json = execute_extraction_code.invoke({\"python_code\": code})\n",
    "    \n",
    "    try:\n",
    "        result = json.loads(result_json)\n",
    "        print(f\"  ‚úì Extracted {len(result) if isinstance(result, list) else 'some'} items\\n\")\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  ‚úó Error: {result_json}\\n\")\n",
    "        return {\"error\": result_json}\n",
    "\n",
    "print(\"‚úì Hybrid agent function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on All Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing: scenario1_books.html\n",
      "Query: Can you return me the books: name and price?\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building semantic search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 3 chunks (clean text ‚Üí raw HTML mapping)\n",
      "  ‚úì Index built\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (84614 chars)\n",
      "\n",
      "Sample:\n",
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\n",
      "    <head>\n",
      "        <title>\n",
      "    All products | Books to Scrape - Sandbox\n",
      "</title>\n",
      "\n",
      "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" /...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (365 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"your html content here\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "for book in soup.find_all('article', class_='product'):\n",
      "    book_info = {}\n",
      "    book_info['name'] = book.find('h2').text\n",
      "    book_info['price'] = book.find('p', class_='price_color').text\n",
      "    result.append(book_info)\n",
      "\n",
      "print(result)\n",
      "\n",
      "\n",
      "Step 4: Executing extraction code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "‚úì Extracted 0 items\n",
      "\n",
      "First 2 items:\n",
      "[]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: scenario2_jobs.html\n",
      "Query: Extract job title, location, salary, and company name from the listings\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building semantic search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 19 chunks (clean text ‚Üí raw HTML mapping)\n",
      "  ‚úì Index built\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (71999 chars)\n",
      "\n",
      "Sample:\n",
      "e Central Coast Region of NSW. The hospital\\u0026#39;s emergency department is the third busiest in the state.\\u003cbr\\u003e\\u003cbr\\u003eThere are multiple services including:\\u003cbr\\u003e\\u003cbr\\u003e‚Ä¢ Aboriginal Health¬†\\u003cbr\\u003e‚Ä¢ Adult Community Services¬†\\u003cbr\\u003e‚Ä¢ Audiology¬†\\u003cbr\\u003e‚Ä¢ Cancer Services¬†\\u003cbr\\u003e‚Ä¢ Cardiology¬†\\u003cbr\\u003e‚Ä¢ Chest Clinic¬†\\u003cbr\\u003e‚Ä¢ Children‚Äôs Services¬†\\u003cbr\\u003e‚Ä¢ Dental Services¬†\\u003cbr\\u003e‚Ä¢ Women‚Äôs Health and Maternity¬†\\u003cbr...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (631 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"your html content here\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "for job in soup.find_all('div', {'class': 'job'}):\n",
      "    job_dict = {}\n",
      "    job_dict['title'] = job.find('h2').text.strip()\n",
      "    job_dict['location'] = job.find('span', {'itemprop': 'addressLocality'}).text.strip()\n",
      "    job_dict['salary'] = job.find('span', {'itemprop': 'baseSalary'}).text.strip() if job.find('span', {'itemprop': 'baseSalary'}) else None\n",
      "    job_dict['company'] = job.find('span', {'itemprop': 'organization'}).text.strip()\n",
      "    result.append(job_dict)\n",
      "\n",
      "print(result)\n",
      "\n",
      "\n",
      "Step 4: Executing extraction code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "‚úì Extracted 0 items\n",
      "\n",
      "First 2 items:\n",
      "[]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: scenario3_clubs.html\n",
      "Query: Get the club names, logo image links and their official websites\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building semantic search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 5 chunks (clean text ‚Üí raw HTML mapping)\n",
      "  ‚úì Index built\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (82610 chars)\n",
      "\n",
      "Sample:\n",
      "home.php\"><img loading=\"lazy\" decoding=\"async\" width=\"500\" height=\"500\" src=\"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png\" alt=\"\" class=\"wp-image-752\" srcset=\"https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png 500w, https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png?resize=150,150 150w, https://www.azsoccerassociation.org/wp-content/uploads/sites/186/2023/09/63.png?resize=300,300 300w, https://www.azsocc...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (579 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"your html content here\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "for club in soup.find_all('details', summary=True):\n",
      "    logo_link = None\n",
      "    website = None\n",
      "    for child in club.find_all('figure'):\n",
      "        if child.a:\n",
      "            logo_link = child.a['href']\n",
      "        fig_caption = child.figcaption\n",
      "        if fig_caption.a:\n",
      "            website = fig_caption.a['href']\n",
      "\n",
      "    result.append({\n",
      "        'club_name': club.summary.text,\n",
      "        'logo_link': logo_link,\n",
      "        'website': website\n",
      "    })\n",
      "\n",
      "print(result)\n",
      "\n",
      "\n",
      "Step 4: Executing extraction code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "‚úì Extracted 0 items\n",
      "\n",
      "First 2 items:\n",
      "[]\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing: scenario4_property.html\n",
      "Query: Return the property name, address, latitude and longitude\n",
      "================================================================================\n",
      "\n",
      "Step 1: Building semantic search index...\n",
      "  Building hybrid index (clean text embeddings + raw HTML storage)...\n",
      "  ‚úì Indexed 9 chunks (clean text ‚Üí raw HTML mapping)\n",
      "  ‚úì Index built\n",
      "\n",
      "Step 2: Searching for relevant HTML sections...\n",
      "  ‚úì Found relevant sections (116861 chars)\n",
      "\n",
      "Sample:\n",
      "ic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;color:var(--variant-containedColor);background-color:var(--variant-containedBg);box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);--variant-textColor:#022B54;--variant-outlinedColor:#022B54;--variant-outlinedBorder:rgba(2, 43, 84, 0.5);--variant-containedColor:#F...\n",
      "\n",
      "Step 3: Generating BeautifulSoup extraction code...\n",
      "  ‚úì Generated code (683 chars)\n",
      "\n",
      "Code:\n",
      "\n",
      "import re\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html_content = \"\"\"... your HTML content here ...\"\"\"\n",
      "\n",
      "soup = BeautifulSoup(html_content, 'lxml')\n",
      "\n",
      "result = []\n",
      "\n",
      "for button in soup.find_all('button', class_='MuiButtonBase-root MuiButton-root MuiButton-contained MuiButton-containedPrimary'):\n",
      "    property_name = re.search(r'--variant-containedColor:(#[0-9A-Fa-f]{6})', str(button)).group(1)\n",
      "    address = re.search(r'--variant-containedBg:(#[0-9A-Fa-f]{6})', str(button)).group(1)\n",
      "    latitude = None\n",
      "    longitude = None\n",
      "\n",
      "    result.append({\n",
      "        'property_name': property_name,\n",
      "        'address': address,\n",
      "        'latitude': latitude,\n",
      "        'longitude': longitude\n",
      "    })\n",
      "\n",
      "print(result)\n",
      "\n",
      "Step 4: Executing extraction code...\n",
      "[]\n",
      "  ‚úì Extracted 0 items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULT:\n",
      "‚úì Extracted 0 items\n",
      "\n",
      "First 2 items:\n",
      "[]\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test hybrid approach on all scenarios\n",
    "hybrid_results = {}\n",
    "\n",
    "for scenario_name, html_file in HTML_FILES.items():\n",
    "    query = TEST_QUERIES[scenario_name]\n",
    "    \n",
    "    try:\n",
    "        result = hybrid_extraction_agent(html_file, query)\n",
    "        hybrid_results[scenario_name] = result\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RESULT:\")\n",
    "        if isinstance(result, list):\n",
    "            print(f\"‚úì Extracted {len(result)} items\")\n",
    "            print(\"\\nFirst 2 items:\")\n",
    "            print(json.dumps(result[:2], indent=2))\n",
    "        else:\n",
    "            print(json.dumps(result, indent=2))\n",
    "        print(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Error: {e}\\n\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        hybrid_results[scenario_name] = {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID APPROACH RESULTS: Semantic Search + Code Generation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for scenario_name, result in hybrid_results.items():\n",
    "    print(f\"{scenario_name}:\")\n",
    "    if isinstance(result, list):\n",
    "        print(f\"  ‚úì Extracted {len(result)} items\")\n",
    "        if len(result) > 0:\n",
    "            print(f\"  Fields: {', '.join(result[0].keys())}\")\n",
    "    elif isinstance(result, dict) and \"error\" in result:\n",
    "        print(f\"  ‚úó Error: {result['error'][:100]}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Partial result\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéØ KEY ADVANTAGES OF THIS APPROACH:\\n\")\n",
    "print(\"1. Search finds WHERE data is (leverages semantic understanding)\")\n",
    "print(\"2. Code generation extracts WHAT data is (systematic, complete)\")\n",
    "print(\"3. Runs on FULL HTML (no chunking limitations)\")\n",
    "print(\"4. Deterministic extraction (same code ‚Üí same results)\")\n",
    "print(\"5. Iterative refinement possible (agent can improve code)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison with Pure RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä APPROACH COMPARISON:\\n\")\n",
    "print(\"Approach            | Search | Extraction | Complete | Fast | Deterministic\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Pure RAG            |   ‚úÖ   |     ‚ùå     |    ‚ùå    |  ‚úÖ  |      ‚ùå\")\n",
    "print(\"Pure Code Gen       |   ‚ùå   |     ‚úÖ     |    ‚úÖ    |  ‚úÖ  |      ‚úÖ\")\n",
    "print(\"HYBRID (This)       |   ‚úÖ   |     ‚úÖ     |    ‚úÖ    |  ‚ö†Ô∏è   |      ‚úÖ\")\n",
    "print(\"-\" * 80)\n",
    "print(\"\\n‚úÖ Pure RAG: Good at finding info, bad at complete extraction\")\n",
    "print(\"‚úÖ Pure Code Gen: Great extraction, but needs to know where to look\")\n",
    "print(\"üéØ HYBRID: Best of both - search finds it, code extracts it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps & Improvements\n",
    "\n",
    "### Possible Enhancements:\n",
    "\n",
    "1. **Better Agent Framework**\n",
    "   - Use LangGraph for more sophisticated agent loops\n",
    "   - Implement proper ReAct pattern with tool calling\n",
    "   - Add memory for multi-turn refinement\n",
    "\n",
    "2. **Code Validation**\n",
    "   - Test generated code on HTML sample first\n",
    "   - Validate results meet expected schema\n",
    "   - Auto-retry with feedback if extraction fails\n",
    "\n",
    "3. **Smarter Search**\n",
    "   - Use query to determine search strategy\n",
    "   - Adjust k based on expected result count\n",
    "   - Try multiple search queries if needed\n",
    "\n",
    "4. **Code Library**\n",
    "   - Cache successful extraction patterns\n",
    "   - Build library of common HTML structures\n",
    "   - Reuse patterns for similar sites\n",
    "\n",
    "5. **Production Features**\n",
    "   - Add rate limiting for LLM calls\n",
    "   - Implement proper error handling\n",
    "   - Log agent reasoning for debugging\n",
    "   - Add metrics (extraction accuracy, speed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrscraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
